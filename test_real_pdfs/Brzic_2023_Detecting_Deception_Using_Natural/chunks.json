{
  "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
  "metadata": {
    "title": "Detecting Deception Using Natural Language Processing and Machine Learning in Datasets on COVID-19 and Climate Change",
    "author": "Barbara Brzic, Ivica Boticki and Marina Bagic Babac",
    "pages": 34,
    "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
    "pdf_library": "pypdf"
  },
  "num_chunks": 147,
  "chunks": [
    {
      "text": "algorithms \nArticle\nDetecting Deception Using Natural Language Processing\nand Machine Learning in Datasets on COVID-19 and\nClimate Change\nBarbara Brzic, Ivica Boticki * and Marina Bagic Babac\nFaculty of Electrical Engineering and Computing, University of Zagreb, 10000 Zagreb, Croatia\n* Correspondence: ivica.boticki@fer.hr\nAbstract: Deception in computer-mediated communication represents a threat, and there is a growing\nneed to develop ef\ufb01cient methods of detecting it. Machine learning models have, through natural\nlanguage processing, proven to be extremely successful at detecting lexical patterns related to\ndeception.\n\nIn this study, four selected machine learning models are trained and tested on data\ncollected through a crowdsourcing platform on the topics of COVID-19 and climate change. The\nperformance of the models was tested by analyzing n-grams (from unigrams to trigrams) and by\nusing psycho-linguistic analysis. A selection of important features was carried out and further\ndeepened with additional testing of the models on different subsets of the obtained features. This\nstudy concludes that the subjectivity of the collected data greatly affects the detection of hidden\nlinguistic features of deception.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p1_c0",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 1,
      "chunk_index": 0,
      "section": "4.0/).",
      "char_start": 1202,
      "char_end": 2427,
      "prev_chunk_id": null,
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p1_c1",
      "semantic_coherence": 0.0,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "The psycho-linguistic analysis alone and in combination with n-grams\nachieves better classi\ufb01cation results than an n-gram analysis while testing the models on own data,\nbut also while examining the possibility of generalization, especially on trigrams where the combined\napproach achieves a notably higher accuracy of up to 16%. The n-gram analysis proved to be a more\nrobust method during the testing of the mutual applicability of the models while psycho-linguistic\nanalysis remained most in\ufb02exible. Keywords: deception detection; machine learning; natural language processing",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p1_c1",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 1,
      "chunk_index": 1,
      "section": "4.0/).",
      "char_start": 2427,
      "char_end": 3005,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p1_c0",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p1_c2",
      "semantic_coherence": 0.6855420470237732,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "In today\u2019s world of fast-growing technology and an inexhaustible amount of data,\nthere is a great need to control and verify data validity due to the possibility of fraud. Therefore, the need for a reliable form of detection of such content is not surprising. Some\nof the ways in which deception manifests itself on the Internet are using identity deception,\nmimicking data and processes for the purpose of stealing credit card numbers or other\nprivate information, charging false invoices for services not performed, hacking sites,\noffering false excuses and promises, false advertising, spreading propaganda and false\ninformation, and other forms of fraud.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p2_c2",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 2,
      "chunk_index": 2,
      "section": "1. Introduction",
      "char_start": 4824,
      "char_end": 5482,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p2_c1",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p2_c3",
      "semantic_coherence": 0.32557153701782227,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "Therefore, detecting deception, whether in face-to-\nface interaction or while communicating through a certain medium, is of great importance. The great need to \ufb01nd a reliable method for deception detection is even more em-\nphasized due to the fact that people tell approximately two lies per day [ 1]. Lying is\nundoubtedly a skill that is deeply rooted in the human existence, and it has been perfected\nover the years to a level that is dif\ufb01cult to recognize by even the most experienced pro-\nfessionals.\n\nThe question is what makes the distinction between truth and lies/deception,\nespecially in the verbal aspect, and if it exists, what is the best way to determine it? Do\nmost of the information lie in non-verbal behavior, or are there certain linguistic patterns\nthat can serve as suf\ufb01ciently precise indicators of deception? Is there a difference in decep-\ntion during face-to-face and computer-mediated communication, whether synchronous or\nasynchronous, verbal or non-verbal? Algorithms 2023, 16, 221.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p2_c3",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 2,
      "chunk_index": 3,
      "section": "1. Introduction",
      "char_start": 5483,
      "char_end": 6492,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p2_c2",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p2_c4",
      "semantic_coherence": 0.5284030437469482,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "https://doi.org/10.3390/a16050221 https://www.mdpi.com/journal/algorithms\n\nAlgorithms 2023, 16, 221 2 of 34\nResearch so far has led to the conclusion that verbal behavior hides a deep amount of\ninformation that can be used in the detection of deception, almost more accurately than\nin the case of non-verbal analysis. Due to the inapplicability of the polygraph method to\ndeception detection in computer-mediated communication, there is an increasing emphasis\non research in methods for analyzing the syntactic and semantic properties of written text\nand \ufb01nding indicators of deception in various forms of digital interaction. So far, the most\ncommonly used methods of deception detection in the text are machine learning models.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p2_c4",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 2,
      "chunk_index": 4,
      "section": "1. Introduction",
      "char_start": 6492,
      "char_end": 7221,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p2_c3",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p2_c5",
      "semantic_coherence": 0.7836185097694397,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "There is a great need for further research into syntactic, semantic, and other properties of\nnatural languages in order to create software that will detect deception with high accuracy. Current research covers deception detection in computer-mediated communication[2,3];\ndetection of fake reviews on social platforms [ 4,5]; deception detection collected from\npublic trials [6,7]; the use of crowdsourcing platforms, such as Amazon Mechanical Turk,\nfor generating deception datasets [ 5,8]; etc.\n\nIn their work, Feng and colleagues [ 5] also\nanalyzed the deep syntax of the data using the principles of probabilistic context-free\ngrammar (PCFG), independently and in combination with the aforementioned methods. In\naddition to the above, the LIWC tool was also used for deception detection by leveraging\ninsight into the psycho-linguistic characteristics of the analyzed text. In this paper, several different machine learning models were used and their perfor-\nmance in differentiating deceptive and true text was tested. Two sets of data were collected\nusing the crowdsourcing platform Clickworker and the survey tool Qualtrics Survey.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p2_c5",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 2,
      "chunk_index": 5,
      "section": "1. Introduction",
      "char_start": 7222,
      "char_end": 8359,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p2_c4",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p2_c6",
      "semantic_coherence": 0.7238779664039612,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "For\ndata processing, n-grams, LIWC, and a combination of the two approaches were used. A\nselection of essential features for each model was carried out over the LIWC dimensions\nusing the WEKA tool in order to obtain subsets of features with which the models provide\nthe highest accuracy. Since two distinct datasets on two topics were created, they were\nused to train their own separate models. These models were then tested on their own data,\nbut they were also cross-tested on the data they were not trained with.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p2_c6",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 2,
      "chunk_index": 6,
      "section": "1. Introduction",
      "char_start": 8359,
      "char_end": 8874,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p2_c5",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p2_c7",
      "semantic_coherence": 0.3040304183959961,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Furthermore, both\ndatasets were combined to create a joint dataset on deceptive text, which was then used to\ntest both models. The performance of the models was examined in order to gain insight\ninto the possibility of model generalization and its applicability to different datasets. This\ngave insight into model parameters with the highest accuracy in predicting deception. The\npossibility of deception detection using natural language processing methods was also\ntested in order to ascertain which methods give the best performance in a generalization or\napplicability to other datasets than the ones they were trained upon; additionally, it was to\ndecide which method gives the best predictions in general.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p3_c7",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 3,
      "chunk_index": 7,
      "section": "1. Introduction",
      "char_start": 8875,
      "char_end": 9585,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p3_c6",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p3_c8",
      "semantic_coherence": 0.37797054648399353,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "To deceive means \u201cto lie, mislead or otherwise hide or distort the truth\u201d. Although\nthe term lie is often regarded similar to the term deception, there is a certain distinction\nbetween the two. Lying is just one of many forms of deception, which does not only\nmean uttering an untrue claim, but also manifests itself in \u201comitting the truth or more\ncomplicated covering-up the truth\u201d commonly with the intention to mislead or deceive\nsomeone [8].\n\nThe traditional de\ufb01nition states that to deceive means \u201cto cause to believe\nwhat is false\u201d, which naturally leads to the question of whether this includes the case of\nmistakenly or unintentionally deceiving another person, on which many have con\ufb02icting\nviews [9]. However, the majority believes that lying and deception necessarily manifest\nthemselves with intent, so the de\ufb01nition itself has been modi\ufb01ed to de\ufb01ne deception as\n\u201cintentionally causing to have a false belief that is known or believed to be false\u201d [ 9]. The de\ufb01nition also implies the success of the act of deception since otherwise, the goal\nof creating a \u201cfalse belief\u201d in another person is not ful\ufb01lled. This is precisely one of the\ndifferences between deception and lying, where lying does not necessarily mean convincing\nanother person was done successfully; it only refers to \u201cmaking a false statement with the\nintention of deceiving\u201d.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p4_c9",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 4,
      "chunk_index": 9,
      "section": "2.1. Lie and Deception",
      "char_start": 14432,
      "char_end": 15785,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p4_c8",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p4_c10",
      "semantic_coherence": 0.08438625931739807,
      "has_citations": true,
      "topic_cluster": 2
    },
    {
      "text": "Algorithms 2023, 16, 221 3 of 34\nA slightly broader, generally accepted de\ufb01nition of lying is the following: \u201cA lie is a\nstatement made by one who does not believe it with the intention that someone else shall be\nled to believe it\u201d [10].\n\nAccording to the stated statement, four main conditions are de\ufb01ned\nthat must be ful\ufb01lled in order for a certain statement to be identi\ufb01ed as a lie: the person\nshould make the statement (statement condition), the person making the statement should\nbelieve that the statement is false (untruthfulness condition), an untrue statement must be\ngiven to another person\u2014the recipient of the statement (addressee condition)\u2014and lastly,\nthe person making the statement must lie with the intention to convince the recipient of the\nstatement to believe the untruthful statement to be true (intention to deceive the addressee\ncondition) [9].",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p4_c10",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 4,
      "chunk_index": 10,
      "section": "2.1. Lie and Deception",
      "char_start": 15786,
      "char_end": 16654,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p4_c9",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p4_c11",
      "semantic_coherence": 0.6810396909713745,
      "has_citations": true,
      "topic_cluster": -1
    },
    {
      "text": "Here, too, there are debates regarding the very de\ufb01nition of \u201clying\u201d, and they\nconcern the intention with which a person lies; these debates lead to two opposing groups,\nnamely the theories of Deceptionism and Non-Deceptionism. The former group believes\nthat intention is necessary for lying while the latter believe the opposite. The theory\nof Deceptionism is further divided into Simple Deceptionism, Complex Deceptionism,\nand Moral Deceptionism. Simple Deceptionists believe that for lying, it is necessary to\nmake an untrue statement with the intention of deceiving another person while Complex\nDeceptionists additionally believe that the intention to deceive must be manifested in the\nform of a breach of trust or belief. Moral Deceptionism state that lying not only requires\nmaking an untrue statement with the intention of deceiving, but also violating the moral\nrights of another person. On the other hand, the theory of Non-Deception dictates that\nlying is a necessary and suf\ufb01cient condition to make an untrue claim, and it is further\ndivided into the theory of Simple Non-Deceptionism and Complex Non-Deceptionism [9].",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p4_c11",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 4,
      "chunk_index": 11,
      "section": "2.1. Lie and Deception",
      "char_start": 16654,
      "char_end": 17783,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p4_c10",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p4_c12",
      "semantic_coherence": 0.6776462197303772,
      "has_citations": true,
      "topic_cluster": 2
    },
    {
      "text": "Unlike lying, deception itself does not only involve verbal communication, but also\nmanifests itself through various non-verbal signs, such as leading another person to the\nwrong conclusion by certain behavior, using non-linguistic conventional symbols or sym-\nbols that determine similarity (icons), etc. It is also possible to deceive someone with\nan exclamation, a question, a command, an omission of an important statement, and\neven silence [9]. This leads to the conclusion that the statement condition does not apply to deception.\n\nIn the same way, the condition of untruthfulness does not apply because it is possible\nto deceive someone by making a true statement that intentionally implies a falsehood\n(e.g., using true statements to create a false belief). Sarcasm and irony are also weapons of\ndeception that violate the condition of untruthfulness, given that a person states an obvious\ntruth with the intention of leading another person to the opposite (false) conclusion.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p5_c12",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 5,
      "chunk_index": 12,
      "section": "2.1. Lie and Deception",
      "char_start": 17784,
      "char_end": 18768,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p5_c11",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p5_c13",
      "semantic_coherence": 0.6847091317176819,
      "has_citations": true,
      "topic_cluster": -1
    },
    {
      "text": "In\naddition, the stated de\ufb01nition of deception [9] does not de\ufb01ne the subject of deception as a\n\u201cperson\u201d, but refers to anything that is capable of having beliefs, like infants or animals,\nwhich violates the addressee condition, which is constant in the de\ufb01nition of a lie. This\ncondition is also violated, for example, in the case when a person is being eavesdropped\non, which they are aware of, and uses this fact to deceive the eavesdroppers (e.g., deceiving\nsecret service agents) [9]. An interesting case of deception, which is not manifested in lying,\nis when a person, by deliberately avoiding or not accepting the truth, deceives himself\nor herself.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p5_c13",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 5,
      "chunk_index": 13,
      "section": "2.1. Lie and Deception",
      "char_start": 18768,
      "char_end": 19425,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p5_c12",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p5_c14",
      "semantic_coherence": 0.5842883586883545,
      "has_citations": true,
      "topic_cluster": 2
    },
    {
      "text": "Deception happens every day and in all forms of interaction, through face-to-face\ncommunication or through certain media such as mobile phones, computers, television,\netc. A common assumption is that detecting deception in face-to-face interaction is an\neasier task given that a person has much more information at their disposal, unlike verbal\n\u201conline\u201d communication, which lacks non-verbal signs such as gestures, body posture, facial\nexpressions, etc.\n\nIn addition, as mentioned in previous research in asynchronous computer-\nmediated communication, the sender has more time, which makes it \u201ceasier for senders\nto construct and/or harder for receivers to detect relative to face to-face interactions\u201d [3]. Nevertheless, in a study conducted on a group of people who participated in face-to-\nface interactions and computer-mediated communication, it was determined that human\n\nAlgorithms 2023, 16, 221 4 of 34\nperformance in detecting deception in computer-mediated communication exceeded that\nin face-to-face interaction and that the truth bias and deception rate in both cases did not\ndifferentiate [11].",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p6_c14",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 6,
      "chunk_index": 14,
      "section": "2.2. Deception in Computer-Mediated Communication (CMC)",
      "char_start": 24466,
      "char_end": 25574,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p6_c13",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p6_c15",
      "semantic_coherence": 0.47335514426231384,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "However, human prediction accuracy still did not exceed chance. Zhou and colleagues [2] believe that the sender in CMC distances himself from the\nmessage that \u201creduces their accountability and responsibility for what they say, and an\nindication of negative feelings associated with the act of deceiving\u201d. Likewise, one of\nthe important indicators of why people are better at detecting deception via computers\nis precisely the fact that they lack certain information about the other person, so they are\nmuch more suspicious and will suspect deception sooner.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p6_c15",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 6,
      "chunk_index": 15,
      "section": "2.2. Deception in Computer-Mediated Communication (CMC)",
      "char_start": 25574,
      "char_end": 26131,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p6_c14",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p6_c16",
      "semantic_coherence": 0.6204829216003418,
      "has_citations": true,
      "topic_cluster": -1
    },
    {
      "text": "Research in CMC examines the in\ufb02uence of Linguistic Style Matching (LSM) and\nInterpersonal Deception Theory (IDT) on the linguistic characteristics of conversations\nduring honest and fake conversations [ 2,12]. The LSM theory explains how people in\na conversation adapt each other\u2019s linguistic style to match their partner\u2019s. According\nto the LSM theory, deception in a conversation can be detected by analyzing the verbal\ncharacteristics of the interlocutor (who is unaware of the deception) and not exclusively\nthose of the speaker (who is lying) given that their linguistic styles match [ 12].\n\nIn the\nstudy conducted during synchronous computer-mediated communication, correlation was\nrecorded in the linguistic style of the interlocutors. More precisely, the correlation was\nachieved when using \ufb01rst, second, and third person pronouns and negative emotions. An\ninteresting conclusion was that the linguistic pro\ufb01les of both interlocutors coincided to a\ngreater extent during false communication compared to true communication, especially\nin a case when the speaker was motivated to lie [12].",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p6_c16",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 6,
      "chunk_index": 16,
      "section": "2.2. Deception in Computer-Mediated Communication (CMC)",
      "char_start": 26132,
      "char_end": 27228,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p6_c15",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p6_c17",
      "semantic_coherence": 0.4726257622241974,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "There is a possibility that speakers\ndeliberately use LSM when trying to deceive in order to appear more credible to the partner,\nwhich is what the IDT theory deals with. IDT studies the context of the speaker (who is\nlying) and the interlocutor (who is not aware of the deception) and the changes in their\nlinguistic styles through honest or false communication, with the difference that it under-\nstands these changes as strategic behavior that the speaker uses to facilitate the deception\nprocess [13].\n\n\u201cDeceivers will display strategic modi\ufb01cations of behavior in response to a\nreceiver\u2019s suspicions, but may also display non-strategic (inadvertent) behavior, or leakage\ncues, indicating that deception is occurring \u201d [2]. On the other hand, interlocutors in the\ncase of non-strategic behavior may become suspicious and ask more questions, thus forcing\nthe speaker to change his linguistic style and adapt to the interlocutor.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p7_c17",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 7,
      "chunk_index": 17,
      "section": "2.2. Deception in Computer-Mediated Communication (CMC)",
      "char_start": 27228,
      "char_end": 28159,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p7_c16",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p7_c18",
      "semantic_coherence": 0.6820109486579895,
      "has_citations": true,
      "topic_cluster": -1
    },
    {
      "text": "So far, the best known method of detecting deception precisely is the use of a poly-\ngraph, which provides insight into a series of autonomous and somatic psycho-physiological\nactivities that are invisible to the human eye, but which strongly signal deception. The\npolygraph relies on the analysis of peripheral activities related to emotions and excitement\nwhile the traditional measures used are most often of a cardiovascular nature (i.e., changes\nin heart rate), electrodermal (changes in the electrical properties of the skin), and respi-\nratory (i.e., rapid and uneven breathing) [ 14].\n\nAlthough a polygraph is one of the more\naccurate methods, it still provides rather limited insight into complex brain processes that\nmay hide deeper and more precise indicators of deception, so it is not surprising that this\narea of research is on the rise. There are other methods of detecting fraud from behavior\nthrough observing a person using the usual human senses without physical contact; by\ninterpreting subtle signals in behavior by analyzing gestures, linguistics, tone of voice, and\nhandwriting; etc. [14].",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p7_c18",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 7,
      "chunk_index": 18,
      "section": "2.2. Deception in Computer-Mediated Communication (CMC)",
      "char_start": 28159,
      "char_end": 29271,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p7_c17",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p7_c19",
      "semantic_coherence": 0.3538324236869812,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "Although some of these approaches give quite satisfactory results,\nsuch as the polygraph, their limitation lies in their inapplicability to computer-mediated\ncommunication. It is for this reason that there exists an increasing interest in methods for\nanalyzing the syntactic and semantic properties of written text and \ufb01nding indicators of\ndeception in various forms of digital interaction.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p7_c19",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 7,
      "chunk_index": 19,
      "section": "2.2. Deception in Computer-Mediated Communication (CMC)",
      "char_start": 29271,
      "char_end": 29661,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p7_c18",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p7_c20",
      "semantic_coherence": 0.6901629567146301,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Natural language processing (NLP) is a multidisciplinary \ufb01eld of linguistics, computer\nscience, and arti\ufb01cial intelligence that focuses on the processing and analysis of large\n\nAlgorithms 2023, 16, 221 5 of 34\namounts of natural language data with the aim of developing software that will understand\nthe content and context of text and speech. By analyzing different aspects of language,\nsuch as syntax, semantics, pragmatics, and morphology, machine learning models learn\nthe rules used to solve given problems.\n\nNLP is commonly applied for \ufb01ltering spam and\ngenerally classifying it with search engines for automatic text correction, sentiment analysis\nof different products, classi\ufb01cation of customer feedback, and automation of customer\nsupport as part of virtual assistants, but it is also for many other tasks, including fraud\ndetection. NLP converts input text data into vectors of real numbers that machine learning\nmodels support. Some of the feature extraction methods for natural language analysis are:\n\u2022 The BOW (Bag of Words) approach extracts features from the text and represents\nthem as the occurrence of words used in the text.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p8_c21",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 8,
      "chunk_index": 21,
      "section": "2.3.1. Natural Language Processing",
      "char_start": 35062,
      "char_end": 36206,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p8_c20",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p8_c22",
      "semantic_coherence": 0.1086907610297203,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "The BOW analysis consists of the\nvocabulary of the used words and the measure of the occurrence of each individual\nword. It should be noted that the word structure and the order of words are ignored. \u2022 N-grams are strings of N symbols or words (tokens) in the analyzed document. Unlike\nBOW, n-grams preserve the order of tokens. Different types of n-grams are suitable for\nsolving different types of problems, so it is necessary to test the models on a wider\nrange of n-grams.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p9_c22",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 9,
      "chunk_index": 22,
      "section": "2.3.1. Natural Language Processing",
      "char_start": 36206,
      "char_end": 36682,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p9_c21",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p9_c23",
      "semantic_coherence": 0.3812223970890045,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "\u2022 TF-IDF (Term Frequency\u2014Inverse Document Frequency) speci\ufb01es how important a\ncertain word is for the analyzed document, but it is not as naive as BOW. In BOW,\nfrequent words can easily dominate while less frequently used words that carry much\nmore information lose their importance. TF-IDF, in addition to the frequency of word\noccurrence in the current document, also records the inverse frequency; that is, for\neach word, it calculates how rarely it appears in all documents.\n\nBy combining the\nabove, TF-IDF solves the problem of a dominance of frequent words in relation to less\nfrequent but more important ones. \u2022 POS (Part of Speech) are categories of words with similar grammatical properties,\nsuch as nouns, adjectives, verbs, etc. This type of analysis assigns a corresponding\ncategory to each word. \u2022 Lemming and stemming are text normalization techniques used in natural language\nprocessing, and their main function is to reduce words to their canonical or root form. Lemming is a canonical dictionary-based approach and, unlike rooting, takes into\naccount the meaning of words.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p9_c23",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 9,
      "chunk_index": 23,
      "section": "2.3.1. Natural Language Processing",
      "char_start": 36683,
      "char_end": 37772,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p9_c22",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p9_c24",
      "semantic_coherence": 0.5031806826591492,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "Stemming is based on rules and is simpler to implement\nand is faster because it does not consider the context when shortening words, which is\nwhy it also does not give as good of a prediction accuracy as lemmatization. \u2022 Stop-words are words of a certain language that do not contribute much information to\nthe sentence, and they have a highly frequent occurrence in the text. This is why they\nare often removed from the text when classifying or grouping using machine learning\nmodels. Removing them can greatly increase the prediction accuracy and reduce\nmodel training and testing time, but they should be chosen carefully to preserve the\ntext meaning.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p9_c24",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 9,
      "chunk_index": 24,
      "section": "2.3.1. Natural Language Processing",
      "char_start": 37772,
      "char_end": 38426,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p9_c23",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p9_c25",
      "semantic_coherence": 0.5546231269836426,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "Machine learning (ML) is a \ufb01eld of arti\ufb01cial intelligence that deals with the study\nof methods that independently learn from data and use it for the purpose of improving\nperformance when solving given problems. The models are built on the training data\nand used to make predictions. Today, machine learning is used across all \ufb01elds, such as\nmedicine, computer vision, text classi\ufb01cation/grouping, speech recognition, etc. The base\nsteps of machine learning are data collection, data preprocessing, model selection, model\ntraining and model evaluation, parameter tuning, and prediction.\n\nThe process of data\ncollection and preprocessing is of great importance given that the models rely on the given\ndata when making decisions. Due to the general lack of labeled data, these are also the\nmost dif\ufb01cult tasks. Machine learning models are one of the methods that can be used in deception de-\ntection. Prior to the actual model training, the data is \ufb01ltered, cleaned, and analyzed\n\nAlgorithms 2023, 16, 221 6 of 34\nusing natural language processing methods, and then, it is transformed into a form that\nis acceptable to a certain machine learning model.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p11_c25",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 11,
      "chunk_index": 25,
      "section": "2.3.2. Machine Learning",
      "char_start": 41862,
      "char_end": 43011,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p11_c24",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p11_c26",
      "semantic_coherence": 0.30603328347206116,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "The models used in this are based on\nlogistic regression, na\u00efve Bayes, SVM (Support Vector Machine), and Random forest:\n\u2022 Logistic regression is a statistical model of machine learning that is used for classi\ufb01ca-\ntion and belongs to supervised machine learning techniques. It outputs a probabilistic\nvalue between 0 and 1. \u2022 SVM is a machine learning model that is often used for classi\ufb01cation, but it is also\nused for regression. It belongs to supervised machine learning techniques. Its task is\nto separate N-dimensional data into classes by selecting the best decision boundary\n(discriminant function).\n\n\u2022 Naive Bayes also belongs to supervised learning techniques. It is based on Bayes theo-\nrem, which naively assumes that the value of a certain variable/feature is independent\nof other variables/features. \u2022 The Random Forest model is used for the classi\ufb01cation and regression problem. It is\nbased on the construction of a large number of decision trees, each of which makes\ndecisions on the outcome of the prediction.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p11_c26",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 11,
      "chunk_index": 26,
      "section": "2.3.2. Machine Learning",
      "char_start": 43011,
      "char_end": 44035,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p11_c25",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p11_c27",
      "semantic_coherence": 0.6272462606430054,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "In the case of classi\ufb01cation, the prediction\nwith the majority of votes is selected while in the case of regression, the average value\nof all predictions is taken as the output of the model. K-fold cross validation is used as a method of evaluating the obtained machine learning\nmodels. The parameter k determines the number of groups into which a given data set\nis divided to separate the training from the testing data. Speci\ufb01cally, one set is taken to\ntest the model while the other k-1 datasets are used to train the model.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p11_c27",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 11,
      "chunk_index": 27,
      "section": "2.3.2. Machine Learning",
      "char_start": 44035,
      "char_end": 44562,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p11_c26",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p11_c28",
      "semantic_coherence": 0.5060188174247742,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "The accuracy of\nthe model is calculated by taking the average value of the model\u2019s prediction through k\niterations. This method provides a less optimistic but less biased assessment of model\nperformance than other methods.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p11_c28",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 11,
      "chunk_index": 28,
      "section": "2.3.2. Machine Learning",
      "char_start": 44563,
      "char_end": 44785,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p11_c27",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p11_c29",
      "semantic_coherence": 0.4923997223377228,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "LIWC (Linguistic Inquiry and Word Count) is a software for text analysis designed\nfor the purpose of studying natural language. It consists of two key components: the\nword processing component and the LIWC dictionary. The dictionary forms the core of the\napplication itself, as it connects psychosocial with linguistic constructs and consists of over\n12,000 words, root words, and phrases. Groups of words from the LIWC vocabulary that\nspecify a particular domain are referred to as \u201ccategories\u201d or \u201cdimensions\u201d. Each LIWC\nentry can belong to several LIWC categories and are mostly arranged hierarchically.\n\nOrigi-\nnally, the categories were cognitive and emotional while with the increasing understanding\nof the psychology of verbal behavior, the number and depth of categories increased [15]. LIWC receives text records in various formats of input, which it then sequentially analyzes\nand compares with the dictionary. The software counts the words in a given text and\ncalculates the percentage of total words represented in all LIWC subcategories. The latest version of the LIWC-22 offers some improvements over previous versions.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p12_c29",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 12,
      "chunk_index": 29,
      "section": "2.3.3. Linguistic Inquiry and Word Count (LIWC)",
      "char_start": 47758,
      "char_end": 48891,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p12_c28",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p12_c30",
      "semantic_coherence": 0.14384426176548004,
      "has_citations": true,
      "topic_cluster": 3
    },
    {
      "text": "The dictionary has been upgraded to handle numbers, punctuation marks, short phrases,\nand regular expressions in order to extend the use of LIWC (for example, to the analysis\nof content from social networks (Facebook, Twitter, Instagram, Snapchat) where such\nlinguistic style is often present). In LIWC-22, the psychometric abilities of the dictionary\nwere improved, and several new categories were added [16]. The construction of the LIWC began with the intention of analyzing verbal speech to\nextract psychological processes described through the use of style words and the content\nof what is written or spoken (content words).\n\nIt was soon concluded that these are quite\ndifferent categories with different psychometric properties. Style words, which are also\ncalled function words, \u201cmake up only 0.05% of the total set of words in the English language,\nand are contained in a total of 55% of all words that we hear, speak or read\u201d. They represent\nthe way people communicate and offer a greater insight into the psychosocial aspect of\nspeech compared to content words, which describe only the content of communication [17].",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p13_c30",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 13,
      "chunk_index": 30,
      "section": "2.3.3. Linguistic Inquiry and Word Count (LIWC)",
      "char_start": 48891,
      "char_end": 50017,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p13_c29",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p13_c31",
      "semantic_coherence": 0.7467666864395142,
      "has_citations": true,
      "topic_cluster": 3
    },
    {
      "text": "Algorithms 2023, 16, 221 7 of 34\nFunction words have proven to be very successful in the analysis of the emotional\nand biological state, status, sincerity, and individual differences; therefore, the emphasis\nis placed precisely on their deeper analysis in order to give insight into psychological\nprocesses that other methods of text analysis simply cannot detect. On the other hand, LIWC\nis a probabilistic system that does not take context into account linguistic constructs such\nas irony, sarcasm, and idioms; therefore, absolute conclusions about human psychology by\nusing only LIWC analysis cannot be drawn [17].\n\nResearch proves LIWC to be successful in detecting deception. \u201cDeceptive statements\ncompared with truthful ones are moderately descriptive, distanced from self, and more\nnegative\u201d [17]. Such a description is not surprising considering that more information\ncarries a greater risk of uncovering the truth. By analyzing deception in synchronous\ncomputer-mediated communication, it was shown how the linguistic style of the sender\n(who lies) and the receiver (who is unaware of the deception) changes. Both respondents\nwere using more words overall (especially sensory) and fewer 1st person pronouns during\ndeception compared to honest interaction [ 2,3].",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p13_c31",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 13,
      "chunk_index": 31,
      "section": "2.3.3. Linguistic Inquiry and Word Count (LIWC)",
      "char_start": 50018,
      "char_end": 51289,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p13_c30",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p13_c32",
      "semantic_coherence": 0.6127728223800659,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "Thus, it is obvious that linguistic style\nhides patterns that are speci\ufb01c to true and false communication, which can to some extent\nbe successfully detected using the LIWC approach. Since LIWC software lacks context\nanalysis, it is recommended to combine it with other natural language processing methods. Based on previous research, LIWC, together with an analysis combined with n-grams,\nachieved satisfactory results [ 4,5]. Given the large number of dimensions that LIWC\npossesses, a selection of important features needs to be done in order to prevent over\ufb01tting\nand maximize the performance of the machine learning model.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p14_c32",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 14,
      "chunk_index": 32,
      "section": "2.3.3. Linguistic Inquiry and Word Count (LIWC)",
      "char_start": 51289,
      "char_end": 51915,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p14_c31",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p14_c33",
      "semantic_coherence": 0.5285910367965698,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Weka (Waikato Environment for Knowledge Analysis) is a software that contains tools\nfor visualization, data analysis, and predictive modeling. Implemented within Waikato\nUniversity, New Zealand, Weka was originally a tool for analyzing data from the agricul-\ntural domain but is used today in various \ufb01elds of research, especially for educational\npurposes. Weka provides support for certain data mining methods, data preprocessing,\nclustering, regression, classi\ufb01cation, data visualization, and feature selection.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p15_c33",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 15,
      "chunk_index": 33,
      "section": "2.3.4. WEKA",
      "char_start": 56170,
      "char_end": 56683,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p15_c32",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p15_c34",
      "semantic_coherence": 0.3215375542640686,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "A notable work in deception detection during public trials included analyzing verbal\nand non-verbal behavior of suspects and witnesses [6]. Videos of witnesses and suspects\nwere collected during testimonies on public trials and were used to build a reliable machine\nlearning model that can distinguish truth from lies by analyzing verbal and non-verbal\ncharacteristics and thus provide assistance in making key decisions in the judiciary. The\nmodels based on n-grams were tested individually on different sets of verbal and non-\nverbal features and in combination.\n\nA further analysis of a subset of features revealed\nthat the model gives better results when it is trained on non-verbal features, mostly by\nanalyzing facial expressions, which are then followed by unigrams. Human performance\nin deception detection was tested by analyzing text, sound, noiseless video, and video with\nsound, which achieved worse results compared to the used machine learning models. In another study, three different approaches were used when analyzing data on false\nand true positive hotel reviews [4].",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p15_c34",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 15,
      "chunk_index": 34,
      "section": "2.4. Related Work on Deception Detection",
      "char_start": 57222,
      "char_end": 58308,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p15_c33",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p15_c35",
      "semantic_coherence": 0.17420165240764618,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "Truthful reviews were collected from the TripAdvisor\nplatform while fake reviews were generated using the Amazon Mechanical Turk platform,\ncontaining opinions created with the intention of deceiving another person. The text was\nanalyzed using POS and n-gram techniques and psycho-linguistic analysis using LIWC\nsoftware. The models were tested individually and in combination, and only those with\nthe best performance were selected. Data processing led to the conclusion that bigrams\ngenerally give the best results while the combination of bigrams and psycho-linguistic\nanalysis gives a slightly better result. In addition, all tested methods outperform humans\non the same dataset.\n\nIn the same paper, by comparing the features obtained using POS\nanalysis, LIWC, and a combination of analysis with n-grams and LIWC, it was noted that\n\nAlgorithms 2023, 16, 221 8 of 34\npeople hardly fake spatial information. Through the analysis of POS, the authors came\nto the conclusion that some of the language structures that are most often used in honest\n(informative) reviews are nouns, adjectives, prepositions, conjunctions, and verbs while\nfake (imaginary) reviews mostly use verbs, adverbs, and pronouns.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p16_c35",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 16,
      "chunk_index": 35,
      "section": "2.4. Related Work on Deception Detection",
      "char_start": 58308,
      "char_end": 59507,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p16_c34",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p16_c36",
      "semantic_coherence": 0.541935920715332,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "Verbs and adverbs\nare common features in both types of reviews, but with an important difference: in the\ninformative text, the past participle is mostly used while in the imaginary text, a much\nwider range of different verb tenses is used. Somewhat better results on the same TripAdvisor hotel reviews dataset were achieved\nwith a deep syntax analysis (that is, by using features derived from the analysis of parsed\ntrees obtained from probabilistic context-free grammar (PCFG)) [ 5].\n\nThe mentioned ap-\nproach is combined with shallow syntax (i.e., POS tags) but still gives the best results in\ncombination with n-grams, proving that the analysis of deep syntax offers information not\npresent in the learned POS features and that it can serve as a more reliable method when\ndetecting deception. The models were tested on 4 different datasets, from the domain of\nfake reviews from the TripAdvisor and Yelp platforms and sets of essays collected through\nAmazon Mechanical Turk on the topics \u201cabortion\u201d, \u201cbest friend\u201d, and \u201cdeath penalty\u201d.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p16_c36",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 16,
      "chunk_index": 36,
      "section": "2.4. Related Work on Deception Detection",
      "char_start": 59507,
      "char_end": 60544,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p16_c35",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p16_c37",
      "semantic_coherence": 0.6995927691459656,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "The same set of essays was analyzed using LIWC software in the work of Mihalcea\n& Strapparava [8], showing slightly less favorable results. In fake essays, references to\nother people (\u201cyou\u201d, \u201cothers\u201d, \u201cpeople\u201d) and words related to certainty are mostly present\nwhile in true essays the person con\ufb01dently connects with the statements made using more\nreferences on oneself (\u201cI\u201d, \u201cfriends\u201d, \u201cself\u201d) and presents several attitudes based on belief\n(\u201cthink\u201d, \u201cfeel\u201d, believe\u201d) [8]. In addition to TripAdvisor, the analysis of fake reviews was\nalso conducted on social platforms, such as Twitter [18].",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p16_c37",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 16,
      "chunk_index": 37,
      "section": "2.4. Related Work on Deception Detection",
      "char_start": 60544,
      "char_end": 61138,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p16_c36",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p16_c38",
      "semantic_coherence": 0.4550079107284546,
      "has_citations": true,
      "topic_cluster": -1
    },
    {
      "text": "The answer to the question of which machine learning model best differentiates the\ntrue from false data is not easy to come by given that each model works speci\ufb01cally\nconcerning the speci\ufb01c problem and dataset on which it is trained and tested, which is also\noften conceptualized by the speci\ufb01c context by the researchers in the area [19]. Zhou and\ncolleagues in their research [2] deal with the deception detection in asynchronous computer-\nmediated communication by examining four machine learning models on data collected\nthrough two experimental studies.\n\nThe research is based on the detection of deception from\nthe point of view of interaction, not individual analysis. A similar study was conducted in\norder to understand changes in the linguistic behavior of people participating in a deceptive\nor truthful discussion during synchronous computer-mediated communication [3]. The\nresearch was conducted by dividing respondents into groups of two people who were\ngiven the task of talking to each other via e-mail and, thus, getting to know each other.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p16_c38",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 16,
      "chunk_index": 38,
      "section": "2.4. Related Work on Deception Detection",
      "char_start": 61139,
      "char_end": 62196,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p16_c37",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p16_c39",
      "semantic_coherence": 0.43346381187438965,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "All respondents were given several topics to discuss in such a way that one respondent\n(sender) was randomly selected from each group and assigned a task to deceive the person\n(recipient) by giving a false opinion on two of the \ufb01ve given topics. Likewise, the importance\nof motivation in deception was examined in such a way that the senders were randomly\nassigned the additional task of being highly or lowly motivated while lying.\n\nThe data were\nanalyzed using the LIWC software in order to extract statistically signi\ufb01cant features related\nto false or true communication and to test the hypotheses based on LSM and IDT. Based on\nLSM and IDT, changes in the behavior of the interlocutor (recipient) were also analyzed in\norder to examine whether deception can be detected from changes in his behavior. It was\nalso investigated the extent to which motivation changes the linguistic style of a person\nwho lies/deceives in synchronous computer-mediated communication.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p17_c39",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 17,
      "chunk_index": 39,
      "section": "2.4. Related Work on Deception Detection",
      "char_start": 62196,
      "char_end": 63162,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p17_c38",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p17_c40",
      "semantic_coherence": 0.7614954710006714,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "The results show\nthat senders statistically use more words during deception, more references to other people\nand less to themselves, and more emotional words. Motivated senders avoid causal terms\nlike \u201cbecause\u201d, \u201chence\u201d, and \u201ceffect\u201d while unmotivated ones use more simple negations. According to LSM and IDT theories, recipients use more words and ask more questions\nduring deception, especially when the sender is not motivated [3]. Previous research focused on detecting deception using and combining a few machine\nlearning algorithms and models, mostly Na\u00efve Bayes and SVM and to a certain extent\nanalysis on a small range of n-grams.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p17_c40",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 17,
      "chunk_index": 40,
      "section": "2.4. Related Work on Deception Detection",
      "char_start": 63162,
      "char_end": 63800,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p17_c39",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p17_c41",
      "semantic_coherence": 0.7494878172874451,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "This paper extends the state of the art by employing\n\nAlgorithms 2023, 16, 221 9 of 34\na wider set of machine learning algorithms and models and uses wider combinations of\nn-grams. Speci\ufb01cally, prior research was exclusively done on bigrams while this study uses\nn-grams ranging from unigrams to trigrams. Such an extended set of models, algorithms,\nand n-grams was further combined with a speci\ufb01c analysis using LIWC, examining its\noperation on different datasets.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p18_c41",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 18,
      "chunk_index": 41,
      "section": "2.4. Related Work on Deception Detection",
      "char_start": 63801,
      "char_end": 64266,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p18_c40",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p18_c42",
      "semantic_coherence": 0.3249199390411377,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "The study contributes to the discussion on models\u2019 ap-\nplicability in terms of generalization, where n-grams were found to be the most robust\nmethod for deception detection and the analysis using LIWC the least \ufb02exible. The best\nopportunity for generalization were noted with the use of combinations of n-grams and\nanalysis using LIWC. The literature review presented in this chapter is summarized in a table form as part\nof Appendix A.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p18_c42",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 18,
      "chunk_index": 42,
      "section": "2.4. Related Work on Deception Detection",
      "char_start": 64267,
      "char_end": 64703,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p18_c41",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p18_c43",
      "semantic_coherence": 0.36559048295021057,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "Given that human capabilities for detecting lies are very limited, machine learning\nmodels that have proven to be the best tool for predicting deception in computer-mediated\ncommunication were used. However, the \ufb01rst problem that arose in model use was the lack\nof labeled input data with truthful and deceptive statements. In a majority of prior work,\ndata were collected using crowdsourcing platforms [4,5,8] while other research was done\non \u201creal\u201d data collected through social experiments [2,3] by analyzing public trials [6,7] or\nby some other method.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p21_c44",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 21,
      "chunk_index": 44,
      "section": "3.1. Problem Statement: Creating a Reliable Deception Detector",
      "char_start": 72298,
      "char_end": 72854,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p21_c43",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p21_c45",
      "semantic_coherence": 0.11209327727556229,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "Another problem was the choice of text processing methods and machine learning\nmodels that gave the best prediction. By analyzing the choice of machine learning models in\nprevious research, it could be concluded that Naive Bayes and SVM classi\ufb01ers have proven\nto be very successful in solving this type of a problem [4,5,8], so they represent the choice\nof methods in this study as well. Logistic regression [2] and Random forest [6] were used\nsomewhat less often, but they achieved acceptable results, so they were also included in the\nset of models for this study as additional methods.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p21_c45",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 21,
      "chunk_index": 45,
      "section": "3.1. Problem Statement: Creating a Reliable Deception Detector",
      "char_start": 72855,
      "char_end": 73443,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p21_c44",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p21_c46",
      "semantic_coherence": 0.46677839756011963,
      "has_citations": true,
      "topic_cluster": -1
    },
    {
      "text": "Due to the performance of computational\nlinguistics methods used in previous research, in this study, data were analyzed using\nn-grams as opposed to POS analysis because it gave more precise results [ 4,5]. Another\nreason for choosing n-gram analysis lay in the fact that it worked very well in combination\nwith other methods. In a previous study, the highest precision was achieved by combining\nn-grams and deep syntax analysis when analyzing four different datasets [5] while or by\ncombining LIWC analysis and bigrams [ 4].\n\nLIWC has also served as a relatively good\ndeception detector in other studies [3] because it provided insight into the psychological\u2013\nlexical characteristics of words, which was not given with n-gram analysis. To increase\nmodel reliability, important LIWC features needed to be selected since the use of redundant\nfeatures could greatly reduce the predictive power of machine learning models. Based on the aforementioned research, in this paper, it was decided to use a combina-\ntion of analysis with n-grams and LIWC with individual approaches as veri\ufb01cation.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p22_c46",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 22,
      "chunk_index": 46,
      "section": "3.1. Problem Statement: Creating a Reliable Deception Detector",
      "char_start": 73444,
      "char_end": 74531,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p22_c45",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p22_c47",
      "semantic_coherence": 0.41561123728752136,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "Deep\nsyntax analysis was not examined in this paper because of the complexity of extracting\nimportant features from the data parsed using PCFG and the complex choice of production\nrules. The complete process of the methods applied in this study is given in Figure 1. Algorithms 2023, 16, 221 10 of 34\nAlgorithms 2023, 16, x FOR PEER REVIEW 10 of 31 \n \nproduction rules. The complete process of the methods applied in this study is given in \nFigure 1. Figure 1. Block diagram of the proposed solution.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p22_c47",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 22,
      "chunk_index": 47,
      "section": "3.1. Problem Statement: Creating a Reliable Deception Detector",
      "char_start": 74531,
      "char_end": 75031,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p22_c46",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p22_c48",
      "semantic_coherence": 0.4680047929286957,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "The Clickworker platform and the Qualtrics Survey were chosen in this study as tools \nto collect two separate datasets on the topics of \u201cClimate Change\u201d and \u201cCOVID-19\u201d. The \n\ufb01rst topic concerned the issue of climate change and read: \u201cWhat is your opinion on cli-\nmate change? What do you think caused it and how will it impact our lives in the future?\u201d. The second topic consisted of the question : \u201cHow did the COVID-19 pandemic impact \nyour life? Share some of the challenges or new experiences during the COVID-19 pan-\ndemic\u201d.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p23_c48",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 23,
      "chunk_index": 48,
      "section": "3.2. Data Collection and Cleansing",
      "char_start": 77902,
      "char_end": 78431,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p23_c47",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p23_c49",
      "semantic_coherence": 0.0002657138102222234,
      "has_citations": false,
      "topic_cluster": 4
    },
    {
      "text": "150 survey participants were selected, each of whom was paid $1.50 for complet-\ning a de\ufb01ned task, and the task itself was scheduled to last up to ten minutes. The selection \nof research participants was limited to residents of North America with English as a native \nlanguage. The gender and age of the participants were not mandated. For each topic, the \nrespondents had to answer ideally 4 to 5 sentences (the range was limited to 200\u2013500 char-\nacters). The time limit was set to 30 days from the start of the survey.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p23_c49",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 23,
      "chunk_index": 49,
      "section": "3.2. Data Collection and Cleansing",
      "char_start": 78433,
      "char_end": 78953,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p23_c48",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p23_c50",
      "semantic_coherence": 0.2395170032978058,
      "has_citations": false,
      "topic_cluster": 5
    },
    {
      "text": "All received responses had to be reviewed manually. Partial records were also taken \ninto account, in which the participants gave their opinion on only one of the two topics \nFigure 1. Block diagram of the proposed solution.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p23_c50",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 23,
      "chunk_index": 50,
      "section": "3.2. Data Collection and Cleansing",
      "char_start": 78955,
      "char_end": 79179,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p23_c49",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p23_c51",
      "semantic_coherence": 0.4276096522808075,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "The Clickworker platform and the Qualtrics Survey were chosen in this study as\ntools to collect two separate datasets on the topics of \u201cClimate Change\u201d and \u201cCOVID-19\u201d. The \ufb01rst topic concerned the issue of climate change and read: \u201cWhat is your opinion\non climate change? What do you think caused it and how will it impact our lives in the\nfuture?\u201d. The second topic consisted of the question: \u201cHow did the COVID-19 pandemic\nimpact your life? Share some of the challenges or new experiences during the COVID-19\npandemic\u201d.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p23_c51",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 23,
      "chunk_index": 51,
      "section": "3.2. Data Collection and Cleansing",
      "char_start": 80530,
      "char_end": 81051,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p23_c50",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p23_c52",
      "semantic_coherence": 0.258550763130188,
      "has_citations": false,
      "topic_cluster": 4
    },
    {
      "text": "150 survey participants were selected, each of whom was paid $1.50 for\ncompleting a de\ufb01ned task, and the task itself was scheduled to last up to ten minutes. The\nselection of research participants was limited to residents of North America with English\nas a native language. The gender and age of the participants were not mandated. For each\ntopic, the respondents had to answer ideally 4 to 5 sentences (the range was limited to\n200\u2013500 characters). The time limit was set to 30 days from the start of the survey. All received responses had to be reviewed manually.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p24_c52",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 24,
      "chunk_index": 52,
      "section": "3.2. Data Collection and Cleansing",
      "char_start": 81052,
      "char_end": 81617,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p24_c51",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p24_c53",
      "semantic_coherence": 0.28669795393943787,
      "has_citations": false,
      "topic_cluster": 5
    },
    {
      "text": "Partial records were also taken\ninto account, in which the participants gave their opinion on only one of the two topics\noffered. A total of 150 records were recorded, each record consisting of a true and a false\nanswer to both topics. An additional 18 partial records were collected, all of which were\nrelated to the \ufb01rst topic \u201cClimate change\u201d. Bot detection excluded 11 records from both\n\nAlgorithms 2023, 16, 221 11 of 34\ndatasets.\n\nIn the \u201cClimate change\u201d dataset (DS1), 25 records were manually labeled as\ninvalid due to inadequate response syntax or semantics while in the second \u201cCOVID-19\u201d\ndataset (DS2), 21 records were \ufb02agged as invalid. The \ufb01nal number of records in both\ndatasets was 132 (DS1) and 118 (DS2), respectively, which made a total of 264 and 236 true\nand false answers. For 58 records, minor syntactic errors were manually corrected, but the\nsemantics were maintained (Table 1). Complete overview of the data cleaning process is\ngiven in Figure 2. Table 1.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p24_c53",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 24,
      "chunk_index": 53,
      "section": "3.2. Data Collection and Cleansing",
      "char_start": 81618,
      "char_end": 82597,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p24_c52",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p24_c54",
      "semantic_coherence": 0.31767016649246216,
      "has_citations": false,
      "topic_cluster": 6
    },
    {
      "text": "Statistical overview of the records obtained through the Clickworker platform. DS1: Climate Change DS2: COVID-19\nThe initial number of records 168 150\nNumber of invalid records 25 21\nNumber of BOT detections 11 11\nThe \ufb01nal number of records\n(including corrected) 132 118\nAlgorithms 2023, 16, x FOR PEER REVIEW 11 of 31 \n \no\ufb00ered. A total of 150 records were recorded, each record consisting of a true and a false \nanswer to both topics. An additional 18 part ial records were collected, all of which were \nrelated to the \ufb01rst topic \u201cClimate change\u201d.\n\nBot detection excluded 11 records from both \ndatasets. In the \u201cClimate change\u201d dataset (DS1), 25 records were manually labeled as in-\nvalid due to inadequate response syntax or semantics while in the second \u201cCOVID-19\u201d \ndataset (DS2), 21 records were \ufb02agged as invalid. The \ufb01n al  n u m b e r  o f  r e c o r d s  i n  b o t h \ndatasets was 132 (DS1) and 118 (DS2), respectively, which made a total of 264 and 236 true \nand false answers.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p24_c54",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 24,
      "chunk_index": 54,
      "section": "3.2. Data Collection and Cleansing",
      "char_start": 82597,
      "char_end": 83585,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p24_c53",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p24_c55",
      "semantic_coherence": 0.9125314950942993,
      "has_citations": false,
      "topic_cluster": 6
    },
    {
      "text": "For 58 records, minor syntactic errors were manually corrected, but the \nsemantics were maintained (Table 1). Complete overview of the data cleaning process is \ngiven in Figure 2. Table 1. Statistical overview of the records obtained through the Clickworker platform.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p24_c55",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 24,
      "chunk_index": 55,
      "section": "3.2. Data Collection and Cleansing",
      "char_start": 83585,
      "char_end": 83852,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p24_c54",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p24_c56",
      "semantic_coherence": 0.5557606816291809,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "DS1: Climate Change DS2: COVID-19 \nThe initial number of records 168 150 \nNumber of invalid records 25 21 \nNumber of BOT detections 11 11 \nThe final number of records (including \ncorrected) 132 118 \nThe collected data were then pre-processed using natural language processing tech-\nniques and used to train and test machine learning models based on n-grams and psycho-\nlinguistic analysis using LIWC. Figure 2. Block diagram of the data pre-processing process. Figure 2. Block diagram of the data pre-processing process. The collected data were then pre-processed using natural language processing tech-\nniques and used to train and test machine learning models based on n-grams and psycho-\nlinguistic analysis using LIWC.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p24_c56",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 24,
      "chunk_index": 56,
      "section": "3.2. Data Collection and Cleansing",
      "char_start": 83856,
      "char_end": 84578,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p24_c55",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p24_c57",
      "semantic_coherence": 0.36950668692588806,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "Data collection was followed by cleaning and pre-processing so that the models that\nwould be subsequently applied analyze and predict more precisely. The \ufb01rst step consisted\n\nAlgorithms 2023, 16, 221 12 of 34\nof removing special characters and numbers from the text, followed by decontraction (i.e.,\nreduction of shortened and connected words to their long form). Word segmentation\n(tokenization) and lemmatization were performed to reduce the words to their normalized\nform. Stemming was not used because compared to lemmatization, it gives less favorable\nresults, which was expected considering that it does not rely on a dictionary.\n\nThe method\nof removing stop words was not used because they were an important factor in the pre-\ndiction of deception. By comparing the performance of the models tested using TF-IDF\nvectorization and the classic BOW approach, the TF-IDF technique was chosen due to more\naccurate prediction. Four different models were chosen: logistic regression, SVM, Naive Bayes, and Ran-\ndom Forest. The models were trained and tested individually and on combined data to\ngain insight into the possibility of mutual applicability of the models and the possibility\nof generalization by comparing the natural language processing approaches.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p26_c57",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 26,
      "chunk_index": 57,
      "section": "3.3. Applying Natural Language Processing (NLP) and Creating Models",
      "char_start": 88706,
      "char_end": 89967,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p26_c56",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p26_c58",
      "semantic_coherence": 0.49746420979499817,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "The models\nwere tested on a wider range and combination of n-grams and a varying number of features\nto identify the models with the highest prediction accuracy. The performance of models\nbased on n-grams, LIWC analysis, and a combination of the two mentioned approaches\nwere compared. All models were tested using 10-fold cross-validation. In the process, the\nmost important features were selected from the LIWC analysis using the WEKA tool. Feature Selection and LIWC\nThe models tested on all LIWC features did not give results comparable with those\nobtained by n-gram analysis, warranting them for feature selection.\n\nImportant LIWC\nfeatures for both datasets (DS1 and DS2) and the combined dataset (DS3) were selected\nwith the use of the WEKA tool. Two different feature selection approaches were used:\nAttribute Correlation Evaluation and Attribute Subset Evaluation. The following three\nclasses were selected:\nThe CorrelationAttributeEval class evaluated features by measuring the Pearson\ncorrelation coef\ufb01cient between the features and the class. The Ranker search method was\nused to rank the attributes according to their evaluations.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p26_c58",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 26,
      "chunk_index": 58,
      "section": "3.3. Applying Natural Language Processing (NLP) and Creating Models",
      "char_start": 89967,
      "char_end": 91108,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p26_c57",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p26_c59",
      "semantic_coherence": 0.39454853534698486,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "The CfsSubsetEval class belonged to attribute subset evaluators and was based on the\nevaluation of feature subset values with regard to the degree of redundancy among features\nand the predictive ability of each individual feature. This approach prefered subsets of\nfeatures that had a high correlation with the class and low correlation with each other. The\nsearch method used with CfsSubsetEval was BestFirst. The WrapperSubsetEval class also belonged to the attribute subset evaluators and\nused a learning scheme to evaluate feature sets and their accuracy by cross-validation. The\nsearch method used in conjunction with WrapperSubsetEval was also BestFirst.\n\nThe models tested on the selected features did not achieve the expected precision,\nso it was decided to narrow down the set of selected features. The models were tested\non different subsets of the selected set of features (only subsets of size 4 to 11 features\nwere considered due to the time complexity of checking all existing combinations of sets\nlarger than 11 features). After testing the models, it was concluded that the attribute\nsubset evaluators selected the features with which the models provided the most accurate\npredictions, especially the WrapperSubsetEval class.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p26_c59",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 26,
      "chunk_index": 59,
      "section": "3.3. Applying Natural Language Processing (NLP) and Creating Models",
      "char_start": 91108,
      "char_end": 92349,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p26_c58",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p26_c60",
      "semantic_coherence": 0.6192982196807861,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "The lists of the best subsets of selected\nLIWC features for each model, for both DS1 and DS2 datasets and the combined dataset\nDS3, can be found in Appendix B. Table 2 shows LIWC subcategories (features), the category to which it belongs, abbre-\nviations, descriptions, the most frequently used examples belonging to that subcategory\nand internal consistency calculated using the alpha coef\ufb01cient (Cronbach\u2019s alpha), and the\nKuder\u2013Richards (KR-20) formula. Algorithms 2023, 16, 221 13 of 34\nTable 2. LIWC-22 dimensions and reliability [16]. Category Abbrev. Description/Most\nFrequently Used\nExamples\nWords/\nEntries in\nCategory\nInternal\nConsistency:\nCronbach\u2019s\u03b1\nInternal\nConsistency:\nKR-20\nSummary\nVariables\nWord count WC Total word count - - -\nClout Clout Language of leadership,\nstatus - - -\nAuthentic Authentic Perceived honesty,\ngenuineness - - -\nEmotional tone Tone Degree or positive\n(negative) tone - - -\nWords per\nsentence WPS Average words per\nsentence - - -\nBig words BigWords Percent words 7 letters\nor longer - - -\nLinguistic\nDimensions Linguistic 4933 0.36 1.00\n1st person singular i I, me, my, myself 6/74 0.49 0.85\n2nd person you you, your, u, yourself 14/59 0.37 0.82\n3rd person plural they they, their, them,\nthemsel 7/20 0.36 0.69\nNegations negate not, no, never, nothing 8/247 0.49 0.92\nConjunctions conj and, but, so, as 49/65 0.11 0.89\nPsychological Processes\nAll-or-none allnone all, no, never, always 35 0.37 0.88\nCognitive\nprocesses cogproc but, not, if, or, know 1365 0.67 0.99\nInsight insight know, how, think, feel 383 0.43 0.96\nCausation cause how, because, make,\nwhy 169 0.21 0.90\nProsocial behavior prosocial care, help, thank, please 242 0.49 0.89\nSocial referents socrefs you, we, he, she 1232 0.35 0.97\nFamily family parent, mother, father,\nbaby 194 0.48 0.89\nExpanded Dictionary\nCulture Culture car, united states,\ngovern, phone 772 0.67 0.92\nPolitics politic\nunited states, govern,\ncongress,\nsenat\n339 0.75 0.91\nWork work work, school, working,\nclass 547 0.74 0.95\nMotives\nRisk risk secur, protect, pain, risk 128 0.28 0.86\nCuriosity curiosity scien, look for, research,\nwonder 76 0.26 0.79\nMotion motion go, come, went, came 485 0.42 0.97\nTime orientation\nTime time when, now, then, day 464 0.50 0.97\nNetspeak netspeak :), u, lol, haha 439 0.73 0.96",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p27_c60",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 27,
      "chunk_index": 60,
      "section": "3.3. Applying Natural Language Processing (NLP) and Creating Models",
      "char_start": 92349,
      "char_end": 94633,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p27_c59",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p27_c61",
      "semantic_coherence": 0.39548158645629883,
      "has_citations": true,
      "topic_cluster": -1
    },
    {
      "text": "Four selected machine learning models were tested on the collected datasets (DS1 and\nDS2) and the combined dataset (DS3). Table 3 shows the results obtained from the analysis\n\nAlgorithms 2023, 16, 221 14 of 34\nusing LIWC, n-grams, and the combination of n-gram analysis with LIWC (by combining\nLIWC with all n-gram sets ranging from unigrams to trigrams). The notation of n-gram\nrecords indicates their range (e.g., 1,2-g represents unigrams and bigrams). Accuracy,\nprecision, and response were used to evaluate the performance of the models.\n\nFor each\nmodel, maximum accuracies obtained by employing LIWC analysis (green), n-grams (blue),\nand the combined approach analysis (red) are indicated while the largest of the three values\nis in bold (the same notation applied to the highest response and precision obtained using\nall the models). Table 3. Performance of selected machine learning models achieved by applying natural language\nprocessing techniques to the climate change dataset (DS1).",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p29_c62",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 29,
      "chunk_index": 62,
      "section": "4.1. Model Testing on DS1 (Climate Change Dataset)",
      "char_start": 100722,
      "char_end": 101716,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p29_c61",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p29_c63",
      "semantic_coherence": -0.009865988977253437,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "Accuracy Precision Recall Accuracy Precision Recall Accuracy Precision Recall Accuracy Precision Recall\nLIWC 78.02 76.83 80.99 77.24 72.58 90.27 76.47 73.41 85.71 78.05 74.89 78.08\n1,1\nn-grams 75.00 73.11 80.44 72.74 71.05 78.13 76.15 73.84 82.64 72.71 74.67 71.17\nn-grams +\nLIWC 78.02 78.19 78.68 76.14 78.02 73.46 76.91 75.80 81.15 73.43 72.57 75.71\n1,2\nn-grams 79.16 76.96 83.52 78.80 76.31 83.52 81.47 78.77 86.48 74.62 74.05 73.41\nn-grams +\nLIWC 78.80 78.32 80.22 75.77 76.97 74.18 81.47 77.48 88.74 74.96 74.00 77.20\n2,2\nn-grams 78.79 76.72 81.98 76.52 75.02 79.67 79.56 77.67 82.69 72.38 70.80 69.73\nn-grams +\nLIWC 78.40 77.53 80.99 77.21 76.96 77.14 77.26 75.83 81.98 74.22 73.21 76.54\n1,3\nn-grams 80.68 78.89 83.52 76.13 74.82 78.96 79.94 77.94 83.57 75.04 73.31 72.53\nn-grams +\nLIWC 79.56 78.33 82.53 76.52 76.84 75.66 78.77 76.16 85.05 75.34 74.22 77.97\n2,3\nn-grams 76.14 74.19 79.62 76.14 75.61 77.36 78.42 75.97 83.46 71.25 75.01 68.30\nn-grams +\nLIWC 79.16 78.12 81.76 79.52 77.11 83.96 79.57 76.50 83.46 74.97 73.86 77.25\n3,3\nn-grams 68.99 67.07 74.34 68.59 67.79 72.75 70.11 69.69 72.03 67.08 64.05 65.99\nn-grams +\nLIWC 78.80 77.59 81.76 74.96 73.67 77.97 74.92 74.21 76.48 74.20 73.11 77.31\nAnalyzing the DS1 dataset with LIWC, the highest accuracy of 78.05% was achieved\nusing random forest.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p30_c63",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 30,
      "chunk_index": 63,
      "section": "LR SVM NB RF",
      "char_start": 102812,
      "char_end": 104120,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p30_c62",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p30_c64",
      "semantic_coherence": 0.2907119691371918,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "A slightly better performance of the models was obtained using\nn-grams; more precisely, the maximum accuracy of 81.47% is given by the multinomial\nNaive Bayes model on unigrams and bigrams. The logistic regression follows with 80.68%\naccuracy in the analysis of unigrams, bigrams, and trigrams. Somewhat lower performance\nis given by the SVM and random forest models with an accuracy of 78.8% and 75.04% on\nthe range of (1,2)-grams and (1,3)-grams.\n\nIn addition, the accuracy of their predictions\nthrough other combinations of n-grams is lower than that obtained with analysis using\nLIWC for the same models while multinomial Naive Bayes and logistic regression gave\nbetter results during an analysis with n-grams compared to an analysis with LIWC. Furthermore, when analyzing DS1 by combining the n-gram and LIWC techniques,\nthe models mostly achieved better results compared to using the other two approaches.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p30_c64",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 30,
      "chunk_index": 64,
      "section": "LR SVM NB RF",
      "char_start": 104121,
      "char_end": 105032,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p30_c63",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p30_c65",
      "semantic_coherence": 0.4170020818710327,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "The best performance is again given by the multinomial Naive Bayes model tested on (1,2)-\ngrams and LIWC with an accuracy of 81.47%, which was also achieved using exclusively\nn-gram analysis. This is followed by logistic regression with an accuracy of 79.56% tested\non the range of n-grams from unigrams to trigrams, which is slightly lower performance\ncompared to the same model when analyzing n-grams, which achieved the maximum\naccuracy of 80.68%.\n\nBy comparing the analysis of DS1 using n-grams and n-grams in combination with\nLIWC, improvements in model performance have been achieved using the combined\n\nAlgorithms 2023, 16, 221 15 of 34\napproach. All models provide maximum accuracy using the combined approach of data\nanalysis (except logistic regression), but the average accuracy of the model obtained by\nthe analysis using the combined approach (77.04%) surpasses the one obtained by the\nanalysis with n-grams exclusively (75.27%). The stated average values are given and\nfurther elaborated upon in Section 4.5.\nTable 4 shows models generally achieved much higher recall compared to precision.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p30_c65",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 30,
      "chunk_index": 65,
      "section": "LR SVM NB RF",
      "char_start": 105032,
      "char_end": 106136,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p30_c64",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p30_c66",
      "semantic_coherence": 0.9096621870994568,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "The maximum precision of 78.89% was achieved using logistic regression on (1,3)-gram\nanalysis while the SVM analysis with LIWC achieved a response of 90.27%. The maximum\nprecision of 78.89% was achieved using a logistic regression with (1,2)-gram analysis while\nthe maximum response of 90.27% was obtained using SVM on LIWC analysis. Table 4. Performance of selected machine learning models achieved by applying natural language\nprocessing techniques to the COVID-19 dataset (DS2).",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p31_c66",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 31,
      "chunk_index": 66,
      "section": "LR SVM NB RF",
      "char_start": 106136,
      "char_end": 106617,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p31_c65",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p31_c67",
      "semantic_coherence": 0.6708934903144836,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Accuracy Precision Recall Accuracy Precision Recall Accuracy Precision Recall Accuracy Precision Recall\nLIWC 74.62 75.13 76.36 73.32 72.37 77.27 74.58 74.75 75.53 69.98 70.88 69.32\n1,1\nn-grams 67.69 67.70 69.77 70.76 70.29 74.70 69.62 71.49 66.36 66.09 67.53 62.05\nn-grams +\nLIWC 75.05 75.72 77.27 73.37 73.98 73.94 72.55 75.45 69.77 71.68 67.82 69.70\n1,2\nn-grams 69.22 68.64 72.27 66.59 65.30 70.53 73.86 72.60 78.11 63.95 67.00 70.53\nn-grams +\nLIWC 73.79 74.05 75.53 74.64 75.23 75.61 76.39 76.17 78.18 71.67 72.51 71.44\n2,2\nn-grams 69.17 69.97 72.35 72.05 72.68 70.68 70.04 69.90 73.94 67.45 70.20 62.95\nn-grams +\nLIWC 73.80 73.81 76.36 76.34 77.35 77.20 70.87 69.59 74.77 72.97 72.54 70.61\n1,3\nn-grams 70.49 70.35 73.11 65.69 64.46 69.62 71.29 69.67 77.27 64.84 65.00 65.53\nn-grams +\nLIWC 74.64 75.16 76.44 74.64 75.59 74.77 73.82 73.22 76.52 71.25 70.29 69.70\n2,3\nn-grams 71.67 71.34 74.85 68.71 67.01 68.68 70.91 70.43 70.40 70.43 70.83 67.20\nn-grams +\nLIWC 74.66 74.88 77.20 74.64 74.17 77.20 69.20 67.82 75.76 72.93 70.79 73.03\n3,3\nn-grams 59.33 58.80 63.03 58.89 59.44 58.71 59.73 59.68 61.21 57.59 55.78 76.52\nn-grams +\nLIWC 73.77 74.52 75.53 72.48 73.29 73.71 65.20 64.04 69.47 69.98 70.81 75.53",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p32_c67",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 32,
      "chunk_index": 67,
      "section": "LR SVM NB RF",
      "char_start": 110450,
      "char_end": 111656,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p32_c66",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p32_c68",
      "semantic_coherence": 0.29805317521095276,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Table 4 shows the results obtained by testing the selected machine learning models\non the DS2 data set by analyzing LIWC, n-grams, and combining those two approaches. The logistic regression model achieved the highest accuracy of 74.62% in the analysis using\nLIWC while the multinomial Naive Bayes gave slightly worse results with 74.58%. SVM\nusing the same approach achieved an accuracy of 73.32% while the random forest model\ntested using LIWC analysis gave the worst accuracy of 69.98%.\n\nExamining the model\nthrough combinations of n-grams, Naive Bayes tested on a combination of unigrams and\nbigrams leads with an accuracy of 73.86%. It is followed by SVM tested on bigrams with",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p33_c68",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 33,
      "chunk_index": 68,
      "section": "4.2. Model Testing on DS2 (COVID-19 Dataset)",
      "char_start": 112890,
      "char_end": 113572,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p33_c67",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p33_c69",
      "semantic_coherence": 0.38143011927604675,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "of 71.67%. All models tested with the LIWC analysis achieved higher accuracy than the\nmaximum model performance obtained by the n-gram analysis. Using the combined analy-\nsis of n-grams and LIWC, the maximum accuracy of 76.39% on the analysis of unigrams\nand bigrams was achieved with the multinomial Bayes, followed by the SVM tested on\nbigrams with an accuracy of 76.34%. The logistic regression and random forest also gave\nsatisfactory results of 75.05% on unigrams and 72.97% on bigrams. Algorithms 2023, 16, 221 16 of 34\nThe maximum recall of 78.18% was achieved by combining an analysis of (1,2)-grams\nand LIWC using multinomial Naive Bayes while the SVM model achieved the maximum\nprecision of 77.35% with the combined analysis of bigrams and LIWC.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p33_c69",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 33,
      "chunk_index": 69,
      "section": "72.05% and logistic regression tested on a set of bigrams and trigrams with the accuracy",
      "char_start": 114344,
      "char_end": 115099,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p33_c68",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p33_c70",
      "semantic_coherence": 0.7421240210533142,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "The combined dataset DS3 was also analyzed with LIWC, n-grams, and their combina-\ntion and was tested using the four machine learning models (Table 5). By testing the model\non the features obtained with the LIWC analysis, the SVM model achieved the highest\naccuracy of 74.60%. It is followed by the logistic regression and random forest with 74.40%\nwhile multinomial Naive Bayes achieved an accuracy of 72.20%.\n\nWhen comparing the\nperformance of the models achieved with the LIWC to the n-gram analysis, virtually all\nmodels classify better using the LIWC approach except for the multinomial Naive Bayes\nmodel, which achieved an accuracy of 74.00% by analyzing unigrams and bigrams, which\nis an increase of 1.8% compared to the LIWC analysis. The logistic regression and SVM\nachieved their maximum accuracy of 72.80% and 71.80% by testing on bigrams while the\nleast favourable results were obtained with classi\ufb01cation using the random forest model\n(69.60%). By combining analysis with n-grams and LIWC, the models achieved the best\nperformance.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c70",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 70,
      "section": "4.3. Model Testing on the Combined Dataset (Climate Change and COVID-19 Datasets Combined)",
      "char_start": 116036,
      "char_end": 117080,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c69",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c71",
      "semantic_coherence": 0.799544632434845,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "SVM tested on bigrams with the maximum accuracy of 77.00%, followed\nby logistic regression with 76.20% and random forest with 75.40%. In this analysis, the\nmultinomial Naive Bayes model (75.20%) achieved slightly worse results by analyzing\ndata using the combined techniques (n-grams and LIWC). By comparing the results by\ntesting the model on different sets of n-grams, it was found that the models mostly perform\nbetter using an analysis combining n-grams and LIWC as opposed to an analysis using\nLIWC only. Table 5. Performance of selected machine learning models achieved by applying natural language\nprocessing techniques on the combined climate change and COVID-19 datasets (DS3).",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c71",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 71,
      "section": "4.3. Model Testing on the Combined Dataset (Climate Change and COVID-19 Datasets Combined)",
      "char_start": 117080,
      "char_end": 117766,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c70",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c72",
      "semantic_coherence": 0.7987773418426514,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Accuracy Precision Recall Accuracy Precision Recall Accuracy Precision Recall Accuracy Precision Recall\nLIWC 74.40 73.90 76.40 74.60 73.94 77.60 72.20 70.34 78.00 74.20 74.09 71.60\n1,1\nn-grams 70.40 69.57 72.80 69.00 68.71 71.20 72.20 72.72 71.20 69.00 67.18 69.20\nn-grams +\nLIWC 75.00 74.89 76.80 73.80 72.97 77.20 72.20 72.06 72.80 71.60 71.89 70.40\n1,2\nn-grams 72.00 70.71 76.00 70.60 69.83 73.60 74.00 69.79 72.80 69.20 66.36 66.40\nn-grams +\nLIWC 75.60 75.20 77.60 76.60 74.98 80.40 73.40 72.74 75.20 75.40 71.43 70.00\n2,2\nn-grams 72.80 72.29 75.60 71.80 71.05 75.20 72.80 73.06 73.20 69.60 70.51 69.20\nn-grams +\nLIWC 76.20 75.19 79.20 77.00 75.52 80.80 75.20 74.10 78.40 74.00 73.48 75.20\n1,3\nn-grams 71.60 70.70 74.40 70.40 70.34 71.60 73.40 72.39 76.00 68.60 70.93 70.40\nn-grams +\nLIWC 75.60 75.12 88.60 74.20 73.12 77.20 74.00 73.20 76.40 73.20 72.20 73.60\n2,3\nn-grams 70.20 69.46 73.60 70.20 69.09 74.00 71.40 70.43 74.40 67.20 68.75 72.80\nn-grams +\nLIWC 75.20 74.32 78.00 76.00 74.54 80.00 72.00 71.19 74.40 74.40 73.34 71.60\n3,3\nn-grams 65.80 65.08 69.60 64.80 63.40 71.20 65.00 64.59 68.00 63.20 60.49 85.20\nn-grams +\nLIWC 74.20 73.29 76.80 73.20 72.83 74.80 70.40 69.02 74.80 75.00 73.86 73.20\nLikewise, the combined approach, compared to the n-gram analysis, gives better\nresults for each set of n-grams.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c72",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 72,
      "section": "LR SVM NB RF",
      "char_start": 119680,
      "char_end": 120998,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c71",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c73",
      "semantic_coherence": 0.27375340461730957,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "The highest overall accuracy of 77.00% was achieved by\ntesting the SVM on the data analyzed with bigrams and LIWC, which is a 5.20% better\nresult compared to an analysis with bigrams only. The mentioned model also achieved\n\nAlgorithms 2023, 16, 221 17 of 34\nthe highest precision (75.52%) compared to the other models. The maximum accuracy\nachieved using the logistic regression or random forest models is 76.20% and 75.40%,\nrespectively. The multinomial Na\u00efve Bayes achieved a slightly less favourable accuracy of",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c73",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 73,
      "section": "LR SVM NB RF",
      "char_start": 120999,
      "char_end": 121513,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c72",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c74",
      "semantic_coherence": 0.3972797989845276,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "(74.00%) or LIWC (72.20%). The maximum recall of 88.60% was achieved by using the logistic regression model\non the dataset analyzed with (1,3)-grams and LIWC. Testing DS3 Models Individually on DS1 (Climate Change Dataset) and DS2\n(COVID-19 Dataset)\nModels trained on the combined dataset DS3 were tested individually on the DS1 and\nDS2 datasets to gain insight into the possibility of model generalization and its applicability\nto different datasets than the one they originate from.\n\nTable 6 shows a comparison of\nthe accuracies of the models trained on DS1 and DS3 and tested on DS1 using the LIWC\nanalysis, n-grams, and a combination of these techniques on all selected groups of n-grams. The above data are shown graphically in Figures 3 and 4. Table 6. Performance comparison of models trained and tested on DS1 (climate change dataset) with\nthe models trained on DS3 (combined dataset) and tested on DS1. DS1 Models Tested on DS1 DS3 Models Tested on DS1",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c74",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 74,
      "section": "75.20%, which is still better compared to those obtained using only the n-gram analysis",
      "char_start": 123374,
      "char_end": 124335,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c73",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c75",
      "semantic_coherence": 0.4518449902534485,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "LIWC 78.02 77.24 76.47 78.05 81.54 81.54 77.31 71.38\n1,1\nn-grams 75.00 72.74 76.15 72.71 71.92 69.62 70.77 68.85\nn-grams +\nLIWC 78.02 76.14 76.91 73.43 74.62 80.00 83.46 78.08\n1,2\nn-grams 79.16 78.80 81.47 74.62 73.85 72.31 73.46 68.85\nn-grams +\nLIWC 78.80 75.77 81.47 74.96 81.92 80.38 77.69 80.77\n2,2\nn-grams 78.79 76.52 79.56 72.38 73.46 67.69 73.85 69.62\nn-grams +\nLIWC 78.40 77.21 77.26 74.22 76.92 76.00 80.77 77.69\n1,3\nn-grams 80.68 76.13 79.94 75.04 72.69 72.69 72.69 76.15\nn-grams +\nLIWC 79.56 76.52 78.77 75.34 76.54 82.31 77.69 78.08\n2,3\nn-grams 76.14 76.14 78.42 71.25 77.31 77.31 76.92 68.46\nn-grams +\nLIWC 79.16 79.52 79.57 74.97 73.85 78.85 80.77 76.54\n3,3\nn-grams 68.99 68.59 70.11 67.08 67.69 68.08 67.31 65.38\nn-grams +\nLIWC 78.80 74.96 74.92 74.20 69.23 79.62 76.54 73.85\n\nAlgorithms 2023, 16, 221 18 of 34\nAlgorithms 2023, 16, x FOR PEER REVIEW 17 of 31 \n \n1,3 n-grams  80.68 76.13 79.94 75.04 72.69 72.69 72.69 76.15 \nn-grams + LIWC 79.56 76.52 78.77 75.34 76.54 82.31 77.69 78.08 \n2,3 n-grams  76.14 76.14 78.42 71.25 77.31 77.31 76.92 68.46 \nn-grams + LIWC 79.16 79.52 79.57 74.97 73.85 78.85 80.77 76.54 \n3,3 n-grams  68.99 68.59 70.11 67.08 67.69 68.08 67.31 65.38 \nn-grams + LIWC 78.80 74.96 74.92 74.20 69.23 79.62 76.54 73.85 \n \nFigure 3.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c75",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 75,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 125472,
      "char_end": 126738,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c74",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c76",
      "semantic_coherence": 0.20807287096977234,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Models trained and tested on DS1 (climate change dataset). Figure 4. Models trained on DS3 (combined dataset) and tested on DS1 (climate change dataset). Figure 3. Models trained and tested on DS1 (climate change dataset).",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c76",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 76,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 126739,
      "char_end": 126961,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c75",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c77",
      "semantic_coherence": 0.02646254375576973,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Algorithms 2023, 16, x FOR PEER REVIEW 17 of 31 \n \n1,3 n-grams  80.68 76.13 79.94 75.04 72.69 72.69 72.69 76.15 \nn-grams + LIWC 79.56 76.52 78.77 75.34 76.54 82.31 77.69 78.08 \n2,3 n-grams  76.14 76.14 78.42 71.25 77.31 77.31 76.92 68.46 \nn-grams + LIWC 79.16 79.52 79.57 74.97 73.85 78.85 80.77 76.54 \n3,3 n-grams  68.99 68.59 70.11 67.08 67.69 68.08 67.31 65.38 \nn-grams + LIWC 78.80 74.96 74.92 74.20 69.23 79.62 76.54 73.85 \n \nFigure 3. Models trained and tested on DS1 (climate change dataset). Figure 4. Models trained on DS3 (combined dataset) and tested on DS1 (climate change dataset).\n\nFigure 4. Models trained on DS3 (combined dataset) and tested on DS1 (climate change dataset). The logistic regression and SVM models trained on the combined dataset (DS3) and\nanalyzed with LIWC achieved the highest accuracy of 81.45%, which is higher than for\nthose models trained on the DS1 dataset. The other models also achieved better results by\nanalyzing the data with LIWC.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c77",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 77,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 126966,
      "char_end": 127942,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c76",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c78",
      "semantic_coherence": 0.49102485179901123,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "The models trained on DS3 and tested on the DS1 using\n\nAlgorithms 2023, 16, 221 19 of 34\nthe n-gram analysis and LIWC achieved better results compared to the n-gram analysis\nonly for all selected sets of n-grams. The maximum accuracy of 83.46% was achieved with\nthe multinomial Naive Bayes with unigram analysis and LIWC, followed by the SVM with",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c78",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 78,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 127946,
      "char_end": 128292,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c77",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c79",
      "semantic_coherence": 0.5458993315696716,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "best prediction using the combined approach analysis (80.77%). Examining the results of\nthe models trained on DS3 and tested on the DS1 dataset using the n-gram analysis, logistic\nregression, and SVM on (1,3)-grams achieved the highest accuracy (77.31%). Compared to the models trained on DS1, the models trained on DS3, during testing\non the DS1 dataset, achieved mostly less favorable results with the n-gram analysis.\n\nOn the\nother hand, the n-gram analysis in combination with the LIWC achieved higher prediction\nmaxima and slightly higher average results considering the performance of all models\nduring training on the combined dataset (78.01%) compared to training models on SP1\ndata (77.04%). Models trained on DS3 and tested on the DS2 dataset with the LIWC analysis achieved\ngenerally less favorable results compared to the models trained and tested on the DS2 data\n(Table 7).",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c79",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 79,
      "section": "82.31% and the logistic regression with 81.92%. The random forest model also achieved the",
      "char_start": 131166,
      "char_end": 132052,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c78",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c80",
      "semantic_coherence": 0.7366088032722473,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "In addition, a similar trend can be noticed in the analysis with n-grams or n-\ngrams in combination with LIWC, which mostly achieved less favorable results. However,\ncomparing the results obtained from the n-gram analysis in relation to the analysis using\nthe combined technique, it follows that the combined approach achieved a higher accuracy\non almost all selected sets of n-grams (while testing the models trained on DS3 on dataset\nDS2).\n\nThe maximum accuracy achieved by analyzing bigrams and trigrams using the\nlogistic regression is 75.42% while the Naive Bayes and random forest achieve a maximum\nof 73.33% by analyzing (1,3)-grams and unigrams. The highest accuracy obtained using the\ncombined approach analysis (78.33%) was achieved with a classi\ufb01cation using the random\nforest model, followed by multinomial Naive Bayes with 77.50%. Table 7. Performance comparison of models trained and tested on DS2 (COVID-19 dataset) with the\nmodels trained on DS3 (combined dataset) and tested on DS2. DS2 Models Tested on DS2 DS3 Models Tested on DS2",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c80",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 80,
      "section": "82.31% and the logistic regression with 81.92%. The random forest model also achieved the",
      "char_start": 132052,
      "char_end": 133101,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c79",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c81",
      "semantic_coherence": 0.759431004524231,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "LIWC 74.62 73.32 74.58 69.98 72.04 70.38 71.58 70.33\n1,1\nn-grams 67.69 70.76 69.62 66.09 61.67 61.67 62.92 73.33\nn-grams +\nLIWC 75.05 73.37 72.55 71.68 70.00 76.67 72.08 72.92\n1,2\nn-grams 69.22 66.59 73.86 63.95 66.67 66.25 72.08 65.83\nn-grams +\nLIWC 73.79 74.64 76.39 71.67 72.92 71.25 72.92 73.33\n2,2\nn-grams 69.17 72.05 70.04 67.45 75.42 72.92 68.75 59.17\nn-grams +\nLIWC 73.80 76.34 70.87 72.97 70.00 74.17 68.75 69.58\n1,3\nn-grams 70.49 65.69 71.29 64.84 65.83 64.58 73.33 63.33\nn-grams +\nLIWC 74.64 74.64 73.82 71.25 73.33 73.33 75.00 69.58\n2,3\nn-grams 71.67 68.71 70.91 70.43 66.25 65.83 66.25 62.08\nn-grams +\nLIWC 74.66 74.64 69.20 72.93 76.67 69.58 68.33 78.33\n3,3\nn-grams 59.33 58.89 59.73 57.59 60.83 62.08 59.17 53.75\nn-grams +\nLIWC 73.77 72.48 65.20 69.98 73.75 76.25 77.50 73.33\n\nAlgorithms 2023, 16, 221 20 of 34\nAlthough the analysis with n-grams trained on DS3 on DS2 data during the testing\nof the models achieved slightly less favorable results than the analysis with the combined\ntechniques, it also achieved slightly higher accuracy maxima than the analysis with LIWC\nwhile the average value across all selected sets of n-grams was somewhat lower.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c81",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 81,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 135216,
      "char_end": 136382,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c80",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c82",
      "semantic_coherence": 0.3008584976196289,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "The DS3\nmodels tested on the DS1 dataset achieved a higher maximum accuracy of predictions\ncompared to the DS3 models tested on the same dataset by analyzing n-grams and LIWC. Figures 5 and 6 graphically show the results obtained by analyzing n-grams and\nn-grams with LIWC for the models trained on DS2 and DS3, respectively, and tested on\nDS2 data. The difference in model performance when using different data processing\ntechniques, which was previously mentioned, is clearly shown from the \ufb01gures.\n\nAlgorithms 2023, 16, x FOR PEER REVIEW 19 of 31 \n \nmodels tested on the DS1 dataset achieved a higher maximum accuracy of predictions \ncompared to the DS3 models tested on the same dataset by analyzing n-grams and LIWC. Figures 5 and 6 graphically show the results obtained by analyzing n-grams and n-\ngrams with LIWC for the models trained on DS2 and DS3, respectively, and tested on DS2 \ndata. The di \ufb00erence in model performance when using di \ufb00erent data processing tech-\nniques, which was previously mentioned, is clearly shown from the \ufb01gures. Figure 5.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c82",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 82,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 136383,
      "char_end": 137443,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c81",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c83",
      "semantic_coherence": 0.31987351179122925,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Models trained and tested on DS2 (COVID-19 dataset). Figure 6. Models trained on DS3 (combined dataset) and tested on DS2 (COVID-19 dataset). Figure 5. Models trained and tested on DS2 (COVID-19 dataset). The accuracies obtained using the models trained on the combined DS3 data and tested\nindividually on DS1 and DS2 datasets given in Tables 6 and 7 are extracted and compared\nfor each model and shown in Table 8.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c83",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 83,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 137447,
      "char_end": 137861,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c82",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c84",
      "semantic_coherence": 0.5642113089561462,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "For each column, representing the performance of a\nspeci\ufb01c model on a speci\ufb01c dataset, the maximum value achieved using the n-gram analysis\nand LIWC is indicated in red, the maximum accuracy obtained with the n-gram analysis is\nindicated in blue, and the highest accuracy achieved with the LIWC analysis is indicated in\ngreen. Yellow horizontal lines indicate those results that exceed the average obtained using\na speci\ufb01c model on a given dataset through all combinations of n-grams (trend by model).\n\nYellow vertical lines indicate accuracies higher than the average obtained using all models\non a certain dataset (trend by method) for each individual set of n-grams. Predictions that\nexceed the average de\ufb01ned by the trend by models and the trend by methods are colored\nin red. Algorithms 2023, 16, 221 21 of 34\nAlgorithms 2023, 16, x FOR PEER REVIEW 19 of 31 \n \nmodels tested on the DS1 dataset achieved a higher maximum accuracy of predictions \ncompared to the DS3 models tested on the same dataset by analyzing n-grams and LIWC.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c84",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 84,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 137866,
      "char_end": 138900,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c83",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c85",
      "semantic_coherence": 0.39945730566978455,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Figures 5 and 6 graphically show the results obtained by analyzing n-grams and n-\ngrams with LIWC for the models trained on DS2 and DS3, respectively, and tested on DS2 \ndata. The di \ufb00erence in model performance when using di \ufb00erent data processing tech-\nniques, which was previously mentioned, is clearly shown from the \ufb01gures. Figure 5. Models trained and tested on DS2 (COVID-19 dataset). Figure 6. Models trained on DS3 (combined dataset) and tested on DS2 (COVID-19 dataset). Figure 6. Models trained on DS3 (combined dataset) and tested on DS2 (COVID-19 dataset). Table 8. Accuracy comparison of DS3 (combined dataset) models tested on DS1 (climate change\ndataset) and DS2 (COVID-19 dataset) using selected data processing techniques.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c85",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 85,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 138902,
      "char_end": 139642,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c84",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c86",
      "semantic_coherence": 0.5984060168266296,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "DS3\nModels\nTested on\nDataset\nDS1\nDS3\nModels\nTested on\nDataset\nDS2\nDS3\nModels\nTested on\nDataset\nDS1\nDS3\nModels\nTested on\nDataset\nDS2\nDS3\nModels\nTested on\nDataset\nDS1\nDS3\nModels\nTested on\nDataset\nDS2\nDS3\nModels\nTested on\nDataset\nDS1\nDS3\nModels\nTested on\nDataset\nDS2\nLIWC 81.54 72.04 81.54 70.38 77.31 71.58 71.38 70.33\n1,1\nn-grams 71.92 61.67 69.62 61.67 70.77 62.92 68.85 73.33\nn-grams +\nLIWC 74.62 70 80 76.67 83.46 72.08 78.08 72.92\n1,2\nn-grams 73.85 66.67 72.31 66.25 73.46 72.08 68.85 65.83\nn-grams +\nLIWC 81.92 72.92 80.38 71.25 77.69 72.92 80.77 73.33\n2,2\nn-grams 73.46 75.42 67.69 72.92 73.85 68.75 69.62 59.17\nn-grams +\nLIWC 76.92 70 76 74.17 80.77 68.75 77.69 69.58\n1,3\nn-grams 72.69 65.83 72.69 64.58 72.69 73.33 76.15 63.33\nn-grams +\nLIWC 76.54 73.33 82.31 73.33 77.69 75 78.08 69.58\n2,3\nn-grams 77.31 66.25 77.31 65.83 76.92 66.25 68.46 62.08\nn-grams +\nLIWC 73.85 76.67 78.85 69.58 80.77 68.33 76.54 78.33\n3,3\nn-grams 67.69 60.83 68.08 62.08 67.31 59.17 65.38 53.75\nn-grams +\nLIWC 69.23 73.75 79.62 76.25 76.54 77.5 73.85 73.33\n\nAlgorithms 2023, 16, 221 22 of 34\nThe SVM and Naive Bayes models achieved the highest number of above-average\npredictions according to the trend by models and methods (11/24) (considering only the n-\ngrams and analysis using a combined approach).",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c86",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 86,
      "section": "LR SVM NB RF",
      "char_start": 144136,
      "char_end": 145422,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c85",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c87",
      "semantic_coherence": 0.6483264565467834,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "The logistic regression gives slightly less\nfavorable performance (9/24) while when examining the generalization possibilities, the\nrandom forest model ended as the least favorable with only 6/24 above-average predictions\naccording to the trend by models and methods. When analyzing using LIWC, the logistic\nregression and SVM achieved the best above-average predictions by testing DS3 models\non the DS1 data (81.54%), which is higher than the average results obtained by training\nand testing the same models on the DS1 dataset (77.45%). Logistic regression also achieved\nabove-average results by testing models trained on DS3 on the DS2 dataset using the LIWC\nanalysis, which are higher than the average results obtained by training and testing models\non DS2 (73.13%).",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c87",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 87,
      "section": "LR SVM NB RF",
      "char_start": 145423,
      "char_end": 146192,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c86",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c88",
      "semantic_coherence": 0.41172492504119873,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "In Table 8, the accuracies of the selected machine learning models obtained from\ntesting the three collected datasets DS1, DS2, and DS3 using LIWC, n-grams, and the\ncombining n-gram analysis with LIWC. For each model and each dataset in the table,\nthe maximum achieved with the n-gram analysis is marked in blue while the maximum\nobtained by the combined technique analysis is marked in red. The maximum accuracy\ngiven by the LIWC analysis is marked in green.\n\nFor each model, the average accuracy\nobtained by analysis with LIWC, n-grams and the combined approach was calculated for\neach of the given datasets (trend by model). In Table 9, values that exceed the average\naccuracy for a speci\ufb01c model (trend by models) or for a speci\ufb01c data processing method\n(trend by methods) are colored in yellow while values that exceed both trends are colored\nin red. Table 9. Comparison of model accuracies on DS1 (climate change dataset), DS2 (COVID-19 dataset),\nand DS3 (combined dataset) using the selected data processing techniques.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c89",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 89,
      "section": "4.4.1. Comparing Model Performance across Datasets",
      "char_start": 148320,
      "char_end": 149346,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c88",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c90",
      "semantic_coherence": 0.0022012607660144567,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "DS1 DS2 DS3 DS1 DS2 DS3 DS1 DS2 DS3 DS1 DS2 DS3\nLIWC 78.02 74.62 74.40 77.24 73.32 74.60 76.47 74.58 72.20 78.05 69.98 74.20\n1,1\nn-grams 75.00 67.69 70.40 72.74 70.76 69.00 76.15 69.62 72.20 72.71 66.09 69.00\nn-grams +\nLIWC 78.02 75.05 75.00 76.14 73.37 73.80 76.91 72.55 72.20 73.43 71.68 71.60\n1,2\nn-grams 79.16 69.22 72.00 78.80 66.59 70.60 81.47 73.86 74.00 74.62 63.95 69.20\nn-grams +\nLIWC 78.80 73.79 75.60 75.77 74.64 76.60 81.47 76.39 73.40 74.96 71.67 75.40\n2,2\nn-grams 78.79 69.17 72.80 76.52 72.05 71.80 79.56 70.04 72.80 72.38 67.45 69.60\nn-grams +\nLIWC 78.40 73.80 76.20 77.21 76.34 77.00 77.26 70.87 75.20 74.22 72.97 74.00\n1,3\nn-grams 80.68 70.49 71.60 76.13 65.69 70.40 79.94 71.29 73.40 75.04 64.84 68.60\nn-grams +\nLIWC 79.56 74.64 75.60 76.52 74.64 74.20 78.77 73.82 74.00 75.34 71.25 73.20\n2,3\nn-grams 76.14 71.67 70.20 76.14 68.71 70.20 78.42 70.91 71.40 71.25 70.43 67.20\nn-grams +\nLIWC 79.16 74.66 75.20 79.52 74.64 76.00 79.57 69.20 72.00 74.97 72.93 74.40\n3,3\nn-grams 68.99 59.33 65.80 68.59 58.89 64.80 70.11 59.73 65.00 67.08 57.59 63.20\nn-grams +\nLIWC 78.80 73.77 74.20 74.96 72.48 73.20 74.92 65.20 70.40 74.20 69.98 75.00\n\nAlgorithms 2023, 16, 221 23 of 34\nStatistically, multinomial Na\u00efve Bayes (22/36) achieved the most above-average results\nper model with the n-gram analysis and with the combination of n-gram analysis and\nLIWC while the logistic regression competes with 21/36 above-average predictions.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c90",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 90,
      "section": "LR SVM NB RF",
      "char_start": 150474,
      "char_end": 151911,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c89",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c91",
      "semantic_coherence": 0.3443503677845001,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "By\nclassifying using the random forest model, 20/36 above-average values were obtained\nwhile the SVM achieved 19/36. Analyzing the results with n-grams and a combined approach on each dataset and\nfor each set of n-grams, the logistic regression achieved most of the above-average values\n(33/36), followed by Naive Bayes with 25/36 and SVM with 22/36. The least favorable\nresults were achieved with the random forest model with only 5/36 above-average predic-\ntions obtained for each dataset and n-gram.\n\nConsidering the overall trends by models and methods by analyzing n-grams and\nn-grams and LIWC, the logistic regression model also achieved the maximum compared to\nthe other models with a total of 20/36 above-average predictions for both the model and\nthe method. By comparing the performance of the models on the results obtained from the LIWC\nanalysis, the logistic regression model statistically achieved the best results with the above-\naverage predictions given for each of the three datasets, DS1, DS2, and DS3, while the other\nmodels predicted somewhat less favorable with 2/3 above-average predictions. Statistically,\nthe datasets on which the LIWC analysis achieved the most above-average results using all\nmodels are DS2 and DS3 with 3/4 above-average predictions.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c91",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 91,
      "section": "LR SVM NB RF",
      "char_start": 151912,
      "char_end": 153190,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c90",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c92",
      "semantic_coherence": 0.2410912811756134,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "(COVID-19 Dataset)\nMachine learning models trained on the DS1 were tested on the DS2 dataset in order\nto test the applicability of the trained models to deception data on another topic (Table 10). The table shows accuracies obtained using the machine learning models for each of the\nselected data processing methods. The data is also presented graphically in Figure 7. For\neach model, the maximum achieved accuracy obtained with the LIWC analysis (green),\nn-gram analysis (blue), and combined analysis with n-grams and LIWC (red) is indicated. The maximum of the three listed values is bold for each model.\n\nAlgorithms 2023, 16, x FOR PEER REVIEW 22 of 31 \n \naccurate results (71.30%) analyzing n-grams. The results obtained with the LIWC analysis \nare generally less favorable than those obtained using other data processing approaches. When using models trained on DS1 and tested on DS2 data, a drop in the model\u2019s perfor-\nmance was recorded with all three approaches compared to training and testing the mod-\nels on the DS1 data. Table 10. Performance comparison of models trained on DS1 (climate change dataset) and tested on \nDS1 and DS2 (COVID-19 dataset).",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c92",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 92,
      "section": "4.4.2. Applicability of Models Trained on DS1 (Climate Change Dataset) to DS2",
      "char_start": 155932,
      "char_end": 157094,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c91",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c93",
      "semantic_coherence": 0.6154545545578003,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "DS1 Models Tested on Dataset \nDS1 \nDS1 Models Tested on Dataset \nDS2 \n LR SVM NB RF LR SVM NB RF \nLIWC 78.02 77.24 76.47 78.05 67.34 64.37 64.38 63.08 \n1,1 n-grams  75.00 72.74 76.15 72.71 67.92 69.57 67.95 65.67 \nn-grams + LIWC 78.02 76.14 76.91 73.43 69.89 67.79 71.72 70.74 \n1,2 n-grams  79.16 78.80 81.47 74.62 69.66 70.00 72.21 65.40 \nn-grams + LIWC 78.80 75.77 81.47 74.96 70.29 67.79 73.44 67.84 \n2,2 n-grams  78.79 76.52 79.56 72.38 69.58 68.28 71.70 71.30 \nn-grams + LIWC 78.40 77.21 77.26 74.22 67.34 68.61 67.88 65.25 \n1,3 n-grams  80.68 76.13 79.94 75.04 69.66 70.49 73.04 65.67 \nn-grams + LIWC 79.56 76.52 78.77 75.34 69.86 68.62 73.82 69.93 \n2,3 n-grams  76.14 76.14 78.42 71.25 70.02 68.73 70.04 64.47 \nn-grams + LIWC 79.16 79.52 79.57 74.97 68.59 68.99 68.30 66.12 \n3,3 n-grams  68.99 68.59 70.11 67.08 59.75 60.18 61.92 57.59 \nn-grams + LIWC 78.80 74.96 74.92 74.20 67.74 67.32 64.80 66.03 \n \nFigure 7. Models trained on DS1 (climate change dataset) and tested on DS2 (COVID-19 dataset).",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c93",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 93,
      "section": "4.4.2. Applicability of Models Trained on DS1 (Climate Change Dataset) to DS2",
      "char_start": 157100,
      "char_end": 158104,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c92",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c94",
      "semantic_coherence": 0.46137288212776184,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Change Dataset) \nTable 11 shows the accuracies of all models trained on DS2 and tested on the DS2 and \nDS1 datasets for each of the selected data processing methods (also Figure 8). For each \nmodel, the highest value obtained with the LIWC analysis is indicated in green, the max-\nimum value obtained with the n-gram analysis in blue, and the maximum obtained using \nthe combined approach analysis is indicated in red. At the same time, for each model, the \nlargest of the three listed values is bold. By testing the performance of the models trained \nFigure 7.\n\nModels trained on DS1 (climate change dataset) and tested on DS2 (COVID-19 dataset). Algorithms 2023, 16, 221 24 of 34\nTable 10. Performance comparison of models trained on DS1 (climate change dataset) and tested on\nDS1 and DS2 (COVID-19 dataset). DS1 Models Tested on Dataset DS1 DS1 Models Tested on Dataset DS2",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c94",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 94,
      "section": "4.4.3. Applicability of Models Trained on DS2 (COVID-19 Dataset) to DS1 (Climate",
      "char_start": 160436,
      "char_end": 161312,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c93",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c95",
      "semantic_coherence": 0.4462587535381317,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "LIWC 78.02 77.24 76.47 78.05 67.34 64.37 64.38 63.08\n1,1\nn-grams 75.00 72.74 76.15 72.71 67.92 69.57 67.95 65.67\nn-grams +\nLIWC 78.02 76.14 76.91 73.43 69.89 67.79 71.72 70.74\n1,2\nn-grams 79.16 78.80 81.47 74.62 69.66 70.00 72.21 65.40\nn-grams +\nLIWC 78.80 75.77 81.47 74.96 70.29 67.79 73.44 67.84\n2,2\nn-grams 78.79 76.52 79.56 72.38 69.58 68.28 71.70 71.30\nn-grams +\nLIWC 78.40 77.21 77.26 74.22 67.34 68.61 67.88 65.25\n1,3\nn-grams 80.68 76.13 79.94 75.04 69.66 70.49 73.04 65.67\nn-grams +\nLIWC 79.56 76.52 78.77 75.34 69.86 68.62 73.82 69.93\n2,3\nn-grams 76.14 76.14 78.42 71.25 70.02 68.73 70.04 64.47\nn-grams +\nLIWC 79.16 79.52 79.57 74.97 68.59 68.99 68.30 66.12\n3,3\nn-grams 68.99 68.59 70.11 67.08 59.75 60.18 61.92 57.59\nn-grams +\nLIWC 78.80 74.96 74.92 74.20 67.74 67.32 64.80 66.03\nConsidering the performance of models trained on DS1 and tested on the DS2 data,\nthe maximum prediction was achieved with the multinomial Naive Bayes (73.82%) with the\nanalysis using the combined approach.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c95",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 95,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 162354,
      "char_end": 163350,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c94",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c96",
      "semantic_coherence": 0.1788463294506073,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "The random forest model gave the most accurate\nresults (71.30%) analyzing n-grams. The results obtained with the LIWC analysis are\ngenerally less favorable than those obtained using other data processing approaches. When\nusing models trained on DS1 and tested on DS2 data, a drop in the model\u2019s performance\nwas recorded with all three approaches compared to training and testing the models on the\nDS1 data.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c96",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 96,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 163351,
      "char_end": 163757,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c95",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c97",
      "semantic_coherence": 0.3371836245059967,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Change Dataset)\nTable 11 shows the accuracies of all models trained on DS2 and tested on the DS2\nand DS1 datasets for each of the selected data processing methods (also Figure 8). For\neach model, the highest value obtained with the LIWC analysis is indicated in green, the\nmaximum value obtained with the n-gram analysis in blue, and the maximum obtained\nusing the combined approach analysis is indicated in red. At the same time, for each\nmodel, the largest of the three listed values is bold.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c97",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 97,
      "section": "4.4.3. Applicability of Models Trained on DS2 (COVID-19 Dataset) to DS1 (Climate",
      "char_start": 165214,
      "char_end": 165708,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c96",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c98",
      "semantic_coherence": 0.5203548073768616,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "By testing the performance of the\nmodels trained on DS2 and tested on the DS1 data set, the best results were achieved\nwith the multinomial Naive Bayes models (81.44%), SVM (81.05%) and logistic regression\n(79.90%) by analyzing (1,3)-grams, while the analysis using combined approach achieved\nthe maximum only using random forest (76.88%). By comparing the accuracy of models\ntrained on DS2 and tested on DS1 dataset compared to DS2, an improvement in model\nperformance was observed using n-gram analysis across all selected sets of n-grams.\n\nAlso,\nthe analysis with the combined approach achieved more favorable results on almost all sets\nof n-grams, while LIWC achieved slightly less favorable results. Algorithms 2023, 16, 221 25 of 34\nTable 11. Performance comparison of models trained on DS2 (COVID-19 dataset) and tested on DS2\nand DS1 (Climate change dataset). DS2 Models Tested on Dataset DS2 DS2 Models Tested on Dataset DS1",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c98",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 98,
      "section": "4.4.3. Applicability of Models Trained on DS2 (COVID-19 Dataset) to DS1 (Climate",
      "char_start": 165709,
      "char_end": 166642,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c97",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c99",
      "semantic_coherence": 0.5424777865409851,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "LIWC 74.62 73.32 74.58 69.98 66.99 65.47 65.57 64.02\n1,1\nn-grams 67.69 70.76 69.62 66.09 75.00 72.74 76.15 70.11\nn-grams +\nLIWC 75.05 73.37 72.55 71.68 72.66 73.79 76.18 74.97\n1,2\nn-grams 69.22 66.59 73.86 63.95 78.40 78.79 80.68 72.29\nn-grams +\nLIWC 73.79 74.64 76.39 71.67 73.79 78.73 80.71 73.43\n2,2\nn-grams 69.17 72.05 70.04 67.45 78.77 78.40 79.17 71.94\nn-grams +\nLIWC 73.80 76.34 70.87 72.97 71.54 74.91 77.28 75.71\n1,3\nn-grams 70.49 65.69 71.29 64.84 79.90 81.05 81.44 72.71\nn-grams +\nLIWC 74.64 74.64 73.82 71.25 74.57 74.94 79.96 76.11\n2,3\nn-grams 71.67 68.71 70.91 70.43 76.88 76.54 77.66 70.80\nn-grams +\nLIWC 74.66 74.64 69.20 72.93 71.54 73.76 79.93 76.88\n3,3\nn-grams 59.33 58.89 59.73 57.59 69.36 69.34 70.50 64.00\nn-grams +\nLIWC 73.77 72.48 65.20 69.98 71.54 71.21 71.27 73.46\nAlgorithms 2023, 16, x FOR PEER REVIEW 3  of 31  \n \n \n \nduring face-to-face and computer-mediated communication, whether synchronous or \nasynchronous, verbal or non-verbal?",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c99",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 99,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 168234,
      "char_end": 169197,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c98",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c100",
      "semantic_coherence": 0.23182258009910583,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Research so far has led to the conclusion that verbal behavior hides a deep amount \nof information that can be used in the detection of deception, almost more accurately than \nin the case of non-verbal analysis. Due to the inapplicability of the polygraph method to \ndeception detection in computer-mediated communication, there is an increasing empha- \nsis on research in methods for analyzing the syntactic and semantic properties of written \ntext and finding indicators of deception in various forms of digital interaction. So far, the \nmost commonly used methods of deception detection in the text are  machine learning \nmodels.\n\nThere is a great need for further research into syntactic, semantic, and other prop- \nerties of natural languages in order to create software that will detect deception with high \naccuracy. Current research covers deception detectio n in computer-mediated communication \n[2,3]; detection of fake reviews on social platforms [4,5]; deception detection collected \nfrom public trials [6,7]; the use of crowdsourcing platforms, such as Amazon Mechanical \nTurk, for generating deception datasets [ 5,8]; etc.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c100",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 100,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 169199,
      "char_end": 170336,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c99",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c101",
      "semantic_coherence": -0.04376034066081047,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "In their work, Feng and colleagues [5] \nalso analyzed the deep syntax of the data using the principles of probabilistic context-free \ngrammar (PCFG), independently and in combination with the aforementioned methods. In addition to the above, the LIWC tool was also used for deception detection by leverag- \ning insight into the psycho-linguistic characteristics of the analyzed text. In this paper, several di\ufb00erent machine learning models were used and their perfor- \nmance in di\ufb00erentiating deceptive and true text was tested. Two sets of data were collected \nusing the crowdsourcing platform Clickworker and the survey tool Qualtrics Survey.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c101",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 101,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 170337,
      "char_end": 170981,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c100",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c102",
      "semantic_coherence": 0.7479203343391418,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "For \ndata processing, n-grams, LIWC, and a combination of the two approaches were used. A \nselection of essential features for each model was carried out over the LIWC dimensions \nusing the WEKA tool in order to obtain subsets of features with which the models provide \nthe highest accuracy. Since two distinct datasets on two topics were created, they were \nused to train their own separate models. These models were then tested on their own data, \nbut they were also cross-tested on the data they were not trained with.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c102",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 102,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 170984,
      "char_end": 171505,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c101",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c103",
      "semantic_coherence": 0.35006508231163025,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Furthermore, both \ndatasets were combined to create a joint dataset on deceptive text, which was then used \nto test both models. The performance of the models was examined in order to gain insight \ninto the possibility of model generalization and its applicability to di\ufb00erent datasets. This \ngave insight into model parameters with the highest accuracy in predicting deception.\n\nThe \npossibility of deception detection using natural language processing methods was also \ntested in order to ascertain which methods give the best performance in a generalization \nor applicability to other datasets than the ones they were trained upon; additionally, it \nwas to decide which method gives the best predictions in general. \n \nFigure 8. Models trained on DS2 (COVID-19 dataset) and tested on DS1 (climate dataset). Algorithms 2023, 16, 221 26 of 34",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c103",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 103,
      "section": "LR SVM NB RF LR SVM NB RF",
      "char_start": 171506,
      "char_end": 172349,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c102",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c104",
      "semantic_coherence": 0.37822702527046204,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "In order to get a better insight into the performance of the used machine learning\nmodels, average values of each model obtained by testing on three selected datasets using\nLIWC analysis, n-grams and by combining n-gram analysis with LIWC was extracted\n(Table 12). The same was done with the models trained on the combined dataset DS3, and\ntested individually on the DS1 and DS3 datasets, which is shown in Table 13. Table 12.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c105",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 105,
      "section": "4.5.1. Overview of the Trends by Model",
      "char_start": 176572,
      "char_end": 176998,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c104",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c106",
      "semantic_coherence": 0.008826331235468388,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Overview of trends by model for the 3 datasets collected (yellow color: maximum average\naccuracy obtained for each individual model and dataset; red color: maximum average accuracy\nobtained using all models on each individual dataset).",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c106",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 106,
      "section": "4.5.1. Overview of the Trends by Model",
      "char_start": 176999,
      "char_end": 177234,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c105",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c107",
      "semantic_coherence": 0.46511051058769226,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "LR SVM NB RF All Models\nDS1 DS2 DS3 DS1 DS2 DS3 DS1 DS2 DS3 DS1 DS2 DS3 DS1 DS2 DS3\nLIWC 78.02 74.62 74.4 77.24 73.32 74.6 76.47 74.58 72.20 78.05 69.98 74.20 77.45 73.13 73.85\nn-grams 76.46 67.93 70.47 74.82 67.12 69.47 77.61 69.24 71.47 72.18 65.06 67.80 75.27 67.34 69.80\nn-grams +\nLIWC 78.79 74.29 75.30 76.69 74.35 75.13 78.15 71.34 72.87 74.52 71.75 73.93 77.04 73.33 74.31\nTable 13.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c107",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 107,
      "section": "4.5.1. Overview of the Trends by Model",
      "char_start": 177235,
      "char_end": 177624,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c106",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c108",
      "semantic_coherence": 0.24440628290176392,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Overview of trends by model for combined dataset DS3 tested individually on datasets DS1\nand DS2 (yellow color: maximum average accuracy obtained for each individual model and dataset;\nred color: maximum average accuracy obtained using all models on each individual dataset).",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c108",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 108,
      "section": "4.5.1. Overview of the Trends by Model",
      "char_start": 177625,
      "char_end": 177900,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c107",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c109",
      "semantic_coherence": 0.30429261922836304,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "LR SVM NB RF All Models\nDS3\nModels\nTested\non DS1\nData\nDS3\nModels\nTested\non DS2\nData\nDS3\nModels\nTested\non DS1\nData\nDS3\nModels\nTested\non DS2\nData\nDS3\nModels\nTested\non DS1\nData\nDS3\nModels\nTested\non DS2\nData\nDS3\nModels\nTested\non DS1\nData\nDS3\nModels\nTested\non DS2\nData\nDS3\nModels\nTested\non DS1\nData\nDS3\nModels\nTested\non DS2\nData\nLIWC 81.54 72.04 81.54 70.38 77.31 71.58 71.38 70.33 77.94 71.08\nn-grams 72.82 66.11 71.28 65.56 72.50 67.08 69.55 62.92 71.54 65.42\nn-grams\n+ LIWC 75.51 72.78 79.53 73.54 79.49 72.43 77.50 72.85 78.01 72.90\nThe average accuracy of all models obtained with the analysis using a combination\nof n-grams and LIWC is higher than that of the one using only the n-gram analysis for\neach of the three datasets, DS1, DS2, and DS3. The analysis based on LIWC alone also\nachieves higher accuracy compared to the average results obtained by testing all models\non different sets of n-grams.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c109",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 109,
      "section": "4.5.1. Overview of the Trends by Model",
      "char_start": 177901,
      "char_end": 178803,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c108",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c110",
      "semantic_coherence": 0.4949614107608795,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "The average accuracy of predictions obtained for all models\nusing the LIWC analysis generally gave less favorable results compared to the analysis\nwith a combined approach (n-grams and LIWC) on datasets DS2 and DS3 and a maximum\naverage accuracy of 77.45% when tested on dataset DS1. Table 13 compares average prediction accuracies obtained using all selected models\nand natural language processing techniques by training the model on the combined dataset,\nDS3, and testing individually on datasets DS1 and DS2.\n\nIn this case as well, the analysis\nwith n-grams on average gives less favorable results compared to the other data processing\ntechniques used. The highest average accuracy for both datasets was achieved with the\nn-gram analysis combined with LIWC while the LIWC analysis also gives favorable results\nwhen tested on both datasets. The maximum average accuracies, considering the results of all DS3 models tested on\nthe DS1 and DS2 datasets, were achieved using the analysis combining n-gram and LIWC\nand are 78.01% and 72.90%, respectively. Algorithms 2023, 16, 221 27 of 34",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c110",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 110,
      "section": "4.5.1. Overview of the Trends by Model",
      "char_start": 178804,
      "char_end": 179890,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c109",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c111",
      "semantic_coherence": 0.6838667392730713,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "The average accuracy of all models trained and tested on the collected datasets ob-\ntained with the analysis with LIWC, n-grams, or the analysis by combining these techniques\nfor all selected sets of n-grams are shown in Table 14. Table 14. Overview of trends by methods on 3 selected datasets (colored values indicate the maximum\naccuracy obtained using a particular data processing method on each of the given datasets).\n\nAll Models\nDS1 DS2 DS3 The Average Accuracy Obtained by Training and\nTesting Models on Datasets DS1, DS2 and DS3\nLIWC 77.45 73.13 73.85 74.81\n1,1\nn-grams 74.15 68.54 70.15 70.95\nn-grams + LIWC 76.13 73.16 73.15 74.15\n1,2\nn-grams 78.51 68.41 71.45 72.79\nn-grams + LIWC 77.75 74.12 75.25 75.71\n2,2\nn-grams 76.81 69.68 71.75 72.75\nn-grams + LIWC 76.77 73.50 75.60 75.29\n1,3\nn-grams 77.95 68.08 71.00 72.34\nn-grams + LIWC 77.55 73.59 74.25 75.13\n2,3\nn-grams 75.49 70.43 69.75 71.89\nn-grams + LIWC 78.31 72.86 74.40 75.19\n3,3 n-grams 68.69 58.89 64.70 64.09\nn-grams + LIWC 75.72 70.36 73.20 73.09\nAnalysis combining n-grams and LIWC achieved better average results for almost\nall selected sets of n-grams compared to the analysis using n-grams only.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c111",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 111,
      "section": "4.5.2. Overview of the Trends by Methods",
      "char_start": 183288,
      "char_end": 184456,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c110",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c112",
      "semantic_coherence": 0.7276191711425781,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Consequently,\nthe overall average accuracy obtained by this approach is also higher than the average\nresults obtained by the n-gram analysis for each of the three datasets, DS1, DS2, and DS3. Considering the average accuracies obtained for all sets of n-grams, the analysis with\nn-grams and LIWC achieves the maximum on DS2 using the analysis of unigrams and\nbigrams (74.12%) and with the analysis of bigrams on DS3 (75.60%). Dataset DS1 was best\nclassi\ufb01ed using unigrams and bigrams (78.51%).\n\nBy comparing the average accuracies obtained using all machine learning models on\nall selected sets of n-grams and LIWC, the analysis using a combination of (1,2)-grams and\nLIWC gives the most accurate results (75.71%). This is followed by the analysis with 2-g\nand LIWC and (2,3)-grams and LIWC with the average accuracy of 75.29% and 75.19%,\nrespectively, on all models and datasets. Table 14 also shows that the models trained and\ntested on DS1 data give the best predictions compared to other datasets for each of the\nchosen data processing methods.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c112",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 112,
      "section": "4.5.2. Overview of the Trends by Methods",
      "char_start": 184456,
      "char_end": 185504,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c111",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c113",
      "semantic_coherence": 0.8002830743789673,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Average model accuracies were also calculated for models trained on the combined\ndataset DS3 and tested individually on DS1 or DS2 data, which is shown in Table 15. The overall average results obtained by testing the DS3 model on DS1 or DS2 data are\nalso shown. Algorithms 2023, 16, 221 28 of 34\nTable 15. Overview of trends by method for models trained on combined dataset (DS3) and tested\nindividually on DS1 and DS2 data (colored values indicate maximum accuracy obtained using a\nparticular data processing method on each of the given datasets).\n\nAll Models\nDS3 Models\nTested on Dataset\nDS1\nDS3 Models\nTested on Dataset\nDS2\nThe Average Accuracy\nObtained by Training and Testing Models\non Datasets DS1, DS2 and DS3\nLIWC 77.94 71.08 74.51\n1,1\nn-grams 70.29 64.90 67.59\nn-grams + LIWC 79.04 72.92 75.98\n1,2\nn-grams 72.12 67.71 69.91\nn-grams + LIWC 80.19 72.61 76.40\n2,2\nn-grams 71.16 69.07 70.11\nn-grams + LIWC 77.85 70.63 74.24\n1,3\nn-grams 73.56 66.77 70.16\nn-grams + LIWC 78.66 72.81 75.73\n2,3\nn-grams 75.00 65.10 70.05\nn-grams + LIWC 77.50 73.23 75.37\n3,3\nn-grams 67.12 58.96 63.04\nn-grams + LIWC 74.81 75.21 75.01\nIn this case as well, the combined analysis with n-grams and LIWC achieved the\nhighest average maximum for both datasets. When testing the DS3 models on DS1 data,\nthe average maximum of 80.19% was obtained by analyzing unigrams and bigrams and\nLIWC. While testing these models on the DS2 data using the same method, the maximum\naverage accuracy of the models was achieved by analyzing LIWC and trigrams (75, 21%).",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c113",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 113,
      "section": "4.5.2. Overview of the Trends by Methods",
      "char_start": 185504,
      "char_end": 187035,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c112",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c114",
      "semantic_coherence": 0.7711173295974731,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Considering the overall average obtained by the testing models trained on DS3 and tested\non DS1 and DS2 datasets, the best average accuracy was achieved using bigram and LIWC\n(76.40%). The second best results were achieved by analyzing unigrams and LIWC (75.98%)\nand (1,3)-grams and LIWC (75.73%). Analyses exclusively with LIWC or n-grams achieved\nworse average results on all models compared to the combined analysis. Models trained on DS3 and tested on DS1 data achieved better results than those\nmodels tested on DS2 dataset for each of the data processing methods used.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c114",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 114,
      "section": "4.5.2. Overview of the Trends by Methods",
      "char_start": 187036,
      "char_end": 187610,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c113",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c115",
      "semantic_coherence": 0.82459557056427,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "The best classi\ufb01cations of datasets DS1, DS2, and DS3, considering the average accu-\nracies obtained using all the models presented in Table 12, were achieved with the LIWC\nanalysis and by combining n-grams and LIWC. The above two methods also generalize\nbest according to testing the combined dataset DS3 individually on the DS1 and DS2\ndata (Table 13). The climate change dataset (DS1) proved to be more applicable to deception detection\ncompared to the COVID-19 dataset (DS2) and the combined dataset (DS3).\n\nMachine learn-\ning models achieved the best average results on DS1 using all data processing approaches\n(Table 14). While examining the possibility of generalization by training models on the com-\nbined dataset (DS3) and testing on individual datasets (DS1 and DS2), models also achieved\nbetter performance tested on DS1 compared to the DS2 dataset. Consequently, models both\ntrained and tested on the combined set achieved better average results compared to the\ntraining and testing on dataset DS2. It appears that DS2 data offers less information that\ncan be used in deception detection compared to other datasets.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c116",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 116,
      "section": "5.1. Datasets and Models",
      "char_start": 192044,
      "char_end": 193172,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c115",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c117",
      "semantic_coherence": 0.0551256462931633,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Given that DS2 (dataset\n\nAlgorithms 2023, 16, 221 29 of 34\nfor COVID-19) is based on answers related to partly more current and potentially more\npersonal experiences, there is a possibility that participants lied more successfully on a topic\nthat is closer to them and more subjective; therefore, the very distinction of lies using the\nselected data analysis methods became more dif\ufb01cult. In addition, there is a possibility that\nfor the same reason participants were more motivated to lie more convincingly considering\nthe scope of experience they have about COVID-19 and consequently achieved a higher\nsuccess rate of deception.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c117",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 117,
      "section": "5.1. Datasets and Models",
      "char_start": 193172,
      "char_end": 193802,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c116",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c118",
      "semantic_coherence": 0.60133296251297,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "On the other hand, data collected on the topic of climate change\nare potentially more objective, which leads to weaker results when trying to deceive. During training and testing of the machine learning models on the DS2 dataset, it was\nnoted that the analysis with LIWC (73.13%) or the combined approach (73.33%) achieves\nbetter average results than those obtained exclusively with n-grams (67.34%) (Table 12). The analysis using the combined approach on the same dataset also achieves better average\nresults on all sets of n-grams compared to the analysis exclusively with n-grams (Table 14).\n\nOn the other hand, during the training and testing models on data DS1, there was no such\nsigni\ufb01cant difference in the average accuracy of predictions using LIWC (77.45%) and the\ncombined approach analysis (77.04%) compared to the n-gram analysis (75.27%), but the\naverage results achieved using these methods are still more favorable than those obtained\nwith the n-gram analysis (Table 12). Given the above, it follows that n-grams better detect\npatterns related to deception by analyzing the DS1 compared to the DS2 dataset.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c118",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 118,
      "section": "5.1. Datasets and Models",
      "char_start": 193803,
      "char_end": 194924,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c117",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c119",
      "semantic_coherence": 0.5042475461959839,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "A statistical consideration of the number of above-average predictions that exceed those\ndefined by trends by models (Tables 12 and 13) and trends by methods (Tables 14 and 15)\nshows that logistic regression proved to be the most reliable model with the most above-\naverage values compared to other methods (Table 9). It also showed good performance\nin combination with the LIWC analysis, where it achieved the highest number of above-\naverage predictions compared to other models tested with the LIWC analysis on the same\ndatasets.\n\nThe random forest model achieved the least favorable above-average results\nwhile training and testing on the same datasets (Table 9). Examining the possibility of a\ngeneralization of the models by training them on DS3 and testing on DS1 and DS2 datasets,\nthe highest number of above-average predictions (in relation to the trend by models and\ntrend by methods) was achieved using the SVM and multinomial Naive Bayes models\nwhile the random forest showed the lowest rate of above-average predictions.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c119",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 119,
      "section": "5.1. Datasets and Models",
      "char_start": 194924,
      "char_end": 195957,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c118",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c120",
      "semantic_coherence": 0.5663612484931946,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "At this point,\nit is dif\ufb01cult to conclude as to which model is generally most applicable to the problem of\ndeception detection given that all models have shown different performances on different\ndatasets using different data analysis methods. It is important to note that the procedure for selecting important LIWC features is\nlimited to testing the models on all combinations of subsets up to the size of 11 features\nobtained using the WEKA tool (due to the factorial time complexity). The feature selection\nadopted in this study potentially needs to be improved by testing the model performance\non feature subsets larger than 11 features or by applying another feature selection method.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c120",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 120,
      "section": "5.1. Datasets and Models",
      "char_start": 195957,
      "char_end": 196646,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c119",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c121",
      "semantic_coherence": 0.40944918990135193,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "The models trained on the DS3 dataset, when tested on DS1, achieved a less favorable\naverage accuracy with the n-gram analysis (71.54%) compared to the models trained and\ntested on DS1 (75.27%). Conversely, by testing the models based on DS3 on the dataset DS1\nwith the analysis using a combined approach, better average results (78.01%) were obtained\ncompared to training and testing the models on the DS1 data using the same processing\ntechnique (77.04%) (Tables 12 and 13). The same trend applies to the LIWC analysis, which\nalso achieved better results.\n\nFor model DS2, the analysis with LIWC or a combination of\nn-grams and LIWC generalizes better compared to an analysis exclusively with n-grams\nsince it achieves better performance during training models on several different datasets\nfrom different domains.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c121",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 121,
      "section": "5.2. Generalization",
      "char_start": 201300,
      "char_end": 202115,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c120",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c122",
      "semantic_coherence": 0.37592414021492004,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "On the other hand, by testing the models obtained based on the DS3 dataset on the data\nDS2, an average drop in performance was recorded using all models and all data analyzing\nmethods compared to both training and testing the models on the dataset DS2, but by\n\nAlgorithms 2023, 16, 221 30 of 34\ncombining the analysis with n-grams and LIWC, all models achieved higher maximums\naccuracies. The LIWC analysis on all models gives an average accuracy of 71.08%, which is",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c122",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 122,
      "section": "5.2. Generalization",
      "char_start": 202115,
      "char_end": 202581,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c121",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c123",
      "semantic_coherence": 0.8106396794319153,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "recorded an average drop in the accuracy of predictions of 1.92%, and the smallest average\ndrop was achieved with the analysis with a combined approach (0.43%) (Tables 12 and 13),\nwhich, on the other hand, for certain n-grams, achieved higher maxima in relation to the\nmodels trained on the DS2 dataset (Table 7). Although the average accuracy when training\nall models on the combined DS3 dataset slightly drops, it again follows that combining the\nanalysis with n-grams and LIWC has somewhat greater generalization power compared\nto exclusively using the n-gram analysis. In addition, the analysis with the combined\napproach, unlike the analysis with n-grams, achieved better average results on almost all\nsets of n-grams during the testing of the models based on the DS3 dataset on the DS1 and\nDS2 data (Table 15). The biggest difference in the performance of the models obtained with\nthe n-gram analysis and the combined approach of analysis is visible during the testing\nof models based on the DS3 on the DS2 data by analyzing trigrams where the combined\napproach achieves a 16.24% higher accuracy.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c123",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 123,
      "section": "2.05% less than in the case of training the model on DS2 data. The analysis with n-grams",
      "char_start": 203904,
      "char_end": 205006,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c122",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c124",
      "semantic_coherence": 0.8069124817848206,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Examining the possibility of mutual applicability by testing the models obtained based\non the datasets DS1 and DS2 on the datasets DS2 and DS1, it was concluded that the n-gram\nanalysis gives the best average predictions compared to other data analyzing methods. When testing models based on the DS2 on the DS1 data, the n-gram analysis even achieves\nan increase in the average accuracy of all models compared to training and testing the\nmodels on the DS1 data (7.77%). The LIWC analysis, on the other hand, shows the least\napplicability to other datasets.\n\nAs a result of the stated claims, the analysis by combining\nn-grams and LIWC achieves less favorable applicability than the analysis with n-grams,\nbut it is still better than the analysis with LIWC. It can be concluded that n-grams are the\nmost robust analysis method that allows the greatest \ufb02exibility when applying models\ntrained on one dataset to another.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c124",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 124,
      "section": "5.3. Model Applicability",
      "char_start": 206288,
      "char_end": 207205,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c123",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c125",
      "semantic_coherence": 0.8408442735671997,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Deception detection has turned out to be quite a demanding problem considering\nthe various inherent limitations. The \ufb01rst problem encountered is \ufb01nding validly labeled\ntrue and false data. Given that there is no absolutely reliable method of verifying the\nveracity of the same information, such a problem was approached in this study with\ncaution, taking into account the amount of motivation and sincerity the people demonstrate\nwhen answering the de\ufb01ned survey questions during the data collection procedure.\n\nMachine learning models proved to be successful when working on these collected\ndatasets, which once again con\ufb01rmed the existence of hidden linguistic features of decep-\ntion present in verbal communication. Natural language processing methods achieved\nsatisfactory results while LIWC analysis and the combined analysis of n-grams and LIWC\nproved to be the most successful in deception detection. By examining the possibility of\ngeneralization, the mentioned methods also achieved better performance compared to\nthe analysis with n-grams, especially on trigrams, where the analysis using a combined\napproach achieved signi\ufb01cantly better prediction compared to the analysis exclusively with\nn-grams.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c125",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 125,
      "section": "6. Conclusions",
      "char_start": 208172,
      "char_end": 209382,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c124",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c126",
      "semantic_coherence": 0.4469146728515625,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Such detected generalization is in line with the recent research efforts leading\nto conclusions that \u201cdeceptive text is more formulaic and less varied than truthful text\u201d,\nhowever more research in this area is warranted [20]. However, during the testing of the\nmutual applicability of the models, the n-gram analysis proved to be a more robust method. While testing the performance of the models obtained based on the DS1 dataset on the\nDS2 data, the n-gram analysis achieved better results compared to training and testing the\nmodels on own data (DS1).\n\nOn the other hand, the LIWC analysis proved to be the most\nin\ufb02exible when it came to its applicability to other datasets. Algorithms 2023, 16, 221 31 of 34\nMachine learning models with LIWC can be used independently in the detection of\ndeception with certain limitations, such as the fact that they are much less applicable to the\ndatasets on which they have not been trained. The combined approach with n-grams and\nLIWC in most cases allows for even more successful differentiation between truth and lies.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c126",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 126,
      "section": "6. Conclusions",
      "char_start": 209382,
      "char_end": 210443,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c125",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c127",
      "semantic_coherence": 0.7849737405776978,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "Therefore, machine learning models and the approach to deception detection should be\ncarefully selected depending on the speci\ufb01cs of the given problem, such as the need for\napplicability to other datasets or the possibility of generalization. In addition to the scienti\ufb01c value of this study, the approach has practical advantages. Unlike state-of-the-art deep learning models, which can be computationally intensive\nand time-consuming, our approach requires less computational resources and is faster.\n\nThis makes it more accessible and feasible for researchers and practitioners who may\nnot have access to high-performance computing resources or who need to process large\namounts of data quickly. Moreover, the ef\ufb01ciency of our approach does not compromise its\neffectiveness in detecting deception. By combining psycho-linguistic analysis and n-grams,\nour study achieves high classi\ufb01cation accuracy, especially on trigrams, which is comparable\nto state-of-the-art models. Therefore, our approach not only provides a more practical\nsolution, but also delivers comparable results to more resource-intensive methods.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c127",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 127,
      "section": "6. Conclusions",
      "char_start": 210444,
      "char_end": 211559,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c126",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c128",
      "semantic_coherence": 0.7084405422210693,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Furthermore, our study highlights the importance of considering the subjectivity of\nthe data when analyzing it for deception, which is a crucial factor in the effectiveness of any\nmachine learning model. By exploring the use of a psycho-linguistic analysis, our study\nalso demonstrates the potential bene\ufb01ts of incorporating linguistic features beyond just\nn-grams, which could inspire further research in this area.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c128",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 128,
      "section": "6. Conclusions",
      "char_start": 211559,
      "char_end": 211975,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c127",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c129",
      "semantic_coherence": 0.8378837704658508,
      "has_citations": false,
      "topic_cluster": 0
    },
    {
      "text": "Author Contributions: Conceptualization, I.B. and B.B.; Methodology, I.B. and B.B; Software, B.B.;\nValidation, I.B., B.B. and M.B.B.; Formal Analysis, B.B.; Investigation, I.B. and B.B.; Resources, I.B.;\nData Curation, B.B.; Writing\u2014Original Draft Preparation, B.B.; Writing\u2014Review & Editing, B.B., I.B.\nand M.B.B.; Visualization, B.B.; Supervision, I.B.; Project Administration, I.B.; Funding Acquisition,\nI.B. All authors have read and agreed to the published version of the manuscript. Funding: This research received no external funding. Data Availability Statement: The data presented in this study are available on request from the\ncorresponding author.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c129",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 129,
      "section": "6. Conclusions",
      "char_start": 211976,
      "char_end": 212635,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c128",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c130",
      "semantic_coherence": 0.1576937586069107,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "The data are not publicly available due to the duration of the review process,\nafter which they will be made available as a public repository. Con\ufb02icts of Interest: The authors declare no con\ufb02ict of interest. Appendix A\nReference Main Findings\nDePaulo et al., 1996 [1] Individuals lie frequently, with most of the lies being\nminor and aimed at protecting the self or others. Zhou et al., 2004 [2] SVM and NB classi\ufb01ers performed the best in detecting\ndeception in computer-mediated communication.\n\nHancock et al., 2005 [3]\nDeceptive and truthful messages can be distinguished\nusing linguistic features such as words, syntax,\nand punctuation. Ott et al., 2011 [4]\nAn algorithm that combines lexical, syntactic, and\ndiscourse features can accurately identify deceptive\nopinion spam. Feng et al., 2012 [5] A method that uses syntax-based features can achieve\nhigh accuracy in detecting deceptive language. Algorithms 2023, 16, 221 32 of 34\nReference Main Findings\nP\u00e9rez-Rosas et al., 2015 [6]\nLinguistic cues can help identify deception in real-life trial\ndata, and combining different types of features can lead to\nbetter performance. Poesio and Fornaciari, 2018 [7] A combination of lexical, syntactic, and semantic features\ncan achieve high accuracy in identifying deceptive text.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c130",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 130,
      "section": "6. Conclusions",
      "char_start": 212636,
      "char_end": 213916,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c129",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c131",
      "semantic_coherence": 0.19121989607810974,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "Mihalcea and Strapparava, 2009 [8] Lexical, syntactic, and discourse features can be used to\nautomatically recognize the deceptive language. Mahon, 2016 [9] The concepts of lying and deception are complex, and\ndifferent de\ufb01nitions have been proposed. Van Swol et al., 2015 [11]\nPeople are more likely to detect deception in face-to-face\ncommunication than in\ncomputer-mediated communication. Hancock et al., 2008 [12] Deception involves more cognitive effort than truth-telling,\nand deception can be detected using linguistic features.\n\nBurgoon and Buller, 2015 [13]\nThis theory proposes that deception is a process that\ninvolves both the deceiver and the receiver, and that\ncommunication is a strategic game. Alowibdi et al., 2015 [18] A machine learning approach can effectively detect\ndeceptive tweets. Barsever et al., 2020 [19] BERT-based models can be effectively used for\nlie detection. Fornaciari et al., 2021 [20] The BERTective model integrates contextual information\nand language models for improved deception detection. Appendix B",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c131",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 131,
      "section": "6. Conclusions",
      "char_start": 213917,
      "char_end": 214959,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c130",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c132",
      "semantic_coherence": 0.8398755192756653,
      "has_citations": true,
      "topic_cluster": 0
    },
    {
      "text": "DS1:\nClimate change\nAllnone Allnone Allnone Allnone\nCulture Culture Culture Culture",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c132",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 132,
      "section": "LR SVM NB RF",
      "char_start": 221776,
      "char_end": 221859,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c131",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c133",
      "semantic_coherence": 0.03042636811733246,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "They They They They\nBigWords Negate Negate Negate\nClout Insight Insight Insight\nConj Politic Conj Politic\ncuriosity WPS WPS WPS\nDS2: COVID-19\nFamily Family Family Family\nExclam Exclam Exclam Exclam\nApostro Apostro Apostro Apostro",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c133",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 133,
      "section": "I I I I",
      "char_start": 221970,
      "char_end": 222199,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c132",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c134",
      "semantic_coherence": 0.051384858787059784,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "Allnone Allnone Allnone Allnone\nComma Comma Comma Comma\nBigWords BigWords Negate BigWords\nNegate Work Negate\nCause Cause\nSocrefs Socrefs\nTime Time\nnetspeak netspeak\n\nAlgorithms 2023, 16, 221 33 of 34",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c134",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 134,
      "section": "WPS WPS WPS WPS",
      "char_start": 222446,
      "char_end": 222645,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c133",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c135",
      "semantic_coherence": 0.3671754002571106,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "DS3:\nClimate change +\nCOVID-19\nTone Tone WC Tone\nWPS BigWords\nThey WPS You WPS\nConj BigWords They BigWords\nNegate Allnone\nInsight Prosocial\nFamily\nThey Negate They\nPolitic Conj Allnone Conj\nWork Curiosity\nMotion Comma\nExclam\nNegate Insight Negate\nAllnone Family Allnone\nInsight\nProsocial\nFamily\nInsight\nPolitic Prosocial\nWork Family\nCuriosity Politic\nWork\nCuriosity\nMotion\nComma\nExclam\nReferences",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c135",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 135,
      "section": "LR SVM NB RF",
      "char_start": 222878,
      "char_end": 223274,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c134",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c136",
      "semantic_coherence": 0.4542996287345886,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "979\u2013995. [CrossRef] [PubMed]",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c136",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 136,
      "section": "1. DePaulo, B.M.; Kirkendol, S.E.; Kashy, D.A.; Wyer, M.M.; Epstein, J.A. Lying in Everyday Life. J. Pers. Soc. Psychol.1996, 70,",
      "char_start": 223698,
      "char_end": 223726,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c135",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c137",
      "semantic_coherence": 0.051251862198114395,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "in computer-mediated communication. J. Manag. Inf. Syst.2004, 20, 139\u2013166. [CrossRef]",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c137",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 137,
      "section": "2. Zhou, L.; Burgoon, J.K.; Twitchell, D.P .; Qin, T.; Nunamaker, J.F. A comparison of classi\ufb01cation methods for predicting deception",
      "char_start": 224016,
      "char_end": 224101,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c136",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c138",
      "semantic_coherence": 0.14186754822731018,
      "has_citations": false,
      "topic_cluster": 7
    },
    {
      "text": "computer-mediated communication. In Proceedings of the 38th Annual Hawaii International Conference on System Sciences, Big\nIsland, HI, USA, 3\u20136 June 2005; p. 22. [CrossRef]",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c138",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 138,
      "section": "3. Hancock, J.T.; Curry, L.; Goorha, S.; Woodworth, M. Automated linguistic analysis of deceptive and truthful synchronous",
      "char_start": 224456,
      "char_end": 224628,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c137",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c139",
      "semantic_coherence": 0.72310471534729,
      "has_citations": false,
      "topic_cluster": 7
    },
    {
      "text": "the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Portland, OR, USA,\n19\u201324 June 2011; Volume 1, pp. 309\u2013319.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c139",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 139,
      "section": "4. Ott, M.; Choi, Y.; Cardie, C.; Hancock, J.T. Finding deceptive opinion spam by any stretch of the imagination. In Proceedings of",
      "char_start": 225048,
      "char_end": 225209,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c138",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c140",
      "semantic_coherence": 0.32035332918167114,
      "has_citations": false,
      "topic_cluster": 8
    },
    {
      "text": "Association for Computational Linguistics, Jeju Island, Republic of Korea, 8\u201314 July 2012; Volume 2, pp. 171\u2013175.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c140",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 140,
      "section": "5. Feng, S.; Banerjee, R.; Choi, Y. Syntactic stylometry for deception detection. In Proceedings of the 50th Annual Meeting of the",
      "char_start": 225636,
      "char_end": 225749,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c139",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c141",
      "semantic_coherence": 0.8029192090034485,
      "has_citations": false,
      "topic_cluster": 8
    },
    {
      "text": "ACM on International Conference on Multimodal Interaction, Washington, DC, USA, 9\u201313 November 2015; pp. 59\u201366. [CrossRef]",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c141",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 141,
      "section": "6. P\u00e9rez-Rosas, V .; Abouelenien, M.; Mihalcea, R.; Burzo, M. Deception detection using real-life trial data. In Proceedings of the 2015",
      "char_start": 226126,
      "char_end": 226247,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c140",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c142",
      "semantic_coherence": 0.12374337017536163,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "assets/Deception_Detection_with_NLP .pdf (accessed on 25 April 2023).",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c142",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 142,
      "section": "7. Poesio, M.; Fornaciari, T. Detecting Deception in Text Using NLP Methods. Available online: https://research.signal-ai.com/",
      "char_start": 226644,
      "char_end": 226713,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c141",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c143",
      "semantic_coherence": 0.1033320501446724,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "the ACL-IJCNLP 2009 Conference, Suntec, Singapore, 4 August 2009; pp. 309\u2013312.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c143",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 143,
      "section": "8. Mihalcea, R.; Strapparava, C. The lie detector: Explorations in the automatic recognition of deceptive language. In Proceedings of",
      "char_start": 227038,
      "char_end": 227116,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c142",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c144",
      "semantic_coherence": 0.26962772011756897,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "Stanford University: Stanford, CA, USA, 2016.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c144",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 144,
      "section": "9. Mahon, J.E. The De\ufb01nition of Lying and Deception. In The Stanford Encyclopedia of Philosophy (Winter 2016 Edition); Zalta, E.N., Ed.;",
      "char_start": 227464,
      "char_end": 227509,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c143",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c145",
      "semantic_coherence": 0.2598503530025482,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "1973; p. 322. ISBN 978-0226385129.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c145",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 145,
      "section": "10. Isenberg, A. Aesthetics and the theory of criticism: Selected essays of Arnold Isenberg; University of Chicago Press: Chicago, NY, USA,",
      "char_start": 227830,
      "char_end": 227864,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c144",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c146",
      "semantic_coherence": 0.15803386270999908,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "Communication. Commun. Res.2015, 42, 1116\u20131142. [CrossRef]",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c146",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 146,
      "section": "11. Van Swol, L.M.; Braun, M.T.; Kolb, M.R. Deception, Detection, Demeanor, and Truth Bias in Face-To-Face and Computer-Mediated",
      "char_start": 228180,
      "char_end": 228238,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c145",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c147",
      "semantic_coherence": 0.1428951621055603,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "mediated communication. Discourse Process.2008, 45, 1\u201323. [CrossRef]",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c147",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 147,
      "section": "12. Hancock, J.T.; Curry, L.E.; Goorha, S.; Woodworth, M. On lying and being lied to: A linguistic analysis of deception in computer-",
      "char_start": 228556,
      "char_end": 228624,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c146",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c148",
      "semantic_coherence": 0.4744253158569336,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "Psychol. Sci. Univ. Tex. Austin Dev.2007, 1, 1\u201322. [CrossRef]",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c150",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 150,
      "section": "15. Pennebaker, J.W.; Chung, C.K.; Ireland, M.; Gonzales, A.; Booth, R.J. The Development and Psychometric Properties of LIWC2007.",
      "char_start": 229434,
      "char_end": 229495,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c149",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c151",
      "semantic_coherence": 0.07526089251041412,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "https://www.livc.app (accessed on 25 April 2023). Algorithms 2023, 16, 221 34 of 34",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c151",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 151,
      "section": "16. Boyd, R.; Ashokkumar, A.; Seraj, S.; Pennebaker, J. The Development and Psychometric Properties of LIWC-22. Available online:",
      "char_start": 229820,
      "char_end": 229903,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c150",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c152",
      "semantic_coherence": 0.25844112038612366,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "Soc. Psychol.2010, 29, 24\u201354. [CrossRef]",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c152",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 152,
      "section": "17. Tausczik, Y.R.; Pennebaker, J.W. The psychological meaning of words: LIWC and computerized text analysis methods.J. Lang.",
      "char_start": 230250,
      "char_end": 230290,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c151",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c153",
      "semantic_coherence": 0.13942809402942657,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "[CrossRef]",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c153",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 153,
      "section": "18. Alowibdi, J.S.; Buy, U.A.; Yu, P .S.; Ghani, S.; Mokbel, M. Deception detection in Twitter. Soc. Netw. Anal. Min. 2015, 5, 32.",
      "char_start": 230584,
      "char_end": 230594,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c152",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c154",
      "semantic_coherence": 0.30452826619148254,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "of the 2020 International Joint Conference on Neural Networks (IJCNN), Glasgow, UK, 19\u201324 July 2020. [CrossRef]",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c154",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 154,
      "section": "19. Barsever, D.; Singh, S.; Neftci, E. Building a Better Lie Detector with BERT: The Difference between Truth and Lies. In Proceedings",
      "char_start": 230868,
      "char_end": 230979,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c153",
      "next_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c155",
      "semantic_coherence": 0.1463259607553482,
      "has_citations": false,
      "topic_cluster": -1
    },
    {
      "text": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, Online, 19\u201323\nApril 2021. [CrossRef]\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.",
      "chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c155",
      "source_file": "Brzic_2023_Detecting_Deception_Using_Natural.pdf",
      "page": 34,
      "chunk_index": 155,
      "section": "20. Fornaciari, T.; Bianchi, F.; Poesio, M.; Hovy, D. BERTective: Language models and contextual information for deception detection.",
      "char_start": 231364,
      "char_end": 231882,
      "prev_chunk_id": "Brzic_2023_Detecting_Deception_Using_Natural_p34_c154",
      "next_chunk_id": null,
      "semantic_coherence": 0.24405965209007263,
      "has_citations": false,
      "topic_cluster": 8
    }
  ]
}