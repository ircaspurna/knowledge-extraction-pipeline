{
  "text": "RESEARCH ARTICLE\nAutomated verbal credibility assessment of intentions: The\nmodel statement technique and predictive modeling\nBennett Kleinberg1 | Yaloe van der Toolen1 | Aldert Vrij2 | Arnoud Arntz1 |\nBruno Verschuere1\n1 Department of Psychology, University of\nAmsterdam, Amsterdam, The Netherlands\n2 Department of Psychology, University of\nPortsmouth, Portsmouth, UK\nCorrespondence\nBennett Kleinberg, University of Amsterdam,\nDepartment of Psychology, Nieuwe\nAchtergracht 129B, 1018WS Amsterdam, The\nNetherlands.\nEmail: b.a.r.kleinberg@uva.nl\nFunding information\nDutch Ministry of Security and Justice\nSummary\nRecently, verbal credibility assessment has been extended to the detection of deceptive inten-\ntions, the use of a model statement, and predictive modeling. The current investigation com-\nbines these 3 elements to detect deceptive intentions on a large scale. Participants read a\nmodel statement and wrote a truthful or deceptive statement about their planned weekend\nactivities (Experiment 1). With the use of linguistic features for machine learning, more than\n80% of the participants were classified correctly. Exploratory analyses suggested that liars\nincluded more person and location references than truth \u2010tellers. Experiment 2 examined\nwhether these findings replicated on independent\u2010sample data. The classification accuracies\nremained well above chance level but dropped to 63%. Experiment 2 corroborated the finding\nthat liars' statements are richer in location and person references than truth\u2010tellers' statements.\nTogether, these findings suggest that liars may over \u2010prepare their statements. Predictive\nmodeling shows promise as an automated veracity assessment approach but needs validation\non independent data.\nKEYWORDS\ncredibility assessment, intentions, machine learning, model statement, verbal deception detection\n1 | INTRODUCTION\nOn March 22, 2016, two suicide bombers detonated nail bombs at\nBrussels Airport in Zaventem, killing and seriously injuring many inno-\ncent civilians. In the aftermath of the terror attack, officials expressed\nconcerns about the level of security, pointing to systematic security\nflaws and insufficient staff training at Brussels Airport (Bilefsky, 2016).\nThis incident suggests that an additional screening of passengersbefore\nthey arrive at the airport could be vital for the detection of aviation secu-\nrity threats. Although many existing methods aim at safeguarding avia-\ntion security, concerns have been voiced about the validity of these\nmethods (Meijer, Verschuere, & Merckelbach, 2017; Ormerod & Dando,\n2015). More research regarding the screening of airport passengers is\nneeded to improve aviation safety. One possible line of inquiry is to\nexplore whether one can differentiate between true and false intentions\n(Jupe, Leal, Vrij, & Nahari, 2017; Vrij, Granhag, Mann, & Leal, 2011).\n1.1 | Verbal deception detection\nAmong the more promising approachesto detect deception is examining\nthe verbal content to discern truthful from deceptive statements (Bond &\nDePaulo, 2006; Oberlader et al., 2016). Verbal deception detection is\nrooted in the assumption that the verbal account of an event is informa-\ntive about the veracity of that account. For example, genuine experiences\nare often reported differently than fabricated experiences, one of the\ncore assumptions of reality monitoring (RM, Johnson & Raye, 1981).\nRM states that the differences are attributable to the process by which\nthe memory of an event is constructed: Memories of truthfully experi-\nenced events have been obtained through perceptual processes, whereas\nfabricated memories were built through cognitive operations. Deception\nresearchers adopted this idea and found promising results for verbal\ndeception detection (Johnson, Bush, & Mitchell, 1998). Meta\u2010analytical\nfindings support the notion that visual, auditory, and temporal details\n------------------------------------------------------------------------------------------------------------------------------- -\nThis is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided\nthe original work is properly cited.\n\u00a9 2018 The Authors Applied Cognitive Psychology Published by John Wiley & Sons Ltd.\nReceived: 10 November 2017 Revised: 15 February 2018 Accepted: 20 February 2018\nDOI: 10.1002/acp.3407\n354 Appl Cognit Psychol. 2018;32:354\u2013366.wileyonlinelibrary.com/journal/acp\n\n\nare useful in distinguishing truthful from deceptive accounts (Masip,\nSporer, Garrido, & Herrero, 2005). Accuracy rates of classifying liars from\ntruth\u2010tellers based on these variables are above chance level and range\nfrom 63% to 82% (Masip et al., 2005; Vrij, Fisher, & Blank, 2017; and\nsee also Levine, Blair, & Carpenter, 2017; Vrij, Blank, & Fisher, 2018;).\n1.2 | Detecting deceptive intentions\nFor many years, deception research focused on people lying about\ntheir past actions (e.g., what someone was doing during the time of a\ncrime). Since recently, attention is also paid to the detection of decep-\ntive intentions (Mac Giolla, Granhag, & Liu\u2010J\u00f6nsson, 2013; Sooniste,\nGranhag, Knieps, & Vrij, 2013; Warmelink, Vrij, Mann, & Granhag,\n2013). There are indications that the principles that apply to the detec-\ntion of deception on past events also apply to deceptive intentions\n(Granhag & Mac Giolla, 2014). When truth\u2010tellers report a past event,\nthey can rely on their memory, whereas liars cannot if they discuss an\nevent they have never experienced. A similar logic may apply to lying\nabout intentions. Plans for future actions that are not accompanied\nby an intention to execute result in a less detailed mental image of\nthe event than plans that are accompanied by the enactment inten-\ntions (Granhag & Knieps, 2011; Szpunar, 2010). It is important to note,\nhowever, that past events are imagined in more detail than future\nevents (D'Argembeau & Van der Linden, 2004; Gamboz et al., 2010).\nCues for deception concerning intentions might, therefore, be less\nclear compared with those for past events.\nTo date, research into the verbal approach to the detection of\ndeceptive intentions has examined different verbal cues with some-\ntimes contradicting findings. In one study, passengers at international\nairports were instructed to lie or tell the truth about their forthcom-\ning trip (Vrij et al., 2011). Those who lied about their journey\nprovided statements that were less plausible and included more con-\ntradictions than truthful statements but did not differ in the amount\nof detail. Building on the notion that the expectedness of the ques-\ntions asked might moderate the effectiveness of the verbal deception\ndetection approach (Vrij & Granhag, 2012), another series of experi-\nments asked participants expected and unexpected questions about\na fabricated or truthful future event (Fenn, McGuire, Langben, &\nBland\u00f3n\u2010Gitlin, 2015; Warmelink et al., 2013; Warmelink, Vrij, Mann,\nJundi, & Granhag, 2012). Although differences in the amount of detail\nemerged in some studies when unanticipated questions were asked\n(Sooniste et al., 2013; Warmelink et al., 2013), these effects were\nabsent in other studies (Fenn et al., 2015; Kleinberg, Nahari, Arntz,\n& Verschuere, 2017). In yet another study, it was found that markers\nof good planning behavior (e.g., effective time allocation andhow an\naction will be carried out) were more prevalent in truthful than in\ndeceptive statements (Mac Giolla et al., 2013). Conversely, deceptive\nstatements contained more justifications for the actions (i.e.,why an\naction will be carried out). Furthermore, a recent study reported that\ndeceptive intentions contained fewer verifiable details than truthful\nones (Jupe et al., 2017). Taken together, the literature on the detec-\ntion of deceptive intentions suggests that the verbal approach could\nbe promising and that the richness of detail might be a useful cue\nto deception.\n1.3 | The model statement technique\nA model statement is a detailed example of a verbal statement given by\nsomeone on a topic unrelated to the current research context, and pro-\nviding such a statement may help to increase verbal differences between\ntruth\u2010tellers and liars. By reading a detailed example before providing\ntheir account, interviewees are thought to learn the level of detail that\nis expected from their statement, which in turn makes them inclined to\nprovide more detail. Providing more detailed information should be eas-\nier for truth\u2010tellers than for liars: The former could easily retrieve details\nfrom their memory of a specific event, whereas liars struggle to include\nsufficient detail to match the expectations raised by the model state-\nment (Vrij, Fisher, & Blank, 2017; Vrij, Hope, & Fisher, 2014). Besides,\nliars will likely not provide more detailed information after reading a\nmodel statement because the provision of extra information could lead\nto cues that give away their lie (e.g., incriminating information, Nahari,\nVrij, & Fisher, 2014) or expose the lack of contextual information in their\naccount (Vrij, Fisher, & Blank, 2017).\nThere are mixed findings as to the usefulness of the model\nstatement method so far. On the one hand, the provision of a model\nstatement led to lengthier statements and better truth\u2013lie discrimina-\ntion (i.e., truthful statements were more plausible; see Leal, Vrij,\nWarmelink, Vernham, & Fisher, 2015). Another study found that the\ndiscrimination between truthful and deceptive insurance claims based\non the number of verifiable details improved with a model statement\n(Harvey, Vrij, Leal, Lafferty, & Nahari, 2017). Moreover, a model state-\nment benefited detection accuracy when details inferred from behavior\nscripts (e.g.,\u201cwe went to the restaurant and ordered food and some-\nthing to drink\u201d) and complications were counted (Vrij, Leal, et al.,\n2017). These studies suggest that the model statement aids deception\ndetection when the quality of information (e.g., plausibility, verifiability,\nand number of complications) is measured. On the other hand, several\nother studies have not found support for the beneficial role of a model\nstatement when the quantity of details is examined. In Bogaard, Meijer,\nand Vrij (2014), a model statement led to lengthier statements but did\nnot benefit the discrimination between truth\u2010tellers and liars with com-\nmonly used verbal content analysis tools measuring quantity of detail (e.\ng., RM). Likewise, there was no evidence to the beneficial effects of the\nprovision of a model statement for the amount of\u201ctotal details\u201d (Ewens\net al., 2016) nor for the statement quantity in children and adolescents\n(Brackmann, Otgaar, Roos af Hjelms\u00e4ter, & Sauerland, 2017). In sum,\nthere are indications that a model statement may improve verbal\ndeception detection when examining verbal aspects other than the\nquantity of details. Importantly, although some studies failed to find\nan effect of the model statement, no study indicated that a model state-\nment impeded deception detection, and regarding quantity of details,\nseveral studies showed that it increased the information provided (e.\ng., Bogaard et al., 2014; Leal et al., 2015). The current study tests\nwhether the model statement technique can facilitate the detection\nof truthful and deceptive intentions.\n1.4 | Large\u2010scale deception detection\nIn a setting such as prospective airport passenger screening, large\u2010scale\ndeception detection may be only applicable when data can be\nKLEINBERG ET AL. 355\n\ncollected and analyzed automatically (Kleinberg, Arntz, & Verschuere,\nin press). A key challenge for verbal deception detection is then the\ntransition from manual, human coding of verbal content towards\ncomputer\u2010automated approaches. Although these two methodological\nlines have the same goal of identifying deceptive and truthful content,\nthey both have different advantages and shortcomings (e.g., Hauch,\nBland\u00f3n\u2010Gitlin, Masip, & Sporer, 2015). First, the manual annotation\nof a text is limited in its large\u2010scale potential because it relies on\ninstructed human coders. The efforts and time involved in the human\ncoding approach make it virtually unfit for the assessment of vast\nnumbers of statements in near real time (e.g., in airport settings).\nComputer\u2010automated approaches are less affected by this require-\nment and can be scaled up and allow for text analysis in real time\n(for a review, see Fitzpatrick, Bachenko, & Fornaciari, 2015). Second,\ninherent to the involvement of human assessors in manual coding is\nthe lack of perfect reliability of the judgments made. Contrary to\ncomputer\u2010automated approaches, the agreement between multiple\nhumans is never entirely perfect and therefore might pose a\nthreat to the validity. Because we are particularly interested in\npotential large\u2010scale applications, we resort to computer\u2010automated\nmethods as a primary analytical tool in the current study. Several\nmethods have been proposed to integrate verbal deception theory\nand computer\u2010automated analysis.\n1.4.1 | Linguistic Inquiry and Word Count\nThe Linguistic Inquiry and Word Count (LIWC) software (Pennebaker,\nBoyd, Jordan, & Blackburn, 2015) examines the proportion of words\nbelonging to one of 92 categories. The attractiveness of the LIWC is\nthat the categories are thought to represent psycholinguistic processes\nsuch as the emotional tone of a text (e.g.,\u201clucky\u201d and \u201cmelancholic\u201d)o r\nthe number of cognitive processes in a text (e.g.,\u201cknow\u201d and \u201cought\u201d).\nEach word category is composed of a comprehensive dictionary, and\nthe analytical task consists of counting the number of words per cate-\ngory. Several studies have successfully used the LIWC to discriminate\nlies from truths (Bond & Lee, 2005; Kleinberg, Mozes, Arntz, &\nVerschuere, 2017; Mihalcea & Strapparava, 2009; Ott, Choi, Cardie,\n& Hancock, 2011; P\u00e9rez\u2010Rosas & Mihalcea, 2014).\n1.4.2 | Named entity recognition\nRecently, it has been proposed to use named entities in verbal decep-\ntion detection (Kleinberg, Mozes, et al., 2017; Kleinberg, Nahari, &\nVerschuere, 2016). Named entity recognition (NER) is an information\nextraction method that identifies and classifies information from natu-\nral language into predefined categories (e.g., persons, dates, and times).\nTruthful statements are expected to contain more named entities than\ndeceptive statements because truthful accounts (a) are typically richer\nin detail (Johnson et al., 1998; Masip et al., 2005), (b) contain more\nverifiable details (Nahari et al., 2014), and (c) are often more contextu-\nally embedded (K\u00f6hnken, 2004). The named entity\u2010based approach\nhas been shown to be useful for the identification of deceptive and\ntruthful hotel reviews (Kleinberg, Mozes, et al., 2017). These findings\nsuggest that named entities might be a means to measure the liars'\nstrategy of withholding potentially incriminating information (e.g.,\npersons that could be consulted to verify an alibi), resulting in liars'\nmentioning fewer named entities.\n1.5 | The current study\nWe investigated whether it is possible to detect truthful and decep-\ntive statements about planned activities in a computer\u2010automated\nverbal deception detection workflow (i.e., automated data collection\nand automated text analysis). Because the majority of verbal decep-\ntion research has been conducted regarding past activities, we also\nincluded a comparison condition of participants who provided a\ntruthful or deceptive statement about their recent activities (Experi-\nment 1). To enhance verbal differences, we provided all participants\nwith a model statement in Experiment 1 and experimentally investi-\ngated the provision of the model statement in Experiment 2.\nIn the first experiment, there were four conditions. In the two\ntruthful conditions, participants were instructed to tell the truth about\ntheir (a) forthcoming or (b) past weekend. In the two deceptive condi-\ntions, participants were instructed to lie about an activity assigned to\nthem (c) for the forthcoming or (d) about the past weekend. The main\nfocus of this study was the automated detection of deception. All\nstatements were therefore coded automatically using the LIWC and\nnamed entity approaches. Because human coding is the standard in\nthe majority of psycholegal deception studies, we added manual anno-\ntations on a subset (40%) of the statements of Experiment 1.\nWe expected several main effects of veracity. On the basis of the\ntheory of RM and the idea that richer mental images accompany gen-\nuinely planned activities, it was expected that truthful statements\nwould be lengthier (dependent variable [DV]: no. of words), be richer\nin detail (DV: richness of detail measured via LIWC and human coding),\ncontain more specific information (DV: named entities), and be more\nplausible (DV: human\u2010coded plausibility) than deceptive statements.\nWe also expected that truthful statements would contain more refer-\nences tohow (DV: human\u2010coded how\u2010utterances) an activity was exe-\ncuted and fewer justifications of the actions (i.e.,why they executed an\nactivity, DV: human\u2010coded why\u2010utterances) than deceptive statements\n(Mac Giolla et al., 2013). Last, we expected that the difference\nbetween truthful and deceptive statements would be more pro-\nnounced for statements about the past than for statements about\nthe future (interaction hypothesis). In the exploratory analysis, we\nlooked at machine learning classification of truthful and deceptive\nstatements and examined individual linguistic predictors.\n1.6 | Data availability statement\nThe confirmatory analyses for the two experiments were pre-\nregistered before data collection. The preregistrations, data, and\nsupporting information are available at https://osf.io/wqc4p/. The\nsource code to the experimental tasks is available at https://github.\ncom/ben\u2010aaron188/verbal_deception_past_future.\n2 | EXPERIMENT 1\nThe local institutional review board approved both experiments\n(#2016\u2010CP\u20107306).\n356 KLEINBERG ET AL.\n\n2.1 | Method\n2.1.1 | Participants\nData were collected through the online crowdsourcing website Prolific\nAcademic (https://www.prolific.ac/) where we opened spots for 327\nparticipants. Participation was open to all participants who were native\nEnglish speakers and had not partaken in previous pilot studies. To\nensure that participants had concrete weekend plans, we collected\ndata just before a weekend (Thursday and Friday). All participants were\nreimbursed with GBP1.50 for this study. Due to simultaneous starting\ntimes, we collected data from 347 participants on which we applied\nfour preregistered exclusion criteria: double IP addresses (n = 23),\nnoncomplete data (n = 4), not following the instructions (n = 0), and\nfailing the manipulation check (i.e., not recalling the instructions after\nwriting the statement,n = 28; all participants were asked\u201cHow were\nyou instructed to write your statement?\u201d on a scale from 0 = answer\ntruthfully to 100 = answer deceptively; we excluded those who indi-\ncated a score higher than 10 in the truthful condition, or a score lower\nthan 90 in the deceptive condition).\nThe final sample of 292 participants was randomly allocated to\none of the four experimental conditions: truthful statement about\nthe past weekend ( n = 73, 58.90% female, M\nage = 33.92 years,\nSDage = 11.43), deceptive statement about the past weekend (n = 60,\n48.33% female,Mage = 35.55,SDage = 11.54), truthful statement about\nthe forthcoming weekend (n = 80, 60.00% female, Mage = 33.41,\nSDage = 11.67), and deceptive statement about the forthcoming week-\nend (n = 79, 53.16% female,Mage = 33.71,SDage = 10.05). There was no\ndifference between the conditions in gender,X2(3) = 2.42,p = .490,\nCramer's V = 0.05, or age,F(1, 290) = 0.04,p = .837,f = 0.01.\n2.1.2 | The model statement\nWe adhered to the suggested guidelines for formulating a model\nstatement (Centre for Research and Evidence on Security Threats,\n2016), with one exception. Given the online context of the current\ninvestigation, we did not provide an audiotaped version but rather\npresented the statement as text (as did Harvey et al., 2017). We\nfollowed the remaining suggestions and created a statement that (a)\nis unrelated to the research scenario (here: weekend plans), (b)\ndescribes an authentic experience, and (c) is not created on the spot\nduring the interview.\nThe actual model statement was created by interviewing a friend\nof one of the authors via telephone about her first day at university.\nThe interview was transcribed and translated into English from Dutch,\nresulting in a length of 527 words (Supporting Information S1). To\nensure that the participants read the statement, they could only pro-\nceed to the next page after 1 min and were informed that they would\nbe asked four multiple\u2010choice questions about the model statement\n(Supporting Information S2). If a participant failed to answer a question\ncorrectly, she or he was redirected to the model statement followed by\nfour new multiple\u2010choice questions.\n2.1.3 | Experimental manipulation\nParticipants were randomly allocated to one of two conditions of\nveracity (truthful vs. deceptive). Thus, participants gave either a decep-\ntive or truthful statement on their planned or past activities. Liars were\nassigned an activity that they had to pretend to intend for the coming\nweekend (or have done on the past weekend). We allotted an activity\nto liars to avoid that they used one of their previously experienced\nweekend activities. To keep the selection of activities standardized,\nall participants had to choose from a drop\u2010down menu of 31 activities\n(e.g., attending a wedding; Supporting Information S3).\nPast weekend plans\nIn the past weekend conditions, participants were asked to select at\nleast one activity that they had carried out last weekend and at least\nthree activities that they had not carried out last weekend. For those\nactivities that they indicated to have carried out last weekend, they\nwere asked to report how often they had done them before (on a\nslider from never to very often). Subsequently, they were asked the\nsame question for the activities that they said they had not carried\nout last weekend. In the truthful condition, participants were\ninstructed to provide a convincing account about one activity that\nwas randomly chosen from their selected truthful activities. In the\ndeceptive condition, participants were assigned one activity that they,\nin the previous step, indicated to not have carried out before. For\ninstance, if a participant in the deceptive condition had indicated to\nhave \u201cvisited the zoo\u201d but did not\u201cgo to a birthday party,\u201d the partic-\nipant could be assigned to declare to have attended a birthday party.\nTo provide a little more context, we added one extra detail to the\nselected activity in the deceptive condition. For example, if the deter-\nmined activity was \u201cthrowing a party\u201d, the assigned activity was\n\u201cthrowing a party with your friends at your favorite pub\u201d (Supporting\nInformation S4).\nFuture weekend plans\nIn the future weekend conditions, participants were asked to select at\nleast one activity that they were planning to do on the upcoming\nweekend and at least three activities that they were not planning to\ndo. For the planned activities, they were asked to indicate how often\nthey had done them before, how certain they were about carrying\nout that activity, and how well they had planned that activity. For\nthe activities that they indicated not to carry out, participants were\nasked how often they had carried them out before and how certain\nthey were of not carrying them out. Equivalent to the truthful past\nweekend condition, those in the truthful forthcoming weekend condi-\ntion were told one activity that they intended to do next weekend. In\nthe deceptive forthcoming weekend condition, they were assigned the\nactivity that had the lowest score on how often they had done it\nbefore and the highest score on how certain they were not to carry\nout that activity. Equivalent to the past weekend plans, we find a little\nmore detail in the deceptive next weekend condition (e.g.,\u201cGoing to a\nfestival in a big city with a friend\u201d).\n2.1.4 | Procedure\nParticipants accessed the experimental task\u2014advertised as\u201cLie detec-\ntion study about your weekend plans\u201d\u2014via their Prolific account. The\nminimal requirement for doing this task was a Web browser. Upon\nstarting the task, participants were informed about the study and gave\ntheir consent for participating. Next, they read general instructions\nKLEINBERG ET AL. 357\n\nabout the purpose of the task that some participants are instructed to\ntell the truth about their last (or upcoming) weekend, and some are\ninstructed to lie. On the next page, they gave information about their\nactivities during last weekend or for the forthcoming weekend (see\nSection 2.1.3). Participants were then randomly allocated to an experi-\nmental condition and read instructions according to their veracity and\ntime condition. In particular, participants were told that they were about\nto write a statement about one specific activity, which was indicated in\nbold letters alongside these instructions. Participants were then directed\nto the model statement. Once they proceeded through the model state-\nment and the subsequent multiple\u2010choice test, participants received\ntheir statement instructions emphasizing that they should make their\nstory \u201cas detailed, plausible and convincing as possible.\u201d In both veracity\nconditions, participants were reminded to write only about the given\nactivity and that they could take the time to prepare their statement.\nMoreover, they were told that each account would be read by deception\nexperts who would determine whether or not they believed the story. If\nthey were believed, they would be rewarded with an additional\nGBP0.50. We paid the bonus to the participants with 20% highest over-\nall proportion of named entities in their statement.\nOn the next screen, participants had to write their statement in a\ntext box. They could only proceed to the next screen if their state-\nment was at least 80 words long and if their statement was proper\nEnglish. If these criteria were not met, they were reminded about\nthe length and language of the required input via a pop\u2010up. We also\ndisabled the copy\u2010and\u2010pasting functionality to prevent participants\nfrom reusing text.\nAfter completing the statement, participants were asked three\nquestions to be answered with a slider from 0 to 100.\n1. \u201cHow were you instructed to write your statement?\u201d (truthful\u2013\ndeceptive)\n2. \u201cHow much of your statement is based on truthful elements?\u201d\n(nothing\u2013all of it)\n3. \u201cHow motivated were you to write a convincing statement?\u201d (not\nat all\u2013absolutely)\nBefore exiting the experiment, all participants provided demo-\ngraphic information.\n2.1.5 | Computer\u2010automated analysis\nLinguistic Inquiry and Word Count\nWe used the LIWC to extract the proportions of words in each\nstatement that belonged to those psycholinguistic LIWC categories\nthat best represent the RM richness of detail. Specifically, we\nmodeled the richness of detail as the sum of the LIWC categories\npercept (perceptual processes; including the subcategoriessee, hear,\nand feel; e.g., saw, touch, and heard),space (spatial references; e.g.,\ndown and in), andtime (temporal references; e.g., until and end; Bond\n& Lee, 2005).\nNamed entity recognition\nIn contrast to lexicon approaches (e.g., LIWC), NER is rather flexible\ntowards unseen words because it bases the information classification\non probabilistic estimates derived from a supervised machine\u2010learning\ntask (Nothman, Ringland, Radford, Murphy, & Curran, 2013). For\nexample, it determines that \u201cHarry Potter\u201d is a person reference\nbecause it is more likely to be a person than, say, a date, location, or\norganization\u2014without looking\u201cHarry Potter\u201d up in a database. By not\nrelying on a lexicon, the NER approach can classify entities without\nhaving learned that information before. Here, we use the natural lan-\nguage processing library spaCy in the Python programming language\n(Version 1.3.0; Honnibal, 2016). We extract named entities of all the\ncategories identified by spaCy: persons (e.g.,\u201cChris\u201d), nationalities or\nreligious groups (e.g.,\u201cChinese\u201d), facilities (e.g.,\u201cAlum Chine\u201d), organi-\nzations (e.g., \u201cIKEA\u201d), geopolitical entities (e.g., \u201cSouth Korea \u201d),\nlocations (e.g., \u201cHenver Road\u201d), products (e.g., \u201cVW\u201d), events (e.g.,\n\u201cBirthday Party\u201d), works of art (e.g.,\u201cGame of Thrones\u201d), languages\n(e.g., \u201cEnglish\u201d), dates (e.g.,\u201c2 nights\u201d), times (e.g.,\u201c8 am tomorrow\u201d),\npercentages (e.g.,\u201c50%\u201d), money (e.g.,\u201can additional $1.00\u201d), quantities\n(e.g., \u201cabout 40 miles\u201d), ordinals (e.g.,\u201cone\u201d), and cardinals (e.g.,\u201c2nd\u201d).\nOur outcome variable is the proportion of the occurrence of unique\noccurrences of named entities (i.e., each entity is counted only once)\nrelative to the word count in each statement (Kleinberg, Mozes,\net al., 2017).\n2.1.6 | Manual coding of statements\nA random subset of 147 statements (73 on past weekend plans and 74\non future weekend plans) was rated manually by two coders who were\nblind to the experimental condition and hypotheses. The coders were\ninstructed to rate each statement as a whole on its plausibility, its rich-\nness of detail, the occurrence ofhow\u2010utterances and why\u2010utterances.\n1\nEach variable was scored on a Likert scale from 1 (very low/few)t o7\n(very high/many). Although recent findings suggest that counting\ndetails is more reliable than scale judgments (Nahari, 2016), we\ndecided to follow the procedure of previous intentions studies\n(Sooniste, Granhag, Str\u00f6mwall, & Vrij, 2015).\nBoth coders received a training session in which statements were\nrated and discussed with an instructor. Further, 40% of the statements\n(n = 58) were rated by both coders, and the remaining 60% (n = 88)\nwere randomly split between the two coders. The intraclass correla-\ntion coefficients were .11 for plausibility (ns), .90 for richness of\ndetail (p < .001), .60 for how\u2010utterances (p < .001), and .67 for\nwhy\u2010utterances (p < .001). Because of the very low reliability of plau-\nsibility, we decided not to analyze plausibility judgments.\n1Plausibility: \u201cCould this incident have happened as described? Could this be an\nhonest description of someone's weekend activities? \u201d (Leal et al., 2015).\nRichness of detail: \u201cThe inclusion of specific descriptions of place, time,\npersons, objects and events in the statement\u201d (Vrij, 2015). The occurrence of\nhow\u2010utterances: \u201cConcrete descriptions of activities. This can include, but is\nnot limited to, sentences that included phrases such as\u2018we planned to\u2026\u2019, \u2018we\nwere going to\u2026\u2019, \u2018we intended to\u2026\u2019\u201d (Mac Giolla et al., 2013). Why\u2010utterances:\n\u201cThere are two types of answers to \u2018why\u2019. First, wider motivations/reasons\nwhy someone planned an activity. Second, motivations/reasons for doing some-\nthing in a certain way\u201d (Mac Giolla et al., 2013).\n358 KLEINBERG ET AL.\n\n3 | RESULTS\n3.1 | Analytical plan\nWe conducted separate 2 (veracity: truthful vs. deceptive) by 2 (time:\npast vs. future) between \u2010subjects ANOVAs with preregistered\nBonferroni significance level correction on each of the DVs. For seven\nkey DVs in the main, preregistered, analysis, we adhered to an alpha\nsignificance level of .05/7 = .007. The effect size Cohen'sf indicates\nthe magnitude of effects, withf = 0.10,f = 0.25, andf = 0.40 for small,\nmoderate, and large effects, respectively (Cohen, 1988).\nTo compare the diagnostic efficiency of the DVs, we conducted\nreceiver operating characteristics analyses. We compare the areas\nunder the curve (AUCs) using Venkatraman's (2000) AUC comparison\ntest. In the exploratory analyses, we used a supervised machine learn-\ning classification task to predict the veracity of statements. All statisti-\ncal analyses were conducted with R (R Core Team, 2016). For AUC\nanalysis, we used the pROC R package (Robin et al., 2011). The\nmachine learning analyses were conducted with the caret package\n(Kuhn, 2017).\n3.2 | Confirmatory analysis\nTable 1 summarizes the results for the confirmatory analyses,\nexpecting main effects of veracity. There was no significant interaction\neffect between veracity and time for any of the DVs. For the number\nof words and how \u2010utterances, a significant main effect of time\nrevealed that the statements were lengthier and contained more\nhow\u2010utterances when they were about past weekend activities than\nwhen they were about forthcoming weekend plans. Only for one of\nthe four human\u2010coded DVs was the hypothesis supported: truth\u2010\ntellers included more how\u2010utterances in their statement than liars.\n3.3 | Exploratory analyses\n3.3.1 | Machine learning classification: Experiment 1\nTo predict the veracity of a statement, we used supervised machine\nlearning classification, which, contrary to classical statistical testing,\nlearns from the data to predict an outcome (for an overview, see\nYarkoni & Westfall, 2017). More specifically, in a supervised machine\nlearning task, a classifier algorithm is trained on a subset of the data\nto predict an outcome class (here: truthful vs. deceptive). To build a\nclassifier algorithm, one selects features (i.e., predictor variables) based\non which the relationship to the outcome class is learned. To avoid\noverfitting, we split the data into a training set (80% of the data) and\na holdout test set (20%). During the training phase, we applied a five-\nfold cross\u2010validation with 10 repetitions (e.g., Ott et al., 2011). The\ncross\u2010validation procedure ensures that each observation in the train-\ning data has been used for building and validating the final predictive\nmodel. Once the final model was determined, we assessed the perfor-\nmance on the holdout test set, which was not used in the training\nphase. This procedure is used as a safeguard to ensure the validity of\nthe final model.\nWe used the commonly applied linear support vector machine\n(SVM) as a classifier (Mihalcea & Strapparava, 2009; Ott et al., 2011).\nLinear SVMs create ann\u2010dimensional space, wheren equals the num-\nber of features and calculates a linear kernel function that splits the\ndata into two classes (here: truthful and deceptive). The aim is to\nderive a hyperplane that splits the data in a way that the distance\nbetween the hyperplane and the two classes in then\u2010dimensional\nspace is maximized (Murphy, 2012).\nAs feature sets, we used (a) all LIWC variables (92 features) and (b)\na subset intended to model psychological processes (40 features, e.g.,\ncognitive processes, negative thinking, perceptual processes, see\nSupporting Information S6). Table 2 shows the performance metrics\nfor both past and forthcoming weekend plans.\nThe findings suggest the predictive models built on all LIWC\nvariables and the \u201cpsychological processes \u201d subset outperform\nchance classification for prospective weekend plans but not for past\nweekend plans.\n3.3.2 | Other LIWC variables and individual named entities\nWe explored whether truth\u2013lie differences emerged on individual\nLIWC or named entity categories. This also enabled us to understand\nthe verbal differences within the composite score of \u201crichness of\ndetail.\u201d Table 3 displays the means and effect size of the veracity\nmain effect for the three LIWC subcategories that formed the LIWC\nrichness of detail (i.e., percept, space, and time) and other individual\nTABLE 1 Summary table with confirmatory analyses for Experiment 1 (M, SD, Cohen'sd)\nDependent variable\nPast Future\nMain effect\nveracity\nMain effect\ntime\nVeracity *\nTime\nInteraction Hyp.\nExpected\ntruth\u2013lie\ndifference\nsupported?Truthful Deceptive Truthful Deceptive\nNumber of words 261.68 (141.65) 284.12 (172.92) 233.72 (139.92) 210.38 (114.88) 0.00 ( p = .978) 0.18* ( p = .003) 0.08 (p = .172) T > D No\nRichness of detail\n(LIWC)\n19.26 (4.39) 19.20 (2.88) 17.83 (4.49) 18.04 (4.39) 0.01 ( p = .880) 0.16 ( p = .009) 0.02 ( p = .777) T > D No\n% of named entities 3.35 (2.18) 4.16 (1.60) 3.90 (1.89) 3.85 (2.06) 0.10 ( p = .101) 0.03 ( p = .605) 0.11 ( p = .065) T > D No\nRichness of detail\n(human coded)\n4.22 (1.64) 4.97 (1.24) 4.43 (1.34) 4.14 (1.45) 0.08 ( p = .336) 0.11 ( p = .193) 0.18 ( p = .031) T > D No\nHow\u2010utterances\n(human coded)\n5.16 (1.24) 4.63 (0.80) 4.60 (1.04) 4.00 (1.13) 0.26* ( p = .002) 0.28* (p = .001) 0.02 (p = .847) T > D Yes\nWhy\u2010utterances\n(human coded)\n3.23 (1.26) 3.20 (1.40) 3.24 (1.06) 3.25 (1.44) 0.00 ( p = .974) 0.01 ( p = .907) 0.01 ( p = .920) D > T No\n*p < .007.\nKLEINBERG ET AL. 359\n\nLIWC and named entity categories that were significant veracity pre-\ndictors in another study with the same approach (Kleinberg, Mozes,\net al., 2017). The findings suggest that although the categories per-\ncept (f = \u22120.12), space (f = \u22120.17), and time (f = 0.23) were significant\nin differentiating deceptive from truthful statements, they did exhibit\ntheir effect in different directions. Only the temporal information cat-\negory (\u201ctime\u201d) was, as could be expected from RM, higher for truthful\nthan for deceptive statements. The spatial information (\u201cspace\u201d) and\nperceptual processes (\u201cpercept\u201d) were higher in deceptive than in\ntruthful texts. These discrepant findings might explain why the com-\nposite index of the LIWC richness of detail did not indicate a signifi-\ncant difference.\nTable 3 further shows that persons (f = \u22120.32) and geopolitical\nentities (f = \u22120.25) were the best discriminators but were both more\nfrequent in deceptive statements than in truthful statements. Further,\nthe occurrence of date (f = 0.13) and time (f = 0.12) references as well\nas of ordinals (f = 0.17) was significantly higher in deceptive than in\ntruthful statements. Because there were no hypotheses about these\nspecific findings, a replication experiment is needed to identify the\nrobustness of these (unexpected) findings.\n3.4 | Discussion: Experiment 1\nThe confirmatory analysis of the first experiment showed that decep-\ntive statements did not differ from truthful statements in length, the\nrichness of detail, named entities, and why\u2010utterances. We found\nsupport only for the hypothesis that truthful statements would contain\nmore how\u2010utterances than deceptive ones. The exploratory predictive\nanalysis yielded promising results for machine learning classification\ntasks. Deceptive and truthful plans for the forthcoming weekend were\nidentified with an accuracy above chance (80.64% and 74.19% for all\nLIWC variables and psychological processes, respectively). Exploratory\nanalysis also suggested that liars included more references to persons\nand places than truth\u2010tellers. However, this result may be due to a con-\nfound: Liars received slightly more specific instructions for their activi-\nties (e.g.,\u201cGoing on a holiday to Spain with a friend\u201d)t h a nt r u t h\u2010tellers\n(e.g., \u201cGoing on a holiday\u201d). As such, the inclusion of person and place\nreferences may have been a function of the instructions rather than\nthe veracity. To further investigate these seemingly contradictory find-\nings and to assess the replicability of the predictive modeling results,\nwe ran a second experiment with preregistered hypotheses. The second\nexperiment also allowed us to isolate the effect of the model statement\ntechnique. Because we were mainly interested in the emerging area of\ndetecting deceptive intentions, in the second experiment, we collected\ndata on future weekend plans only and manipulated the veracity of\nthe statements as well as the provision of a model statement. We fur-\nther adjusted the instructions so that both liars and truth\u2010tellers were\ngiven identical instructions when writing their statement.\nMoreover, recently, there has been a criticism that a cross \u2010\nvalidation procedure of prediction models of any kind is lacking in\nthe psycholegal verbal deception research literature and has likely\nresulted in overestimates of the reported accuracies (Levine et al.,\nTABLE 2 Accuracies of the supervised machine learning task (linear support vector machine) for two different LIWC feature sets\nFeature set Data Accuracy [95% CI] Sens. Spec. AUC (95% CI)\nComplete LIWC Past weekend plans 69.23 [48.21, 85.67] 71.43 66.67 0.70 [0.48, 0.91]\nForthcoming weekend plans 80.65 [62.53, 92.55] a 62.50 100.00 0.75 [0.56, 0.94]\nPsychological processes Past weekend plans 61.54 [40.57, 79.99] 78.87 41.67 0.77 [0.58, 0.96]\nForthcoming weekend plans 74.19 [55.39, 88.14] a 62.50 86.67 0.78 [0.62, 0.94]\nNote. LIWC = Linguistic Inquiry and Word Count; Sens. = sensitivity; Spec. = specificity.\naSignificantly better than the chance level.\nTABLE 3 Means (SDs, Cohen'sd) for the dependent variables used in the exploratory analyses per time and veracity\nDependent variable\nMain\neffect\nveracity\nPast weekend plans Future weekend plans\nTruthful Deceptive Main effect veracity Truthful Deceptive Main effect veracity\nRichness in detail: percept \u22120.12* 1.93 (1.37) 2.10 (1.26) \u22120.06 1.52 (1.53) 1.99 (1.30) \u22120.17*\nRichness in detail: time 0.23* 8.96 (3.05) 8.09 (2.01) 0.12 8.75 (3.41) 7.16 (2.36) 0.27*\nRichness in detail: space \u22120.17* 8.38 (2.67) 9.02 (2.56) \u22120.17 7.56 (2.96) 8.90 (3.43) \u22120.21*\nFunction words (function) \u22120.16* 53.49 (4.08) 55.35 (3.12) \u22120.25* 55.89 (3.99) 56.46 (3.90) \u22120.07\nPersonal pronouns (ppron) \u22120.09 9.67 (2.70) 10.79 (2.30) \u22120.22* 10.69 (2.56) 10.53 (2.53) 0.03\nFirst person singular (i) 0.24** 6.53 (2.70) 5.15 (2.69) 0.26* 6.80 (3.54) 5.40 (2.55) 0.23*\nNumbers (number) 0.12* 1.86 (1.42) 1.57 (1.03) 0.12 1.70 (1.47) 1.41 (1.04) 0.11\nPersons \u22120.32* 0.29 (0.49) 0.76 (0.70) \u22120.39* 0.34 (0.63) 0.73 (0.77) \u22120.27*\nGeopolitical entities \u22120.25* 0.17 (0.45) 0.48 (0.59) \u22120.30* 0.27 (0.51) 0.51 (0.66) \u22120.21*\nDates 0.13* 1.11 (0.77) 1.06 (0.62) 0.03 1.56 (0.98) 1.17 (0.89) 0.21*\nTime 0.12* 0.54 (0.68) 0.56 (0.52) \u22120.02 0.53 (0.59) 0.29 (0.42) 0.24*\nOrdinal 0.17** 0.24 (0.39) 0.09 (0.20) 0.25* 0.13 (0.28) 0.09 (0.22) 0.08\nNote. Negative effect sizes imply higher values in deceptive than in truthful statements.\n*p < .05. **p < .01.\n360 KLEINBERG ET AL.\n\n2017). We decided to extend the cross\u2010validation from Experiment 1\nby validating the models from Experiment 1 with data from a new sam-\nple in Experiment 2.\n4 | EXPERIMENT 2\nExperiment 2 served four purposes. First, we wanted to replicate the\nfindings obtained in the machine learning analysis on data from an inde-\npendent sample. Second, the potential confound of different instruc-\ntions to liars and truth\u2010tellers was corrected. Third, we wanted to test\nwhether the significant (and unexpected) differences found in the\nexploratory analysis of Experiment 1 for individual LIWC and named\nentity categories could be replicated. Fourth, we manipulated the pro-\nvision of the model statement to examine whether a model statement\nis beneficial to the detection of deceptive and truthful forthcoming\nweekend plans. Because the primary interest of this investigation is\nthe detection of deceptive intentions, all participants were asked to\nwrite about their plans for the coming weekend. Furthermore, because\nthe analytical focus of this investigation is on potentially scalable\nmethods, we used only automated analyses in Experiment 2. On the\nbasis of the findings from Experiment 1 and from studies that show\nthe beneficial effect of the model statement technique (Harvey et al.,\n2017; Leal et al., 2015), we preregistered the following hypotheses:\n\u2022 Deceptive statements will contain more (computer\u2010scored) per-\nson, location, temporal, spatial, date, and time references than\ndeceptive statements.\n\u2022 The machine learning classification accuracy of truthful and decep-\ntive statements is above chance level. The classifier trained on the\ndata of Experiment 1 performs with above chance level accuracy\non the data of Experiment 2.\n\u2022 The differences in linguistic and verbal content variables between\ntruthful and deceptive statements are larger when a model state-\nment is provided than when it is not, resulting in higher classifica-\ntion accuracy.\n4.1 | Method\n4.1.1 | Participants\nThe data collection procedure was identical to that of Experiment 1. We\naimed to replicate the effects found in the first experiment and adhered\nto the same sample size including a buffer for potential data loss,\nresulting in 100 participants required per condition. Due to simultaneous\nstarting times, we collected data of 413 participants and, as per the\npreregistered exclusion criteria, excluded those who could not recall\nwhether they were instructed to write a truthful or deceptive statement\nafter writing the statement (n = 28, final sample = 385).\n2 The remaining\n385 participants were allocated blockwise into four experimental condi-\ntions: a truthful condition with a model statement (n = 90, 66.67%\nfemale, Mage =3 2 . 5 6y e a r s ,SDage = 9.23), a deceptive condition with a\nmodel statement (n = 97, 70.10% female,Mage = 32.39,SDage =1 0 . 4 2 ) ,\na truthful condition without a model statement (n = 101, 73.27% female,\nMage =3 2 . 0 0 ,SDage = 9.36), and a deceptive condition without a model\nstatement (n = 97, 69.07% female,Mage =3 3 . 5 5 ,SDage = 11.06). There\nwas no difference between the conditions in gender,X2(3) = 1.02,\np = .795, Cramer'sV =0 . 0 3 ,o ra g e ,F(1, 383) = 0.24,p =. 6 2 6 ,f =0 . 0 3 .\n4.1.2 | Changes compared with Experiment 1\nThose who read the model statement followed the same procedure as\nthose in Experiment 1. Participants who did not read a model state-\nment were directed to the input field immediately after they received\ntheir veracity instructions (including the prompt to be as detailed,\nplausible, and convincing as possible). This procedure was based on\nrelated previous studies (Bogaard et al., 2014; Leal et al., 2015). The\ninstructions provided to deceptive participants were changed to be\nidentical to those given to truth\u2010tellers; that is, all participants received\nthe nonspecific instructions (e.g.,\u201cthrowing a party\u201d).\n4.2 | Results\n4.2.1 | Confirmatory analyses\nTable 4 shows that the findings of Experiment 1 were supported for\nperson references and location references, which were both more\nprevalent in deceptive than in truthful statements. There were no\nveracity\u2010by\u2010model statement interaction effects. For person refer-\nences (with > without model statement) as well as for temporal\ninformation (without > with a model statement) and date references\n(without > with a model statement), there was a significant main effect\nof the provision of the model statement, albeit only for person refer-\nences in the expected direction.\n3\nMachine learning classification: Experiment 2\nWe predicted that the overall classification accuracy with a machine\nlearning approach would be significantly better than chance level. Spe-\ncifically, we predicted that when with all LIWC categories, the resulting\nclassification accuracy was better than the chance level (here: 50.39%\ndue to a slight condition imbalance). The machine learning classifica-\ntion resulted in an accuracy of 67.11% (95% CI [55.37%, 77.46%]) with\nAUC = 0.69 (95% CI [0.57, 0.82]; sensitivity = 68.42%, specific-\nity = 65.79%). An exact binomial test revealed that accuracy was signif-\nicantly higher than chance (p = .002).\nWe also predicted that the classification accuracy would be higher\nwhen a model statement was provided than when participants did not\nread a model statement. When a model statement was provided, we\nfound an accuracy of 62.16% (44.76\u201377.54%) with AUC = 0.66 (95%\nCI [0.48, 0.84]; sensitivity = 38.89%, specificity = 84.21%), which was\nnot better than chance (p = .125). Without a model statement, the\naccuracy was 56.41% (39.62\u201372.10%) with AUC = 0.63 (95% CI\n[0.45, 0.82]; sensitivity = 65.00%, specificity = 47.37%,ns, p = .316).\n4\n2The IP exclusion was obsolete and not preregistered because Prolific Academic\nhas several control mechanisms built in to prevent multiple participations per\nparticipant.\n3For an exploration of automating how\u2010 and why\u2010utterances, see Supporting\nInformation S7.\n4The results show that the accuracy on the whole dataset is better than on both\nseparate subsets (model statement and no model statement). This is likely due to\nthe sample size used to train the classification models, whereby larger samples\ncontain more information to be used in the predictive model.\nKLEINBERG ET AL. 361\n\nWe expected that the diagnostic efficiency of the classifier for partic-\nipants with the model statement would be significantly better than for\nthe participants who did not read the model statement. There was no\ndifference between the two classifiers, Venkatraman's AUC compari-\nson test (E = 0.04, 2,000 bootstraps,p = .868). Note also that both\nclassifiers' accuracy did not outperform chance level.\nCross\u2010experiment machine learning classification\nTo assess the classification accuracy of machine learning classifiers on\nindependent data, we used the exact SVM classifier with the full LIWC\nfeature set of the intentions data from Experiment 1 and tested its per-\nformance on the data from Experiment 2. That is, rather than evaluat-\ning the performance on holdout data from the same data collection, we\ntest it on truly independent data from a different sample. This analysis\nresulted in an accuracy of 61.30% (56.23\u201366.19%) with an AUC of\n0.64 (95% CI [0.59, 0.70]; sensitivity = 68.59; specificity = 54.12,\np < .001). Moreover, when we tested the classifier on the data of par-\nticipants who read the model statement (i.e., identical to Experiment 1),\nthe accuracy was 63.10% (55.75\u201370.03%; AUC = 0.64, 95% CI [0.57,\n0.72]; sensitivity = 66.67; specificity = 59.79,p = .001).\n4.2.2 | Exploratory analysis\nFor comparison purposes, we also explored the length of statement\n(Table 4) as a function of veracity and the model statement. As in pre-\nvious research, statements were lengthier when participants read the\nmodel statement (M = 188.35, SD = 97.24) than when they did not\n(M = 115.94, SD = 51.67). The findings are in line with previous\nresearch showing that a model statement increased information pro-\nvided by the participants (Bogaard et al., 2014; Leal et al., 2015).\n5 | GENERAL DISCUSSION\nThis study examined whether the statements written about someone's\nweekend plans can reveal his or her veracity. In two experiments, par-\nticipants wrote either a deceptive or truthful statement about their\nplanned activities on the forthcoming weekend. In the first experiment,\nall participants read a detailed model statement and were asked to lie\nor tell the truth about their weekend plans. The theory of verbal\ndeception detection predicts that truthfully intended activities can be\nrecalled in more detail and contain more planning markers and fewer\njustifications for the intended actions than deceptive intentions.\nBecause the primary aim of this study was to test the detectability of\ndeceptive intentions in a potentially large\u2010scale setting, we collected\ndata through an online interface and focused on computer\u2010automated\nanalysis.\n5.1 | Predicting the veracity of statement\nFrom an applied perspective, such as prospective passenger screening,\nthe prediction accuracy of a model might be more important than the\nexplanatory aspects underlying it. With the use of machine learning,\ndeceptive and truthful statements were classified well above chance\nwith relatively high accuracies of 74.19% and 80.65%, respectively.\nTo assess the\u201ctrue\u201d performance of a predictive model, it is important\nto test it on newly collected data. In fact, most machine learning\napproaches to verbal deception detection are not evaluated on data\nfrom a new sample (Fitzpatrick et al., 2015), and most of the reported\naccuracy rates in the psycholegal literature were obtained without any\ncross\u2010validation (see the critique by Levine et al., 2017). We, therefore,\nexamined the robustness of these accuracy rates with cross\u2010validation\nwithin the sample as well as on a new sample in the second experiment.\nThe current investigation is, to the best of our knowledge, the only one\nthat tested a classifier's accuracy on fresh, independent data from a new\nsample. The results are promising in that they withstood the cross\u2010\nexperiment test, but they also highlight the drop of the accuracy when\nclassifiers were applied to out\u2010of\u2010sample data. The accuracy rates will\nper definition be higher if the classifier is trained and tested on the same\ndata, compared with a proper validation on a new sample (Yarkoni &\nWestfall, 2017). Although data from the first experiment suggest accura-\ncies of up to 80%, the independent\u2010sample validation indicated that the\ntrue boundaries might be closer to 63% (similar accuracies using\nTABLE 4 Summary table with the confirmatory analyses for Experiment 2 (M, SD, Cohen'sd)\nDependent variable\nWithout model statement With model statement\nMain effect\nveracity\nMain effect\nmodel\nstatement\nVeracity *\nModel\nStatement Hyp.\nExpected\ntruth\u2013lie\ndifference\nsupported?Truthful Deceptive Truthful Deceptive\nPerson references\n(NER)\n16.58 (51.59) 23.59 (51.22) 23.68 (48.18) 40.23 (54.53) \u22120.11* (p = .026) 0.12* (p = .025) 0.04 ( p = .365) D > T Yes\nLocation references\n(NER)\n18.91 (49.71) 29.67 (57.63) 24.55 (57.81) 42.82 (68.69) \u22120.12* (p = .016) 0.08 (p = .118) 0.03 ( p = .532) D > T Yes\nTemporal information\n(LIWC)\n9.10 (3.79) 9.06 (3.92) 7.91 (2.69) 8.01 (3.07) 0.01 ( p = .941) 0.16* (p = .002) 0.01 ( p = .849) T > D No\nSpatial information\n(LIWC)\n7.56 (3.58) 7.81 (2.93) 7.86 (3.11) 7.88 (2.96) 0.02 ( p = .680) 0.03 (p = .562) 0.02 ( p = .727) D > T No\nDate references\n(NER)\n170.54 (124.20) 175.76 (132.47) 133.89 (93.32) 139.11 (97.44) 0.02 ( p = .652) 0.16* (p = .002) 0.00 ( p = .999) T > D No\nTime references\n(NER)\n52.79 (87.51) 36.91 (61.22) 45.60 (55.57) 48.95 (55.99) 0.05 ( p = .358) 0.02 (p = .722) 0.07 ( p = .159) T > D No\nNumber of words 121.83 (57.37) 118.55 (48.54) 202.88 (107.36) 188.43 (93.47) 0.06 ( p = .276) 0.48*** (p < .001) 0.04 (p = .493) \u2014\u2014\nNote. Negative effect sizes imply higher values in deceptive than in truthful statements. LIWC = Linguistic Inquiry and Word Count; NER, named entity recognition.\n*p < .05. **p < .01. ***p < .001.\n362 KLEINBERG ET AL.\n\nautomated analysis were achieved by P\u00e9rez\u2010Rosas & Mihalcea, 2014).\nWe strongly recommend that future research that makes claims about\nprediction incorporate a cross\u2010validation (e.g., train\u2013test split or leave\u2010\none\u2010out cross\u2010validation) and proper, actual validation on a new sample\nto avoid the reporting of overestimated accuracies. In the current study,\nwithout proper validation on a new sample, the reported accuracies\nwould have been falsely exaggerated by more than 25%.\n5.2 | Do liars over\u2010prepare their statement?\nAs expected, past weekend activities were, in general, lengthier and\ncontained more planning markers than statements about the forthcoming\nweekend. This effect is in line with other studies showing that experi-\nenced events can be recalled in more depth than not yet experienced\nevents (D'Argembeau & Van der Linden, 2004). We found support for\nthe hypothesis that truthful statements contain more indicators of careful\nplanning (i.e., how\u2010utterances) than false ones, which might be attribut-\nable to the motivation of actually executing the plan, whereas fabricated\nintentions do not evoke such a motivation (Mac Giolla et al., 2013). Crit-\nically, however, there were no differences in the length, the richness of\ndetail, or justifications between truthful and deceptive accounts.\nAlthough no differences emerged in the computer \u2010automated\nextraction of the richness of detail (LIWC) and the specificity of infor-\nmation (named entities), exploratory analyses hinted at unexpected\nunderlying dynamics of deceptive and truthful accounts: In line with\nthe theory, truthful statements about intentions contained more tem-\nporal information, more time, and more date references than deceptive\nones. However, contrary to the expectation, deceptive statements\ncontained more person entities, more place entities, and more spatial\ninformation. Theoretical lines would predict that these kinds of aspects\nare rather unlikely for liars because they would offer potentially check-\nable details (e.g., a person to consult or a CCTV camera at a specific\nplace to examine). To assess whether these findings replicate, we\npreregistered a second experiment where we hypothesized the\nobserved, unexpected dynamics. Moreover, the second experiment\nexcluded a potential confound in the instructions (i.e., adding a person\nor location reference to the liars' instructions) and experimentally\nmanipulated the presence of the model statement.\nDid the unexpected findings for location and person entities rep-\nlicate? The effect sizes of the location entities (Experiment 1:\nf = \u22120.21; Experiment 2:f = \u22120.12) and person entities (Experiment\n1: f = \u22120.27; Experiment 2: f = \u22120.11) were smaller in the second\nexperiment. One reason for the decrease in the magnitude of the\ntruth\u2013lie differences could be that Experiment 2 did not contain the\nconfounding, overly specific instructions of Experiment 1. If this were\nthe case, the corroboration of these counterintuitive findings is even\nmore interesting because it suggests that even without any hint at\npersons or locations, liars tend to include significantly more of these\nentities. Interestingly, comparable findings were reported in a study\nabout a forthcoming trip (Warmelink et al., 2012). When asked about\ntheir intention ( \u201cWhat is the main purpose of your trip? \u201d), liars\nreported significantly more detail than truth\u2010tellers, and vice versa\nfor less expected questions (\u201cHow are you going to travel to your\ndestination?\u201d). There are two potential explanations for the current\nfindings. First, liars might have simply chosen to bluff. Possibly, this\nstrategy is specific for the online data collection context applied here,\nwith liars being aware that the information about a future event\nwould be difficult to check. Second, liars might have prepared more\nfor the statement and might have been preoccupied with a detailed,\nconvincing yet false account. Truth\u2010tellers, in contrast, could have\nrelied on the idea that their truthfulness\u201cshines through\u201d (\u201cthe illusion\nof transparency,\u201d Vrij, Granhag, & Porter, 2010, p. 109) without the\nneed to prepare extensively. Tentative support in that direction stems\nfrom post hoc analysis on the time needed to write the statement\n(seconds per word): In the second experiment, liars took longer\n(M = 3.17 sec./word,SD = 2.94) than truth\u2010tellers (M = 2.52,SD = 1.31,\nf = 0.14, p < .001). This trend was not significant for Experiment 1\n(Mt = 2.51, SDt = 1.87, Md = 2.64, SDd = 1.22, f = 0.04, ns).\nLiars might find it difficult to imagine what a truthful statement\nabout an intended action might look like so that they include unrealis-\ntically many specific pieces of information out of precaution to sound\nbelievable. If this were the case, the naivet\u00e9 of liars might possibly\nwork in their disadvantage and give away their deceit. It would be\ninteresting for future research to use questions that asks about things\nthat truth\u2010tellers typically donot have an answer for.\n5.3 | The model statement technique\nWe did not find support for the hypothesis that providing a model\nstatement benefits deception detection. Unexpectedly, participants\nwho read a model statement provided fewer date entities and tempo-\nral information but more person entities than those who did read a\nmodel statement. These latter findings would need corroboration.\nThe absence of a beneficial effect of the model statement was also\nreported elsewhere (Bogaard et al., 2014; Brackmann et al., 2017;\nEwens et al., 2016; Harvey et al., 2017; Leal et al., 2015; Vrij, Leal,\net al., 2017). We see two possible explanations. First, hidden modera-\ntors might determine the role of the model statement. Looking at the\nverbal cues\u2014especially details, at a more granular level (e.g., qualifying\ndetails into verifiable details, script behavior details, and complications)\n\u2014could be an important aspect for further research (the data of the\ntwo experiments are openly available). Second, the null findings might\nbe due to boundary conditions of the model statement technique. We\nprovided participants with a model statement about apast event (i.e.,\nfirst day at uni). Future research could assess whether an alignment\nof the temporal focus of the model statement and the participants'\naction (i.e., past or future action) is necessary. Furthermore, the length\nrequirement that we imposed on all statements (minimum of 80 words)\ncould have played a role. Although intended as a safeguard to elicit suf-\nficient information in the online context, it is possible that this resulted\nin unnatural content and blurred potential truth\u2013lie differences.\n5.3.1 | Manual versus automated text analysis\nConcerning the large\u2010scale focus in this study, two aspects merit\nattention.\n1. Although the computer\u2010automated analysis was applied success-\nfully above chance level in the current study, the value of manual\nhuman scoring cannot (yet) be dismissed. Semantic, linguistic con-\ncepts such as plausibility are not yet easily automatable. Likewise,\nKLEINBERG ET AL. 363\n\npromising approaches such as the verifiability approach (i.e.,\nlooking at verifiable details, Nahari et al., 2014) are currently lim-\nited to manual annotation, which limits their large\u2010scale potential.\nThe technical question of human versus automated coding per-\nformance might best be answered in direct comparisons and\nrigorous empirical testing. Such a comparison should test which\ntechnique yields the best accuracies and, most importantly, pro-\nduces replicable and generalizable results. Because the aim of\nthe current paper was to predict the veracity rather than illumi-\nnate the theoretical underpinnings of it, we focused more on\nthe machine learning part rather than the individual cues under-\npinning it. We do acknowledge that the theory matters and\nshould, in fact, be incorporated into predictive models to make\nuse of the best of both worlds. In the future, hybrid approaches\n(e.g., Kleinberg, Mozes, et al., 2017) might help bridge the gap\nbetween theory and methods and human and automated analy-\nses: Human annotations of the verifiability, for example, could\nbe used as outcome variables for a predictive linguistic model.\nIdeally, this could result in a real\u2010time and valid proxy for other-\nwise manually coded constructs.\n2. The current study relied on a passive collection of data. Alter-\nnatively, future approaches could explore how dynamic conver-\nsational environments (e.g., online chat) facilitate deception\ndetection. Such a line of inquiry might also help to shorten the\nparticipation duration which is essential for applied purposes\nand would allow for the targeted elicitation of needed informa-\ntion (e.g., those pieces that could be verified).\n6 | CONCLUSION\nVerbal deception detection is a promising yet complex path for the\ndetection of deceptive intentions\u2014both from an academic and from\nan applied perspective. In two experiments, we found evidence that\nliars mentioned more person and location references than truth\u2010tellers,\nwhich may be exploited for the detection of their false accounts. Pre-\ndictive modeling with psycholinguistic features yielded promising\nresults above chance level. At the same time, independent validation\nshowed that within\u2010sample cross\u2010validation might still overestimate\nclassification accuracies. The current findings provide novel insights\ninto liars' strategies, highlight the promise of machine learning for\ndeception detection, and emphasize the need for proper validation of\npredictive deception detection analysis.\nACKNOWLEDGEMENT\nB. K. was supported by the Dutch Ministry of Security and Justice.\nORCID\nBennett Kleinberg\nhttp://orcid.org/0000-0003-1658-9086\nAldert Vrij http://orcid.org/0000-0001-8647-7763\nREFERENCES\nBilefsky, D. (2016, March 31). Brussels attacks renew criticism of security\nat Europe's airports. The New York Times . Retrieved from https://\nwww.nytimes.com/2016/04/01/world/europe/brussels\u2010attacks\u2010air-\nport\u2010security.html\nBogaard, G., Meijer, E. H., & Vrij, A. (2014). Using an example statement\nincreases information but does not increase accuracy of CBCA, RM,\nand SCAN: Using an example statement with truth tellers and\nliars. Journal of Investigative Psychology and Offender Profiling, 11(2),\n151\u2013163. https://doi.org/10.1002/jip.1409\nBond, C. F., & DePaulo, B. M. (2006). Accuracy of deception judgments.\nPersonality and Social Psychology Review, 10(3), 214\u2013234. https://doi.\norg/10.1207/s15327957pspr1003_2\nBond, G. D., & Lee, A. Y. (2005). Language of lies in prison: Linguistic\nclassification of prisoners' truthful and deceptive natural language.\nApplied Cognitive Psychology , 19(3), 313 \u2013329. https://doi.org/\n10.1002/acp.1087\nBrackmann, N., Otgaar, H., Roos af Hjelms\u00e4ter, E., & Sauerland, M. (2017).\nTesting a new approach to improve recall in different ages: Providing\nwitnesses with a model statement.Translational Issues in Psychological\nScience, 3(2), 131\u2013142. https://doi.org/10.1037/tps0000116\nCentre for Research and Evidence on Security Threats. (2016). CREST\nguide: The model statement technique. Retrieved from https://\ncrestresearch.ac.uk/resources/model\u2010statement\u2010technique/\nCohen, J. (1988). Statistical power analysis for the behavioral sciences .\nNew York, NY: Academic Press.\nD'Argembeau, A., & Van der Linden, M. (2004). Phenomenal characteristics\nassociated with projecting oneself back into the past and forward into\nthe future: Influence of valence and temporal distance.Consciousness\nand Cognition, 13(4), 844\u2013858.\nEwens, S., Vrij, A., Leal, S., Mann, S., Jo, E., Shaboltas, A.,\u2026 Houston, K.\n(2016). Using the model statement to elicit information and cues to\ndeceit from native speakers, non\u2010native speakers and those talking\nthrough an interpreter: Using the MS to elicit information.Applied Cog-\nnitive Psychology, 30(6), 854\u2013862. https://doi.org/10.1002/acp.3270\nFenn, E., McGuire, M., Langben, S., & Bland\u00f3n\u2010Gitlin, I. (2015). A reverse\norder interview does not aid deception detection regarding intentions.\nFrontiers in Psychology, 6. Retrieved from https://www.ncbi.nlm.nih.\ngov/pmc/articles/PMC4553365/\nFitzpatrick, E., Bachenko, J., & Fornaciari, T. (2015).Automatic detection of\nverbal deception (Vol. 8). Morgan & Claypool. Retrieved from http://\nwww.morganclaypool.com/doi/abs/10.2200/\nS00656ED1V01Y201507HLT029\nGamboz, N., De Vito, S., Brandimonte, M. A., Pappalardo, S., Galeone, F.,\nIavarone, A., & Della Sala, S. (2010). Episodic future thinking in amnesic\nmild cognitive impairment.Neuropsychologia\n, 48(7), 2091\u20132097.\nGranhag, P. A., & Knieps, M. (2011). Episodic future thought: Illuminating\nthe trademarks of forming true and false intentions.Applied Cognitive\nPsychology, 25(2), 274\u2013280. https://doi.org/10.1002/acp.1674\nGranhag, P. A., & Mac Giolla, E. (2014). Preventing future crimes: Identify-\ning markers of true and false intent. European Psychologist, 19(3),\n195\u2013206. https://doi.org/10.1027/1016\u20109040/a000202\nHarvey, A. C., Vrij, A., Leal, S., Lafferty, M., & Nahari, G. (2017). Insurance\nbased lie detection: Enhancing the verifiability approach with a model\nstatement component. Acta Psychologica, 174,1 \u20138. https://doi.org/\n10.1016/j.actpsy.2017.01.001\nHauch, V., Bland\u00f3n\u2010Gitlin, I., Masip, J., & Sporer, S. L. (2015). Are computers\neffective lie detectors? A meta\u2010analysis of linguistic cues to deception.\nPersonality and Social Psychology Review, 19(4), 307\u2013342.\nHonnibal, M. (2016). SpaCy (version 1.3.0). Retrieved from https://\nspacy.io/\nJohnson, M. K., Bush, J. G., & Mitchell, K. J. (1998). Interpersonal reality\nmonitoring: Judging the sources of other people's memories.Social Cog-\nnition, 16(2), 199\u2013224.\nJohnson, M. K., & Raye, C. L. (1981). Reality monitoring. Psychological\nReview, 88(1), 67.\n364 KLEINBERG ET AL.\n\nJupe, L. M., Leal, S., Vrij, A., & Nahari, G. (2017). Applying the verifiability\napproach in an international airport setting.Psychology, Crime & Law,\n(just\u2010accepted), 1\u201329.\nKleinberg, B., Arntz, A., & Verschuere, B. (in press). Detecting deceptive\nintentions: Possibilities for large\u2010scale applications. In T. Docan\u2010Morgan\n(Ed.), The handbook of deceptive communication.\nKleinberg, B., Mozes, M., Arntz, A., & Verschuere, B. (2017). Using named\nentities for computer\u2010automated verbal deception detection. Journal\nof Forensic Sciences.. https://doi.org/10.1111/1556\u20104029.13645\nKleinberg, B., Nahari, G., Arntz, A., & Verschuere, B. (2017). An investiga-\ntion on the detectability of deceptive intent about flying through\nverbal deception detection. Collabra: Psychology. . https://doi.org/\n10.1525/collabra.80\nKleinberg, B., Nahari, G., & Verschuere, B. (2016). Using the verifiability of\ndetails as a test of deception: A conceptual framework for the automa-\ntion of the verifiability approach. In Proceedings of NAACL \u2010HLT\n(pp. 18 \u201325). Retrieved from http://www.anthology.aclweb.org/W/\nW16/W16\u20100803.pdf\nK\u00f6hnken, G. (2004). Statement validity analysis and the\u201cdetection of the\ntruth\u201d. The Detection of Deception in Forensic Contexts,4 1\u201363.\nKuhn, M. (2017). caret: Classification and regression training (version R\npackage version 6.0\u201376). Retrieved from https://cran.r \u2010project.org/\npackage=caret\nLeal, S., Vrij, A., Warmelink, L., Vernham, Z., & Fisher, R. P. (2015). You can-\nnot hide your telephone lies: Providing a model statement as an aid to\ndetect deception in insurance telephone calls.Legal and Criminological\nPsychology, 20(1), 129\u2013146.\nLevine, T. R., Blair, J. P., & Carpenter, C. J. (2017). A critical look at meta\u2010\nanalytic evidence for the cognitive approach to lie detection: A re\u2010\nexamination of Vrij, Fisher, and Blank (2017).Legal and Criminological\nPsychology.. https://doi.org/10.1111/lcrp.12115\nMac Giolla, E., Granhag, P. A., & Liu \u2010J\u00f6nsson, M. (2013). Markers of\ngood planning behavior as a cue for separating true and false intent:\nGood planning behavior and true and false intent.PsyCh Journal, 2(3),\n183\u2013189. https://doi.org/10.1002/pchj.36\nMasip, J., Sporer, S. L., Garrido, E., & Herrero, C. (2005). The detection of\ndeception with the reality monitoring approach: A review of the\nempirical evidence.Psychology, Crime & Law, 11(1), 99\u2013122.\nMeijer, E. H., Verschuere, B., & Merckelbach, H. (2017). Failing to tell friend\nfrom foe: A comment on Wijn et al. (2017).\nLegal and Criminological\nPsychology.. https://doi.org/10.1111/lcrp.12118\nMihalcea, R., & Strapparava, C. (2009). The lie detector: Explorations in the\nautomatic recognition of deceptive language. In Proceedings of the\nACL\u2010IJCNLP 2009 Conference Short Papers (pp. 309\u2013312). Association\nfor Computational Linguistics. Retrieved from http://dl.acm.org/cita-\ntion.cfm?id=1667679\nMurphy, K. P. (2012). Machine learning: A probabilistic perspective. MIT\nPress.\nNahari, G. (2016). When the long road is the shortcut: A comparison\nbetween two coding methods for content\u2010based lie\u2010detection tools.\nPsychology, Crime & Law, 22(10), 1000\u20131014.\nNahari, G., Vrij, A., & Fisher, R. P. (2014). Exploiting liars' verbal strategies by\nexamining the verifiability of details.Legal and Criminological Psychology,\n19(2), 227\u2013239. https://doi.org/10.1111/j.2044\u20108333.2012.02069.x\nNothman, J., Ringland, N., Radford, W., Murphy, T., & Curran, J. R. (2013).\nLearning multilingual named entity recognition from Wikipedia.Artifi-\ncial Intelligence, 194, 151\u2013175.\nOberlader, V. A., Naefgen, C., Koppehele\u2010Goseel, J., Quinten, L., Banse, R.,\n& Schmidt, A. F. (2016). Validity of content\u2010based techniques to distin-\nguish true and fabricated statements: A meta\u2010analysis. Law and Human\nBehavior, 40(4), 440\u2013457.\nOrmerod, T. C., & Dando, C. J. (2015). Finding a needle in a haystack:\nToward a psychologically informed method for aviation security screen-\ning. Journal of Experimental Psychology: General, 144(1), 76\u201384. https://\ndoi.org/10.1037/xge0000030\nOtt, M., Choi, Y., Cardie, C., & Hancock, J. T. (2011). Finding deceptive opin-\nion spam by any stretch of the imagination. In Proceedings of the 49th\nAnnual Meeting of the Association for Computational Linguistics:\nHuman Language Technologies\u2014Volume 1 (pp. 309\u2013319). Association\nfor Computational Linguistics. Retrieved from http://dl.acm.org/cita-\ntion.cfm?id=2002512\nPennebaker, J. W., Boyd, R. L., Jordan, K., & Blackburn, K. (2015). The\ndevelopment and psychometric properties of LIWC2015. Retrieved\nfrom https://repositories.lib.utexas.edu/handle/2152/31333\nP\u00e9rez\u2010Rosas, V., & Mihalcea, R. (2014). Cross\u2010cultural deception detection.\nIn ACL (2) (pp. 440 \u2013445). Retrieved from http://www.anthology.\naclweb.org/P/P14/P14\u20102072.pdf\nR Core Team (2016).R: A language and environment for statistical computing.\nVienna, Austria: R Foundation for Statistical Computing. Retrieved from\nhttps://www.r\u2010\nproject.org/\nRobin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.\u2010C., &\nM\u00fcller, M. (2011). pROC: An open\u2010source package for R and S+ to ana-\nlyze and compare ROC curves.BMC Bioinformatics, 12(1), 77.\nSooniste, T., Granhag, P. A., Knieps, M., & Vrij, A. (2013). True and false\nintentions: Asking about the past to detect lies about the future.\nPsychology, Crime & Law , 19(8), 673\u2013685. https://doi.org/10.1080/\n1068316X.2013.793333\nSooniste, T., Granhag, P. A., Str\u00f6mwall, L. A., & Vrij, A. (2015). Statements\nabout true and false intentions: Using the cognitive interview to\nmagnify the differences. Scandinavian Journal of Psychology , 56(4),\n371\u2013378. https://doi.org/10.1111/sjop.12216\nSzpunar, K. K. (2010). Episodic future thought: An emerging concept.\nPerspectives on Psychological Science, 5(2), 142\u2013162. https://doi.org/\n10.1177/1745691610362350\nVenkatraman, E. S. (2000). A permutation test to compare receiver operat-\ning characteristic curves.Biometrics, 56(4), 1134\u20131138.\nVrij, A. (2015). Verbal lie detection tools: Statement validity analysis,\nreality monitoring and scientific content analysis. InDetecting deception:\nCurrent challenges and cognitive approaches(1st ed.) (pp. 3\u201335). Wiley.\nRetrieved from https://books.google.nl/books?hl=en&lr=&id=4brl\nBQAAQBAJ&oi=fnd&pg=RA1-PA3&dq=Verbal+Lie+Detection+tools:+\nStatement+validity+analysis,+reality+monitoring+and+scientific+content\n+analysis&ots=4sFTBKx24S&sig=5lA5qnbszpbpaGcYokvw8n37ekw\nVrij, A., Blank, H., & Fisher, R. P. (2018). A re\u2010analysis that supports our main\nresults: A reply to Levine et al.Legal and Criminological Psychology, 23(1),\n20\u201323. https://doi.org/10.1111/lcrp.12121\nVrij, A., Fisher, R. P., & Blank, H. (2017). A cognitive approach to lie\ndetection: A meta\u2010analysis. Legal and Criminological Psychology, 22(1),\n1\u201321. https://doi.org/10.1111/lcrp.12088\nVrij, A., & Granhag, P. A. (2012). Eliciting cues to deception and truth:\nWhat matters are the questions asked.Journal of Applied Research in\nMemory and Cognition , 1(2), 110 \u2013117. https://doi.org/10.1016/\nj.jarmac.2012.02.004\nVrij, A., Granhag, P. A., Mann, S., & Leal, S. (2011). Lying about flying: The\nfirst experiment to detect false intent.Psychology, Crime & Law, 17(7),\n611\u2013620. https://doi.org/10.1080/10683160903418213\nVrij, A., Granhag, P. A., & Porter, S. (2010). Pitfalls and opportunities in\nnonverbal and verbal lie detection.Psychological Science in the Public\nInterest\n, 11(3), 89\u2013121. https://doi.org/10.1177/1529100610390861\nVrij, A., Hope, L., & Fisher, R. P. (2014). Eliciting reliable information in\ninvestigative interviews. Policy Insights from the Behavioral and Brain\nSciences, 1(1), 129\u2013136.\nVrij, A., Leal, S., Mann, S., Dalton, G., Jo, E., Shaboltas, A.,\u2026 Houston, K.\n(2017). Using the model statement to elicit information and cues to\ndeceit in interpreter\u2010based interviews. Acta Psychologica, 177,4 4\u201353.\nhttps://doi.org/10.1016/j.actpsy.2017.04.011\nWarmelink, L., Vrij, A., Mann, S., & Granhag, P. A. (2013). Spatial and tempo-\nral details in intentions: A cue to detecting deception.Applied Cognitive\nPsychology, 27(1), 101\u2013106. https://doi.org/10.1002/acp.2878\nKLEINBERG ET AL. 365\n\nWarmelink, L., Vrij, A., Mann, S., Jundi, S., & Granhag, P. A. (2012). The\neffect of question expectedness and experience on lying about inten-\ntions. Acta Psychologica, 141(2), 178\u2013183.\nYarkoni, T., & Westfall, J. (2017). Choosing prediction over explanation\nin psychology: Lessons from machine learning. Perspectives on\nPsychological Science , 12(6), 1100 \u20131122. https://doi.org/10.1177/\n1745691617693393\nHow to cite this article:Kleinberg B, van der Toolen Y, Vrij A,\nArntz A, Verschuere B. Automated verbal credibility assess-\nment of intentions: The model statement technique and predic-\ntive modeling.Appl Cognit Psychol. 2018;32:354\u2013366.\nhttps://\ndoi.org/10.1002/acp.3407\n366 KLEINBERG ET AL.",
  "metadata": {
    "title": "Automated verbal credibility assessment of intentions: The model statement technique and predictive modeling",
    "author": "Bennett Kleinberg, Yaloe Toolen, Aldert Vrij, Arnoud Arntz, Bruno Verschuere",
    "pages": 13,
    "source_file": "Kleinberg_2018_Automated_verbal_credibility_assessment.pdf",
    "pdf_library": "pypdf"
  },
  "page_mapping": {
    "0": [
      1,
      0,
      4480
    ],
    "4480": [
      2,
      4480,
      11516
    ],
    "11516": [
      3,
      11516,
      17960
    ],
    "17960": [
      4,
      17960,
      24498
    ],
    "24498": [
      5,
      24498,
      30884
    ],
    "30884": [
      6,
      30884,
      36393
    ],
    "36393": [
      7,
      36393,
      41862
    ],
    "41862": [
      8,
      41862,
      48114
    ],
    "48114": [
      9,
      48114,
      53922
    ],
    "53922": [
      10,
      53922,
      60945
    ],
    "60945": [
      11,
      60945,
      67635
    ],
    "67635": [
      12,
      67635,
      75731
    ],
    "75731": [
      13,
      75731,
      76430
    ]
  }
}