{
  "text": "Detecting Deception by Analysis of Competing Hypotheses \n________________________________________________________ \n \n \nChristopher Elsaesser   Frank J. Stech \nThe MITRE Corporation \n7515 Colshire Drive  \nMcLean, Virginia 22102-7508 \n \n \nAbstract \nThis paper describes the central component of a \nsystem to assist intelligence analysts detect \ndeception. We describe how deceptions exploit \ncognitive limits and biases and review prior \nwork on processes that can help people \nrecognize organized deceptions. Our process is \nbased on Heuer\u2019 s Analysis of Competing \nHypotheses, which we automate by generating \nstate-based plans and converting them to \nBayesian belief networks. Our decision aid uses \na concept from Bayesian classification to \nidentify distinguishing evidence that a deceiver \nmust hide and a counter-deceiver must uncover. \nWe illustrate the process with one of the most \nimportant deceptions of the 20th Century. \n1    INTRODUCTION \nDeception is ubiquitous, ranging from the common \n(magic, financial fraud and scams) to the famous (e.g., D-\nDay; Indian nuclear tests [CIA 1998]). Complex \nstratagems, even though they have many opportunities to \nfail, often fool even those on guard against deception. \nNevertheless, we think it is possible to construct a system \nto help intelligence analysts detect deceptions. This paper \ndescribes the central component of such a system. \nWe begin by noting how deceptions exploit cognitive \nlimits and biases. Next we describe prior work that guides \nour approach. Our model is based on a process called \nAnalysis of Competing Hypotheses (ACH). Section 4 \ndescribes how we automate ACH by generating state-\nbased plans which represent alternate hypotheses and \nconvert the plans into belief networks. Using a modified \ntype of Bayesian classification, our decision aid identifies \ndistinguishing evidence that a deceiver must hide and a \ncounter-deceiver must uncover. We describe how a \nprototype system identified the key to one of the most \nimportant deceptions of the 20th Century. We conclude by \nlisting areas for further research. \n2    WHY DECEPTION WORKS \nThere is no need to sally forth, for it remains true \nthat those things which make us human are, curiously \nenough, always close at hand. \u2026  we have met the \nenemy, and not only may he be ours, he may be us. \n \u2013  Walt Kelly (1913-1973) \nEffective deceptions exploit reasoning errors, cognitive \nlimitations, and concomitant biases. The most important \nof these are: \n?? Reasoning from evidence to hypotheses \n?? Failure to entertain a deception hypothesis \n?? Biased estimates of probabilities \n?? Failure to consider false positive rates of evidence \nThe first two involve considering too few alternative \nhypotheses due to incomplete generation or premature \npruning (which may involve misestimates of \nprobabilities). The sources and effects of biases arising \nfrom mental estimates of probabilities are well known \n[Gilovich 2002]. We are particularly concerned with bias \ndue to tunnel vision: making conclusions that support \npreconceptions, and \u201cmirror imaging \u201d : assuming an \nadversary is likely to choose a course of action that \nappeals to the observer.  \nTo recognize deception one must consider many \nalternatives and overcome biases that lead to \ninappropriately weighing evidence that seems to support \none of only a few alternatives. The next section reviews \nwork related to these aims. \n3    RELATED WORK \nA major contributor to susceptibility to deception is \nbiased interpretation of observations [Dawes 2001]. \nSeveral techniques to reduce bias in probabilistic \nassessments have been investigated [Elsaesser 1989]. The \nmost promising method is to require a subject to perform \nand document a systematic analysis of evidence \n[Fischhoff 1982]. But when not carefully applied, this \nprocess can sometimes make one more susceptible to \ndeception. \n\nDragoni, et al. [Dragoni 1996] used a Bayesian approach \nin a decision aid for judicial proceedings to help assess \nwitness deception. Abduction is used to eliminate \ninformation of low credibility and find maximally \nconsistent subsets of evidence. Dragoni\u2019 s technique helps \nin cases of common crime, where extensive coordination \nof deceptive testimony is unlikely. The technique is less \nuseful against coordinated deception where plotters \nensure that the evidence of many controlled sources will \nbe confirmed with supposedly objectively (or even \nactually) verifiable information. \nJohnson et al. [Johnson 2001] observed forensic \naccountants while they examined questionable business \nrecords. Protocol analysis indicated accountants who were \nbest able to detect fraudulent information in financial \nstatements used four processes:  \n?? Activation: detect inconsistencies between \nexpectations and observations of the environment.  \n?? Detection: produce hypotheses about possible \ndeceptive manipulations of the environment and \nadjust the assessments of evidence to reflect possible \ndeception tactics.  \n?? Editing: edit the initial hypotheses based on the \ndeceptive manipulations and re-assess observations.  \n?? Reevaluation: decide on appropriate actions to test \nthe deception hypotheses. \nRecent suggestions for training intelligence analysts to \ndetect deception are consistent with Johnson\u2019 s cognitive \nmodel. Whaley & Busby\u2019 s Congruity Theory & \nOmbudsman Method [Whaley 2002] identifies \ninformation that must be collected to reveal \ninconsistencies and other cues to deception. R.V. Jones\u2019 s \nTheory of Spoof Unmasking [Jones 1995] describes how \nto check the validity of evidence, highlight \ninconsistencies, and develop deception hypotheses. \nHeuer\u2019 s Analysis of Competing Hypotheses (ACH) \n[Heuer 1999] specifies how to consider inconsistent and \nanomalous information, develop competing hypotheses \n(including deception), and test hypotheses in a manner \nthat reduces susceptibility to cognitive limits and biases. \nACH is the basis of the decision aid we report in this \npaper. ACH consists of the following steps: \n1. Identify the possible hypotheses to be considered. \n2. List the significant evidence and assumptions for and \nagainst each hypothesis. \n3. Draw tentative conclusions about the relative likelihood \nof each hypothesis. \n4. Analyze sensitivity of the conclusion to critical items of \nevidence. \n5. Identify future observations that would confirm one of \nthe hypotheses or eliminate others. \nSummarizing prior work, we know how and why \ndeception succeeds and procedures to detect deception. \nSince teaching intelligence analysts the procedures does \nnot seem to produce consistently effective deception \ndetectors, a decision support system seems necessary. The \nnext section describes a prototype of a key part of such a \nsystem. \n \n4.    AUTOMATING ACH \nThis section describes how we use state-based planning \nand Bayesian belief networks to automate Heuer\u2019 s \nAnalysis of Competing Hypotheses (ACH) in an attempt \nto overcome cognitive limitations and biases that make \npeople susceptible to deception.  \n4.1   HYPOTHESIS GENERATION VIA \nAUTOMATED PLANNING \nStep one of ACH is to develop alternate hypotheses about \nan adversary\u2019 s course of action. This is intended to help \nthe subject consider alternate explanations of evidence \nand avoid prematurely making conclusions based on a \nfew salient observations or preconceptions. We use a \ndomain independent task decomposition planning system \ncalled Adversarial Planner (AP) [Applegate 1990] to \nautomate as much hypothesis generation as is practical. \nHere we describe the parts of AP that relate directly to \nACH and conversion to Bayesian belief networks. \nTask decomposition planning starts with an abstract goal, \nrefines it with successively more concrete (less abstract) \nsubgoals, and terminates when a sequence of atomic \nactions is found. Subgoals come from the \u201c expansion\u201d \nspecification in action templates that are the raw material \nof planning. Figure 1 shows a typical action template. \nFigure 5 gives an example of a template for an atomic \naction, that is, one with no further decomposition. \n(define (action transport) \n    :parameters (?force_module - force_module \n           ?destination - destination \n           ?conveyance - conveyance) \n    :constraints ((= (get-value ?force_module 'location)  \n              (get-value ?conveyance 'location))) \n    :expansion (series \n            (parallel (contains ?conveyance ?force_module) \n            (adequate_fuel ?conveyance)) \n            (location ?conveyance ?destination) \n            (contains ?conveyance nothing)) \n    :effect (location ?force_module ?destination) \n    :documentation \u201c Load, move, unload\u201d ) \nFigure 1: A task decomposition action template. \n\nDuring plan generation, if an :effect of a template unifies \nwith a subgoal,1the action\u2019 s :expansion tells the planner \nwhat to do to accomplish the :effect, but not how those \nsubgoals are to be accomplished. The planner can \nconsider alternative methods of accomplishing the \nsubgoals. AP attempts to expand each alternative, creating \na contingency plan when more than one action can fulfill \na subgoal. This is intended to help the user consider all \nthe alternatives (Figure 2). \nAP computes a temporal model of the plans it generates. \nThe model consists of numerical time points for each \naction\u2019 s earliest and latest start and end times based on \ndynamic estimates of action duration and the temporal \nrelations among subgoals noted in each abstract action\u2019 s \n:expansion. Any of the temporal relations in [Allen 1984] \nmay be used. The :expansion in the template in Figure 1 \nhas two temporal relations. \u201c Series\u201d means the subgoals \n                                                \n \n1 The :effect can be a conjunction of propositions and those that do \nnot unify with the subgoal become side effects of the action. \nthat follow have to be accomplished in the order listed. \n\u201c Parallel\u201d means that the enclosed subgoals may be \naccomplished in any order. In a deception example, a \ndiversionary action might \u201c cover\u201d the beginning of an \nattack. When AP generates contingency subplans, the \ntemporal information is used to determine if the alternate \nactivities are mutually exclusive.  \nAP allows variables to designate resources. For example, \nthe action in Figure 1 stipulates that some \u201c conveyance\u201d \nis available to transport a \u201c force_module.\u201d Depending on \nthe resources available, it often is not necessary to settle \non a particular conveyance until the plan is prepared for \nexecution. Since our application is concerned with what \nmight happen, rather than planning for a specific \noutcome, the belief network made from a plan represents \nall the possible assignments of a designator. To generalize \nthis for ACH, we extended AP so that there need be no \nidentified resources to fulfill all plan parameters. When \nthis happens, the planner simply notes that resources of \nparticular types are required. In the analysis application, \n\nobserving such a resource increases the probability that \nalternative is viable. \nAP allows user-supplied estimates of the probability an \naction will establish the subgoal it was put in the plan to \nfulfill, given that all the action\u2019 s preconditions hold. We \nimpose a condition that the probabilities cannot be 0.0 or \n1.0 to preclude premature pruning. \nConventional planners cannot generate a plan if any of the \npreconditions of the actions required to establish a \nsubgoal do not hold in the initial situation and cannot be \naccomplished by a planned action. This is problematic \nwhen one is uncertain about the disposition and capacities \nof an adversary. To address ignorance, AP can assume \npreconditions not listed in the input situation (assessment \nof the current state) and capabilities (actions). This allows \nAP to develop competing hypotheses with incomplete \nknowledge and alternatives a user might not consider. \nAssumptions \u2013  if confirmed \u2013  tend to be key indicators to \nthe adversary\u2019 s possible course of action. \nACH suggests that an analyst entertain all possible \nhypotheses. AP can generate many alternate plans, \nincluding alternate subplans within a single plan. \nContingency planning in concert with the ability to make \nassumptions can cause combinatorial explosion. This has \nnot been a problem on proof-of-concept domains with \nwhich we have experimented, but is an issue for further \nresearch.  \nWith the capabilities listed above, AP is able \nautomatically to generate competing hypotheses, fulfilling \nsteps 1, 2, and 3 of ACH. Figure 2 shows a very simple \nexample that we will discuss in the remainder of this \npaper.  \n4.2 CONVERTING PLANS TO BELIEF \nNETWORKS \nA preliminary version of our process for converting plans \nto a belief networks was described by Seligman, et al. \n[Seligman 2000]. Here we recap the process and describe \nextensions made for this application.  \nA plan is a partially ordered sequence of actions. Each \naction has an input situation and causes (if it succeeds) a \nsubsequent output situation. Situations are sets of \npropositions, which are relations on objects. Each action \nis represented as a node with two states: succeed and fail. \nThe predecessors of action nodes are the nodes \nrepresenting the action\u2019 s preconditions. The states of the \nnodes are the possible values of the relation on the \narguments. Most domain representations are purposely \nsparse and only a few of the input situation\u2019 s propositions \nare changed by action execution. The domain \nrepresentation is usually constructed so that propositions \nhave a finite domain and range, although this is not \nnecessary. \nAction node beliefs are computed based on the states of \nthe precondition nodes and the user-specified estimate of \nthe probability of success of the action. Action nodes are \npredecessors of nodes representing their effects. Under \ntypical persistence assumptions [Shoham 1988], a \nproposition\u2019 s value persists until an action changes it. \nThus, failure of an action means that the value of the \nproposition after the action was scheduled to execute \nwould be the same as its value at the latest corresponding \nnode before the action. Hence, a typical propositional \nnode has two parents.2 A representative segment of a \nbelief network made from a plan is depicted in Figure 3.  \n \nFigure 3: Segment of a belief network from a plan \nAP\u2019 s extensions are uncomplicated to represent in a belief \nnetwork. Contingency nodes are treated as disjunctions, \nand will be exclusive disjunctions if temporal and \nresource constraints indicate that children subnodes are \nmutually exclusive. Designators that represent possible \nassignments become parent nodes of the actions where \nthey originate. Designator nodes are treated the same way \nas action precondition nodes in the sense that they must \ntake on legal combinations of values for the action to \nexecute. The domain and range of these nodes is \ndetermined by constraints imposed by the actions that use \nthe resources. For example, if you want to transport \ntroops, then the conveyance should not be a tanker. \nFinally, assumptions are treated like propositions without \npredecessors and have a default probability. Setting the \ndefault to a low probability, say 0.10, ensures that \nanalysis will indicate all but the most trivial assumption \nas crucial to the hypothesized outcome. \n4.3    IDENTIFYING KEY INDICATORS \nSteps 4 and 5 of ACH require identifying indicators of an \nadversary\u2019 s intention. This is where ACH is susceptible to \nbias when people, as they often do, fail to weigh the \nimpact of evidence by its false positive rate [Dawes 2001] \nand misestimate prior probabilities. To avoid these errors, \nwe treat each state in the network as a potential two-\ncategory dicotomizer [Duda 2001]. The minimum error \nrate discriminant for a two-category dicotomizer is: \n                                                \n \n2 We allow the possibility of decay into a state of ignorance for \npropositions with a range of unknown cardinality. \nActionm \nReli(args) \nRelk(args) \n\u2026  \u2026  \nReli(args) \nRelj(args) \u2026  \n\n)(\n)(ln)| (\n)| (ln) (\n?\n?\n?\n?\nP\nP\neP\nePeg\ni\ni\ni ??  \nPattern classification doctrine would have us compute \ng( i\ne ) and apply the decision rule that when g( ie ) > 0 and \nie  is observed, then ?  is more likely, its complement \notherwise. We do not compute the second term of the \ndiscriminant, the prior log likelihood of the outcome ? . It \nis not necessary for focusing attention on the most \nimportant evidence of the alternatives, and by ignoring it \nwe avoid relying on potentially biased priors.  \nSummarizing, the ACH as we have implemented it, \nconsists of the following steps: \n0. Generate a contingency plan representing one or \nmore hypotheses about the possible course of action \nof an adversary (Figure 2) \n1. Create a belief network from the plan (Figure 3) \n2. Enter a finding ? ? -- typically the success state of the \nplan or one of the branches of a contingency plan. \n3. Store the conditional probability of all states for only \nthose nodes that precede the hypothesized outcome \nstate ? , P( i\ne |?? ). This is done since we are trying to \nidentify evidence that will indicate the adversary\u2019 s \nintent before it can be accomplished. \n4. Remove the finding ? ?and enter a finding of its \ncomplement? . \n5. For each state ie compute the log likelihood ratio, \nln[P( ie |?? )/P( ie |? )],For human factors purposes, \nwe scale the likelihood ratios to (-1,1). \n6. Apply a threshold to eliminate states that provide \nlittle evidence to distinguish between ? ?and \n? . On a \n(-1,1) scale, a threshold of +/- 0.05 is typical. \n7. Display the results, as in Figure 4. \nThe closer a state\u2019 s (scaled) log likelihood is to 1.0 or -\n1.0, the more diagnostic that state is of ? ??or? . These are \nstates the deceiver must hide, as revealing them should \nlead the deceived to recognize the true course of action. \nConversely, the counter-deceiver must look for these \nstates. \nThis process outlined in this section addresses the main \nsources of bias interferes with people\u2019 s ability to \nrecognize deception. The next section gives an example to \nillustrate how it might be used for counter-deception.  \n \n \n \nFigure 4: Analysis of Normandy as objective of D-Day \ninvasion is most sensitive to having port facilities. \n \n5    EXAMPLE: D-DAY \nThe D-Day invasion of France via Normandy was the \nturning point in World War II. A key factor in its success \nwas the Allied campaign of deception that played on \nGermany\u2019 s predisposing that the invasion would come via \nthe Straits of Dover in the vicinity of Pas de Calais. Pas \nde Calais was considered a favorable landing site for the \ninvasion, as it was on the most direct route to Germany, \nminimized flying time for air cover, and would help the \nAllies avoid the threat of V-1 flying bombs. An important \nconsideration influencing the German assessment was that \nan invasion requires port facilities to offload troops and \nsupplies. The Pas de Calais region had four major ports; \nNormandy had none.  \nAllied planners knew they could not break through \nGermany\u2019 s defensive forces if the German Army \nconcentrated where the Allies choose to land. Therefore, \nthe Allies had to convince the Germans to defend some \npoint other than the true Allied objective. Pas de Calais \nwas the obvious invasion objective, so they had either to \nconvince the Germans of another, or find an alternative \nlanding site and deceive the Germans so that they would \nnot consider the true destination a serious objective. As \nwe know from history, the latter is what happened. \nAllied planners embarked on a deception campaign called \nBODYGUARD whose purpose was to reinforce the \nGerman preconception of an Allied landing at Pas de \nCalais. They carefully hid the key fact that the Allies \nwould not need to capture a major port because they had \nbuilt transportable port facilities, called MULBERRY, to \nuse on the Normandy beaches and had devised the first \nundersea oil cables, PLUTO (pipelines under the ocean). \n\nMULBERRY and PLUTO headed the Allied list of \n\u201c items which it is undesirable\u201d for the enemy to see. We \nimplemented a simple version of BODYGUARD to \nillustrate our ACH process. \nWe started with an existing planning domain description \nfor the transportation of supplies. The key action \nrepresents unloading a ship at a destination, depicted in \nFigure 5. \n \n(define (action unload) \n    :domain transportation \n    :parameters (?conveyance - ship \n                         ?destination - destination \n          ?force_module \u2013  force_module) \n    :precondition (and (location ?conveyance ?destination) \n     (contains ?conveyance ?force_module) \n                    (has_port ?destination)) \n    :effect (contains ?conveyance nothing) \n    :probability-of-success 0.95 \n    :duration 2.0) \n \nFigure 5: Unload action from transportation domain \nWe created an initial situation with the relevant facts of \nBODYGUARD. From these parts we planned the \ntransportation of invasion supplies from the south of \nEngland to alternate destinations in France.  \nThe first plans we generated were from the Allied point of \nview and indicated nothing remarkable; with port \nfacilities you can supply an invasion at either Normandy \nor Pas de Calais, sensitive only to the usual mundane \nitems such as availability of sufficient transport. But the \nGermans did not know about MULBERRY or PLUTO, so \nwe deleted the proposition has_port(Normandy) from \nthe initial situation,3 to represent the German \npreconception. The result was a single feasible plan with \nPas de Calais as the destination \u2013  the upper branch of the \ncontingency plan in Figure 2 \u2013  just as the Germans \nconcluded. \nThe German High Command did not have our tool. If \nthey had it, the next step would be to allow AP to make \nassumptions for preconditions that can\u2019 t be accomplished \nwith actions. The relevant part of  the analysis on the plan \nin Figure 2 is shown in Figure 4. The top two lines \nindicate that the decisive factor that would make an \ninvasion at Normandy feasible is port facilities. This is \nwhat the Allies knew to hide. The Germans should \nentertained this deception hypothesis and tried to \ndetermine if the Allies could establish its key \nprecondition.  \n                                                \n \n3 alternately, we could have set its probability to 0.0 \n6 CONCLUSION AND FURTHER \nRESEARCH \nWe have described a system that automates Heuer\u2019 s ACH \nas the basis of a counter-deception decision support \nsystem. Our effort now is on extending the core ACH \nprocess. On the front we will build an interface to help \nusers create domain descriptions. The planning system \nwill fill in these plans and create contingencies as \nalternatives. \nOn the back end of our ACH we are creating a system to \nsuggest deception tactics to keep an adversary from \nrecognizing the true plan (dissimulation) and ways to give \nthe adversary a false apprehension of reality (simulation). \nThe temporal model generated with the alternate courses \nof action will be an important input to this process.  \nAfter we complete a deception planning system we will \nextend it to counter-deception planning using AP\u2019 s \ncounter-planning process. Along the way, we will conduct \nexperiments to see if our system can (a) reliably plan \ndeceptions, and (b) reliably detect deceptions. \nAcknowledgements \nThe research reported in this paper is sponsored by The \nMITRE Corporation. \nReferences \nAllen, James. F. (1984) \u201cA General Model of Action and \nTime\u201d Artificial Intelligence 23, 2, July 1984. \nApplegate, Carol, C. Elsaesser, and J. Sanborn (1990) \n\u201cAn A rchitecture for Adversarial Planning,\u201d IEEE \nTransactions on Systems, Man, and Cybernetics, Volume \n20, Number 2, January, 1990.  \nCIA (1998), Press Release: Indian Nuclear Testing. \nwww.cia.gov/cia/public_affairs/press_release/archives/19\n98/pr051298.html \nDawes, Robyn M. (2001) Everyday irrationality: how \npseudo scientists, lunatics, and the rest of us \nsystematically fail to think rationally. Westview Press. \nDragoni, A. F. (1996) \u201cMaximal Consistency, Theory of \nEvidence and Bayesian Conditioning in the Investigative \nDomain;\u201d  Proceedings of the Fifth Iberoamerican \nConfernece on Computer Science and Law, Havana. \nDuda, Richard O., P.E. Hart, and D. G. Stork (2001) \nPattern Classification. Second edition, John Wiley & \nSons, Inc. \nElsaesser, Christopher (1989) \u201c Explanation of \nProbabilistic Inference,\u201d Uncertainty in Artificial \nIntelligence 3, L.N. Kanal, T.S. Levitt, and J.F. Lemmer \n\n(Editors), Elsevier Science Publishers B.V. (North-\nHolland), pp. 387-400. \nFischhoff, Baruch (1982) Debiasing. Judgement \nunderuncertainty: Heuristics and biases, Daniel \nKahneman, Paul Slovic, and Amos Tversky (Editors), \nCambridge University Press, Cambridge, United \nKingdom, pp. 422-444. \nGilovich, Thomas, D. Griffin, and D. Kahneman (2002) \nHeuristics and Biases Cambridge University Press, \nCambridge, United Kingdom. \nHeuer, Richards J. (1999) Psychology of Intelligence \nAnalysis. Washington: Central Intelligence Agency \nCenter for the Study of Intelligence.  \nJohnson, Paul E., S. Grazioli, K. Jamal, and R. G. \nBerryman (2001) \u201cDetecting deception: Adversarial \nproblem solving in a low base-rate world,\u201d Cognitive \nScience 25(3), May-June.  \nJones, R. V. (1995) \u201c Enduring principles: Some Lessons \nin Intelligence,\u201d CIA Studies in Intelligence, Volume 38, \nNumber 5. \nhttp://www.cia.gov/csi/studies/95unclass/Jones.html \nSeligman, Leonard J., P. Lehner, K. Smith, C. Elsaesser \nand D. Mattox (2000) \u201cDecision -Centric Information \nMonitoring,\u201d Journal of Intelligent Information Systems, \nKluwer Scientific Publishers, 14(1). \nShoham, Yoav (1988) Reasoning about change, The MIT \nPress, Cambridge, MA, USA. \nWhaley, Barton and J. Busby (2002), \u201cDetecting \nDeception: Practice, Practitioners, and Theory\u201d in Roy \nGodson and James J. Wirtz (Editors.), Strategic Denial \nand Deception: The Twenty-First Century Challenge. \nNew Brunswick: Transaction Publishers. ",
  "metadata": {
    "title": "",
    "author": "",
    "pages": 7,
    "source_file": "Analysis_2022_Detecting_Deception_Analysis_Competing_Hypotheses.pdf",
    "pdf_library": "pypdf"
  },
  "page_mapping": {
    "0": [
      1,
      0,
      3892
    ],
    "3892": [
      2,
      3892,
      8733
    ],
    "8733": [
      3,
      8733,
      10990
    ],
    "10990": [
      4,
      10990,
      16109
    ],
    "16109": [
      5,
      16109,
      20069
    ],
    "20069": [
      6,
      20069,
      24587
    ],
    "24587": [
      7,
      24587,
      26061
    ]
  }
}