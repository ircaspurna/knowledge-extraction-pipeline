{
  "text": "Extending Sparse Text with\nInduced Domain-Speci\ufb01c Lexicons and Embeddings:\nA Case Study on Predicting Donations\nMeiXing Donga, Rada Mihalceaa, Dragomir Radevb\naDepartment of Computer Science and Engineering, University of Michigan, 2260\nHayward St., Ann Arbor, MI 48105, United States\nbDepartment of Computer Science, Yale University, 17 Hillhouse Ave., New Haven, CT\n06511, United States\nAbstract\nThis paper addresses the problem of expanding sparse textual content to\nincrease the accuracy of data-driven prediction tasks. We evaluate the use\nof word embeddings and lexicons within the context of a donation prediction\ntask, where we classify potential donors as either likely or unlikely to donate.\nWe perform several comparative experiments and analyses, and show that our\nmethods to automatically enhance sparse textual data signi\ufb01cantly improve\nthe predictive performance on this task.\nKeywords: Natural language processing, Text expansion, Sparse text\n1. Introduction\nOver the past three decades, data-driven learning has made great strides\nand brought signi\ufb01cant progress across many disciplines, ranging from com-\nputer science and information sciences, to psychology, astronomy, economics,\nand many other science or humanities \ufb01elds. While many of the most recent\nlearning strategies assume the availability of a large amount of data, there\nare still many applications that only bene\ufb01t from limited amounts of data.\nAmong these, we often deal with datasets that include only small amounts\nof textual information that, because of their size and limited vocabulary, end\nup not contributing as much as they could to the overall learning process.\nIn this paper, we explore the question of whether we can enrich sparse\ntextual content inside categorical datasets, to bring into the learning frame-\nwork additional information that is implied by the text but not explicitly\nPreprint submitted to Computer Speech & Language October 25, 2021\n\nstated. As an example, consider a dataset that includes a text \ufb01eld whose\nvalue for one of the instances is the word \u201ccomputer.\u201d Typically, such cate-\ngorical features are used \u201cas is\u201d and are weighted and used alongside other\nfeatures, depending on the learning framework. However, aside from being\na string of characters, the word \u201ccomputer\u201d implies \u201can electronic device\nfor storing and processing data,\u201d has associations with other words such as\n\u201cdata,\u201d \u201chardware,\u201d \u201csoftware,\u201d and so forth. In this paper, we present sev-\neral methods for automatically enriching categorical \ufb01elds in a dataset where\nthe categorical elements can also be treated as text. Our goal is to improve\ndata-driven predictions, so we perform comparative evaluations that allow\nus to learn what text expansion techniques work best.\nSpeci\ufb01cally, we primarily ask our questions in the context of a donation\nprediction problem, where we use a dataset consisting of the pro\ufb01les of uni-\nversity alumni who have previously donated, as well as alumni who did not\nmake any donations, and attempt to predict for a new instance whether they\nare likely to donate or not. We also consider the task of gender prediction\non a dataset of blog pro\ufb01les to determine to what extent our methods can\nbe applied to other datasets.\nThe amount of textual data available in both datasets is limited in terms\nof both quantity and variety; each piece of text is a few words at most, and\nthe category de\ufb01nitions restrict the vocabulary. Yet, it can still be quite\nuseful. For instance, a \u201cCEO\u201d is more likely to donate than a \u201cclerk\u201d, or a\n\u201csenior\u201d employee is more likely to donate than a \u201crecent graduate.\u201d\nWe explore four di\ufb00erent strategies for extending sparse text, including\ntwo lexicon generation methods, and two embedding methods that are in\ufb02u-\nenced by domain knowledge. Using features obtained from these methods,\nwe build models that predict whether someone is likely to donate, and com-\npare their performance with baseline models that do not make use of such\nadditional features.\nThe paper makes two main research contributions. First, we address the\nquestion of whether we can e\ufb00ectively augment text \ufb01elds in a dataset by\nleveraging information speci\ufb01c to the target domain, and show that with such\ntextual expansion strategies we can signi\ufb01cantly improve over a baseline that\ndoes not make use of this additional information. Second, we compare sev-\neral di\ufb00erent models for extending sparse text in datasets, including methods\nthat rely on information drawn from (a) the database itself; or (b) external\nresources, and gain new insights into what methods lead to the highest perfor-\nmance improvements. We seek to answer these questions using the donation\n2\n\nprediction task, where we rely on a dataset that has information on previous\ndonors including limited free-form text, and show the role played by di\ufb00erent\ntext expansion strategies to improve the e\ufb00ectiveness of our predictive model.\nWe also show that these methods can apply to other cases by evaluating on\na second task and dataset.\n2. Related Work\nOur task is related to the classi\ufb01cation of short texts, which is challeng-\ning because the text is typically sparse and do not provide much word co-\noccurrence information. In contrast to standard free-form short-text datasets,\nsuch as tweets from Twitter, our categorical text is not only short but also\nrestricted in content. For instance, the set of academic majors available at a\nparticular university only contains text from the the names of the majors.\nUnfortunately, the bulk of recent machine learning methods assume the\navailability of large amounts of varied data, but there exist many ways of\ntackling machine learning without this. Hand-built lexical resources have\nbeen used extensively in natural language processing tasks like word-sense\ndisambiguation (Banerjee and Pedersen (2002)), sentiment analysis (Moham-\nmad et al. (2013)), and short text classi\ufb01cation (Jiang et al. (2011)). Text\nembedding methods allow models trained on one domain to be adapted to\nnew domains that have little data.\nWe focus on lexical resources and embedding methods as they are two of\nthe most straightforward and commonly used methods for text classi\ufb01cation\ntasks. In this section, we overview the work that has been previously done\non these related directions.\n2.1. Lexical Resources\nLexicons have been used extensively in sentiment analysis tasks (Taboada\net al. (2011)). There are many manually created sentiment lexicons such as\nthe NRC Emotion Lexicon (Mohammad and Turney (2010)), MPQA Lexicon\n(Wilson et al. (2005)), and Bing Liu Lexicon (Hu and Liu (2004)). General\nlexical resources have been adapted to the domain of sentiment analysis as\nwell. For instance, SentiWordNet (Esuli and Sebastiani (2007b)) extends\nWordNet (Miller (1995)) such that each group of synonyms in WordNet,\na manually-created lexical database, is tagged with three sentiment scores:\npositivity, negativity, objectivity. These lexical resources are very useful but\nmanual e\ufb00orts to create them are costly and time-consuming (Mohammad\n3\n\nand Turney (2010)), requiring experts or crowdsourced annotators. This has\ninspired great interest in automatically inducing sentiment lexicons.\nMuch work has been focused on Twitter, a microblogging website with\nhundreds of millions of users from around the world. User-generated text\nis always short, as tweets are limited to 280 characters. Mohammed et al.\n(Mohammad et al. (2013)) construct a sentiment lexicon for Twitter based\non calculating how closely a word is associated with positive or negative sen-\ntiment. A word\u2019s association score is calculated using the pointwise mutual\ninformation (PMI) between the word and a seed set of hashtags, such as\n#good and #bad.\nMany other lexicon induction methods use label propagation to build\nsentiment lexicons from a seed set of words (Rao and Ravichandran (2009);\nEsuli and Sebastiani (2007a)). Typically, a lexical graph is built, where each\nword or phrase is a node and edges represent the similarity between two\nnodes. Then, propagation methods are used to determine the sentiment of\neach node, given the sentiment of an initial set of nodes.\nMost of these lexicons are built for large, general domains like Twitter.\nHowever, the sentiment of a word depends on the speci\ufb01c domain in which\nit is used. Recent work builds domain-speci\ufb01c sentiment lexicons using label\npropagation methods and domain-speci\ufb01c corpora (Hamilton et al. (2016)).\nLexicons are also used for many tasks outside of sentiment analysis. For\ninstance, LIWC, a general lexicon, is used to quantitatively analyze content in\ntasks ranging from personality prediction (Schwartz et al. (2013); Pennebaker\nand Graybeal (2001)) to deception detection (Ott et al. (2011)).\n2.2. Text Representations\nThere are numerous ways of representing text for computational process-\ning, most of which transform text into a numerical vector. These vectors\nideally embed important characteristics of the text, such as the semantics.\nClassical representations of text include bag-of-words (BOW), where a\nbody of text is represented as the set of words that compose it, and latent\nsemantic analysis (LSA) (Deerwester et al. (1990)), where the representation\nis derived from the factorization of a term-document occurrence matrix.\nRecent text embedding methods such as Word2Vec (Le and Mikolov\n(2014)) and GloVe (Pennington et al. (2014)) are able to capture semantic\nrelationships such as \u201cman is to woman as brother is to sister.\u201d A particular\ntype of the Word2Vec model, skip-gram with negative sampling, has been\n4\n\nshown to be implicitly factorizing a word-context matrix (Levy and Gold-\nberg (2014); Levy et al. (2015)). There have been many extensions of these\nmethods that embed larger bodies of text such as sentences, paragraphs and\nentire documents (Le and Mikolov (2014); Kiros et al. (2015)). A downside of\nneural embedding models like Word2Vec is the prerequisite of large amounts\nof training data. For instance, the pre-trained Word2Vec vectors released by\nGoogle were trained on part of the Google News dataset, containing about\n100 billion words.\nRepresentations for sets of words such as phrases and sentences can be\nconstructed by linearly averaging the embeddings of the constituent words.\nThis has remained a strong feature or baseline across many tasks (Faruqui\net al. (2015); Kenter and De Rijke (2015); Yu et al. (2014); Kenter et al.\n(2016)).\n3. Predicting Alumni Donations\nWe conduct our exploration in the context of a donation prediction task,\nin which we attempt to determine the likelihood of an alumnus/alumna to\ndonate, based on the limited background data available for that person. This\nis not a straightforward task. Previous studies on alumni donations (Hoyt\n(2004); Meer and Rosen (2012); McDearmon (2013)) found that there are\nmany di\ufb00erent contributing factors to alumni giving, including having the\ncapacity to give, extracurricular involvement during the time at the univer-\nsity, and the prestige of the university.\nWe use the dataset described in this section. The ground truth is ex-\ntracted from the alumni donation history, where those who have donated\n$10,000 or more to a single fund are designated as having donated, and those\nwho have not donated anything to any fund are designated as not having do-\nnated. The resulting set of alumni has a much greater number of non-donors\nthan donors. There are 31,780 non-donors, as compared to 655 donors, which\nallow models to achieve 98% donor classi\ufb01cation accuracy by simply classi-\nfying all samples as the majority class. Sampling methods to balance classes\nare commonly used when working with imbalanced data. We therefore cre-\nate a balanced dataset by including all of the 655 alumni who donated more\nthan $10,000 and randomly sampling an equal number of those who donated\nnothing.\nIn all of our experiments, we use 10-fold cross validation, resulting in\ntraining and test set sizes of 1179 and 131 respectively for each split. We use\n5\n\nName Educational Professional\nAmanda\nAlamns\nMSE in Electrical Engi-\nneering - 2000\nElectrical Engineer, Senior Project\nEngineer, Principal Systems Engi-\nneer\nBob Beustton BS in Economics - 2000 Financial Analyst Trainee\nClaire\nCarshter\nBS/Teaching Certi\ufb01cate in\nElementary Education -\n2000\nElementary School Teacher, CEO\nof EduStartup\nTable 1: Fictitious Alumni Examples\na logistic regression model with L2 penalties and a regularization parameter\nC of 1.0 in all cases. 1\n3.1. What Makes a Donor?\nWe want to be able to predict whether a person will donate from her\npersonal and professional attributes. Let us consider the \ufb01ctitious alumni\nin Table 1 (real examples could not be used due to privacy agreements).\nAmanda Alamns graduated with a graduate degree in engineering and has\nsteadily climbed the ranks in her professional career. From her position in her\ncareer, we can infer that she has the means to donate. Bob Beustton, on the\nother hand, has somehow remained a trainee for over a decade. It is unlikely\nthat he will make any donations for the time being. Lastly, we have Claire\nCarshter. If we look solely at her educational history and \ufb01rst job, it appears\nunlikely that she would donate; the teaching profession is not known for its\nlucrative opportunities. However, we see that she then proceeded to start her\nown company. She appears to be a successful individual and is probably more\nlikely to donate because she has the means to do so. Additionally, perhaps her\nexperience at the university helped inspire her to pursue entrepreneurship.\n3.2. Data Description\nThe work in this paper is based on a database of alumni information\nmaintained by a large, public Midwest university. We call this dataset Donor\n1We also obtained results using an SVM classi\ufb01er, but obtained results and trends\nsimilar to those obtained from a logistic regression model. We therefore show results only\nfor the regression model.\n6\n\nSource Features\nDI age, gender, graduation year, degree level T , degree\ntypeT , degree majorT\nLinkedIn city, state, country, most recent three job titlesT , most\nrecent three companiesT , NAICS number\nTable 2: Dataset features (text \ufb01elds are marked with T )\nInformation (DI). In addition, we also have a dataset of public LinkedIn\npro\ufb01les for a subset of the alumni who are in DI. The DI dataset contains\neach alumna\u2019s donation history along with her educational history while at\nthis particular university.\nAn alumna\u2019s educational history contains her major, graduation year,\ndegree level (e.g. Bachelor\u2019s level, Master\u2019s level, Doctoral Level), and degree\ntype (e.g. BS, MD, PhD). Every record in the LinkedIn dataset contains all\njob titles and companies listed on the corresponding LinkedIn pro\ufb01le. In our\nexperiments, we only consider the most recent three job titles and companies.\nWe consider the degree level, degree type, degree major, and the most recent\nthree job titles and companies as text \ufb01elds that are used both as categorical\nfeatures and as input for the textual feature methods.\nThere are 56,259 people who appear in both the DI and LinkedIn datasets;\nwe focus on this subset of alumni. Of this set, approximately half have\ndonated some amount. However, many donations are on the order of a few\ndollars. Therefore, we further hone in on those alumni who have donated\nmore than $10,000 to a single fund.\nTo represent a person, we extract categorical features such as major,\nrecent job titles, gender, and age, among others. To focus our results on\nthe e\ufb00ects of textual enhancement, we use only the categorical features that\ncan also serve as textual features. Each instance in our dataset is then\nrepresented as a feature vector that encodes all of the categorical features\nby concatenating one-hot embeddings of each feature. Table 2 lists all the\nfeatures that are available in the dataset.\n3.3. Qualitative Analysis\nTo gain further insight into the data, we conduct several qualitative anal-\nyses of the backgrounds of donors. We \ufb01rst look at the percentage of people\nwho donate at di\ufb00erent degree levels, shown in Figure 1. Of the di\ufb00erent de-\n7\n\nFigure 1: Percentage of population who donated for obtained degree levels at several\ndonation amount thresholds.\ngree levels, a much higher percentage of those with professional level degrees\nare donors. This is consistent across di\ufb00erent donation amount thresholds.\nThe donor statistics of the other degree levels are consistent with the overall\nstatistics, across the entire population.\nWe further look at di\ufb00erent types of professional level degrees, which are\ncomprised of various medical and law degrees. The \ufb01ve professional degree\ntypes with the highest percentages of donors are shown in Figure 2. We see\nthat Juris Doctor degrees (J.D.) and Doctor of Medicine degrees (M.D.) are\namong the top \ufb01ve, which is consistent with the correlation lexicons that we\nautomatically generate, as described in the next section.\nMedical residencies (Med. Res.) and medical fellowships (Med. Fellow-\nship) occur much less than J.D.s and M.D.s in our dataset, which could have\ncontributed to their lack of representation in the lexicons.\nFinally, we look at the number of popular majors across di\ufb00erent depart-\nments. We see that those who studied law consistently donated more than\nthe others across the di\ufb00erent donation thresholds. We also see that educa-\ntion majors have a higher percentage of donors than other popular majors\nshown in Figure 3. This could be because those who choose to pursue edu-\ncation are more philanthropic by nature, wanting to teach and help others\nwithout the promise of a high salary.\n8\n\nFigure 2: Percentage of population who donated for obtained professional degree types at\nseveral donation amount thresholds.\nFigure 3: Percentage of population who donated for obtained majors of study at several\ndonation amount thresholds.\n9\n\n4. Text Expansion using Domain-Speci\ufb01c Knowledge\nA core hypothesis of our work is that the sparse text that is available in\nmany sources of data, such as our alumni dataset, can still hold much useful\ninformation. To make the sparse text useful, we can augment the text with\nadditional information by using natural language processing methods that\nleverage knowledge about the target domain drawn from within or outside\nthe dataset.\nWe explore four main methods, described in detail below: (1) word em-\nbeddings obtained from a domain-speci\ufb01c corpus; (2) correlation lexicons\nthat aim to identify from within the dataset additional words that are in-\ndicative of donations; (3) lexicons induced starting with a few seeds and\nusing external corpora and graph propagation; and (4) domain-speci\ufb01c dis-\ntance representations, re\ufb02ecting the semantic similarity between the textual\nfeatures and a set of domain-speci\ufb01c seeds.\nAll of these methods are illustrated, and later evaluated, using the dona-\ntion prediction task and associated dataset described above.\n4.1. Domain-speci\ufb01c Embeddings\nUnsupervised methods for learning word embeddings represent one of\nthe most recent successes in word representations (Mikolov et al. (2013);\nPennington et al. (2014)). As a \ufb01rst method to expand the text \ufb01elds we\nthus use word embeddings.\nWe construct a corpus of articles that discuss philanthropy-related topics\nfrom the New York Times that we will refer to as the NYT Philanthropy News\ncorpus. We use their API2 and collect 8,525 articles dated from January 1981\nto March 2017. The \ufb01nal corpus includes 57 million words, with a vocabulary\nof 94,623 words. Of those, only the words that occur \ufb01ve times or more are\nconsidered during the training of the GloVe model; 32,324 such words exist\nin the corpus.\nWe create a set of word embeddings using the GloVe embedding model\n(Pennington et al. (2014)) trained on this philanthropy-focused news corpus.\nWe chose to use GloVe as it was shown to have better performance on several\nword representation and word similarity tasks (Pennington et al. (2014);\nHamilton et al. (2016)). We use 300 dimensions for the embeddings, as is\n2https://developer.nytimes.com/\n10\n\nstandard practice3. For each text \ufb01eld in the dataset, we take the constituent\nwords. The embeddings for all of the words from every text \ufb01eld are then\naveraged to form a feature vector.\n4.2. Correlation Lexicons\nPrevious work has shown that domain-speci\ufb01c lexicons can be e\ufb00ectively\nused to induce features for prediction tasks. Speci\ufb01cally, our method is in-\nspired from previous work on sentiment analysis, where a lexicon of positive\nand negative words generated speci\ufb01cally for Twitter was found to bring sig-\nni\ufb01cant improvements (Mohammad et al. (2013)). We adapt their method\nto our task, and generate a lexicon of words that are speci\ufb01c to the task of\ndonation.\nUsing pointwise mutual information (PMI), as done in (Mohammad et al.\n(2013)), we measure the strength of association between each word in the\ndataset and the labels of donation/no-donation. The words are drawn from\nall the textual \ufb01elds, consisting of the degree levels, degree types, degree ma-\njors, job titles, and job companies. Note that the correlations are calculated\nonly from the training data. Speci\ufb01cally, given a word W, we calculate its\nPMI score as:\nPMIScore (W) =PMI (W, donated)\n\u2212PMI (W, nondonated) (1)\nwhere the PMI (W, class) for any of the two classes is calculated as:\nPMI (W, class) = log p(W, class)\np(W)p(class)\nTo create the lexicon, we \ufb01rst calculate the PMIScore for each of the\nwords included in the text \ufb01elds in the dataset, as described in Section 3.2.\nWe then rank the words in decreasing order of their score, and select the\ntop 30 with the assumption that the words that have the highest score are\nmost strongly correlated with the class of donation. Table 3 shows the top\n10 words from a generated lexicon.\n3We use the author-provided code for GloVe at https://github.com/stanfordnlp/\nGloVe. All parameters are left as default other than the embedding size.\n11\n\nSample words\nTop 10 (donation) educational, partner,\nm.d., j.d., ceo, profes-\nsional, board, law, owner,\nmanaging\nTable 3: Sample words from the PMI lexicon\nUsing the PMI lexicon, we generate 30 binary features, one for each entry\nin the lexicon. We set the value of each feature to 1 (0), re\ufb02ecting the presence\n(absence) of the feature in any of the text \ufb01elds.\n4.3. Seed-Induced Lexicons\nThe third method we consider is to generate a lexicon starting with a\nfew seed words and expanding the set of words using a label propagation\nalgorithm on a lexical graph. We use the SentProp method introduced in\n(Hamilton et al. (2016)), which was originally proposed for the task of build-\ning a lexicon for sentiment analysis.\nWe \ufb01rst manually build two sets of seed words, associated with philan-\nthropic tendencies and the lack thereof, respectively. Table 4 shows these\nseed words.\nSeed words\nDonation donation, endowment,\ninvestment, charity, gen-\nerosity, benefaction, giver,\ngrantor, donor, donator,\nbenefactor, benefactress,\nendow, sponsor, backer\nNon-donation miserly, stingy, unchari-\ntable, ungenerous, frugal,\nsel\ufb01sh, skimping, scrimp-\ning, tight\ufb01sted, close\ufb01sted,\nparsimonious, inhospitable,\ngreedy, cheap\nTable 4: Seed words used to generate the SentProp lexicon\n12\n\nWe then build a weighted lexical graph using the words from the text\n\ufb01elds in the dataset, as well as all of the seed words. Each word is connected\nto its nearest 10 neighbors by using a measure of cosine similarity applied\non word embedding representations for each word that is present in both\nthe dataset vocabulary and the trained word embeddings. We use GloVe\nembeddings, following the original SentProp implementation.\nThe donation and non-donation labels are then propagated through the\ngraph using a random walk method. Finally, a word\u2019s donation score is cal-\nculated as the probability of a random walk from the corresponding seed set\nhitting that word. In our experiments, we try lexicon generation using both\ngeneric pre-trained GloVe embeddings4 as well as GloVe embeddings that we\ntrain on the NYT Philanthropy News corpus. They perform comparably; we\nonly show results using the latter embeddings.\nTo create the \ufb01nal lexicon, we take only the words that have a dona-\ntion association score higher than 0 .7. We chose this threshold heuristically;\nlower thresholds introduced noisy words and higher thresholds excluded many\nwords that appear in the dataset. Sample words from the resulting lexicon\nare shown in Table 5. As with the PMI lexicon, we create a feature for each\nof the lexicon words, and set its value as 1 (0) depending on whether the\nfeature is present (absent) among the words in the text \ufb01elds.\nSample words\nNYT GloVe based contributor, giving,\ninvestor, management,\nbanking, mutual, ven-\nture, institutional, pro\ufb01t,\ncorporate, philanthropy,\nmarket, cash, asset,\nhedge, managed\nTable 5: Seed-induced lexicon entries using label propagation on graphs\n4.4. Seed-Similarity Embeddings\nFinally, as an alternative to the previous seed-induced lexicon method,\nwe also consider a method that measures the semantic distance between the\n4https://nlp.stanford.edu/projects/glove/\n13\n\nSource Features\nBaselines\nDI degree level, degree type, degree major\nLinkedIn most recent three job titles, most recent three compa-\nnies\nText expansion features\nDomainEmbed 300-dimension GloVe embeddings trained on the do-\nnation corpus, averaged over all the words in the text\n\ufb01elds\nCorrelLex 30-word correlation lexicon generated from training\ndata (text \ufb01elds in both DI and LinkedIn); one fea-\nture for each lexicon word, re\ufb02ecting presence/absence\namong words from text \ufb01elds\nSeedProp Seed-induced donation lexicon using label propaga-\ntion on a lexical graph formed by using pretrained\nGloVe embeddings; one feature for each lexicon word,\nre\ufb02ecting presence/absence among words from text\n\ufb01elds\nSeedSim Semantic similarity between the text \ufb01elds and the 15\ndonation seeds, using cosine similarity between pre-\ntrained GloVe embeddings; one feature for each of the\n15 donation seeds\nTable 6: Summary of features\n14\n\nwords in the text \ufb01elds and the donation seed words. The hypothesis behind\nthis method is that we can circumvent the need for a domain-speci\ufb01c corpus\nby measuring the distance between a small set of domain words and the text\n\ufb01elds.\nWe use the same seed set as listed in Table 4 (row Donation). We use\nthe pre-trained GloVe embeddings with 300 dimensions. For each seed word,\nwe \ufb01nd the maximum cosine similarity score between that word\u2019s embedding\nand each of the word embeddings from the text \ufb01elds. The result is a feature\nvector that re\ufb02ects these maximum similarity scores, and is the same length\nas the seed set.\n5. Results and Discussion\nWe evaluate the performance of the donation prediction task described\nin Section 3 using the original features available in the dataset, as well as ex-\npanded feature sets obtained with the four text expansion methods described\nabove. Table 6 summarizes the features we use, described in the previous\nsections.\nThe top part of Table 7 shows the results obtained with the two baselines\n(DI features, and DI combined with LinkedIn features), while the bottom part\nof the table shows the results obtained when augmenting the top perform-\ning baseline with the various text-expansion features. We combine features\nby concatenating their feature vectors. This combination method has been\nshown to work well in many applications (Argamon et al. (2007); Le and\nMikolov (2014); Maas et al. (2011)).\nStatistical signi\ufb01cance over the DI+LinkedIn baseline is calculated using\nthe McNemar two-tailed test. We used an alpha value of 0.05.\nAmong the four text expansion methods, the correlation lexicons, domain-\nspeci\ufb01c embeddings, and seed-induced lexicon result in signi\ufb01cant improve-\nments over the baselines as seen from Table 7. The seed-similarity embedding\nfeatures also bring small improvements, but they are not found to be signif-\nicant.\nTo gain further insight into the performance of these models, we perform\nseveral additional analyses and evaluations, which we describe next.\n5.1. Model Correlation\nFirst, we measure the correlation between the output produced by the top\nbaseline model (DI+LinkedIn) and by the four di\ufb00erent methods considered.\n15\n\nSource Accuracy\nBaselines\nDI 68.8%\nDI+LinkedIn 76.8%\nText expansion features\nDI+LinkedIn+DomainEmbed 81.3%\u2217\nDI+LinkedIn+CorrelLex 80.1%\u2217\nDI+LinkedIn+SeedProp 78.5%\u2217\nDI+LinkedIn+SeedSim 77.2%\nTable 7: Donation prediction results using text expansion methods. Results with \u2217 are\nstatistically signi\ufb01cant compared to the DI+LinkedIn baseline system.\nDI+LinkedIn +SeedSim +SeedProp +DomainEmbed +CorrelLex\nDI+LinkedIn 1.0 0.90 0.83 0.56 0.63\n+SeedSim 1.0 0.83 0.57 0.63\n+SeedProp 1.0 0.56 0.66\n+DomainEmbed 1.0 0.68\n+CorrelLex 1.0\nTable 8: Pearson correlation coe\ufb03cients among the output of the four models and the\nbaseline. Each model includes the DI and LinkedIn categorical features with the speci\ufb01ed\nadditional single feature type.\n16\n\nSource Accuracy\nDI 66.5%\nDI+LinkedIn 72.6%\nDI+LinkedIn+DomainEmbed 75.3%*\nDI+LinkedIn+CorrelLex 75.6%*\nDI+LinkedIn+SeedProp 73.3%\nDI+LinkedIn+SeedSim 73.9%\nTable 9: Classi\ufb01cation results when non-donors include alumni who donated any amount\nbelow $10,000. Results with \u2217 are statistically signi\ufb01cant compared to the DI+LinkedIn\nbaseline.\nTable 8 shows the Pearson correlation between all the pairs of two models.\nAs seen in this table, most of the models are medium correlated, which\nindicates there is some overlap between the predictions they make. The\nmodels that are most divergent from the baseline are the correlated lexicons\n(CorrelLex) and the domain speci\ufb01c embeddings (DomainEmbed), which is\nalso re\ufb02ected in the higher performance of these models (see Table 7). The\nhighest correlation is found between the model that measures the similarity\nwith the seed set (SeedSim) and the model that performs label propagation\non a lexical graph starting with the seed set (SeedProp); their high correlation\nis likely a re\ufb02ection of the dependence of these two models on the same seed\nset.\n5.2. Classi\ufb01cation with Less Distinguishable Classes\nWe also perform an evaluation for a classi\ufb01cation task where the division\nbetween donors and non-donors is less clear. Speci\ufb01cally, we again consider\nall the alumni who donated $10,000 and above as donors, but now we con-\nsider alumni who donated any amount below $10,000 as a non-donor. The\nnon-donors are randomly sampled from the instances corresponding to peo-\nple who donated less than $10,000. Table 9 shows the results obtained during\nthese evaluations. As expected, all the results are lower than the ones ob-\ntained during the earlier evaluations. In this more di\ufb03cult setup, the use\nof correlation lexicons and domain embeddings continues to bring consistent\nimprovements over the baseline.\n17\n\n5.3. In\ufb02uence of Seeds on Sentprop Lexicon\nWe analyze the e\ufb00ects of changing the seed words used for SentProp by\nlooking at the overlap between our original lexicon and the new lexicons\ngenerated using di\ufb00erent seed words. To highlight the e\ufb00ects of di\ufb00erent\ndonation words, we change the donation words to be entirely di\ufb00erent from\nthose used in our experiments, but maintain the topic of donation among the\nwords. For each of the three di\ufb00erent donation word sets, the non-donation\nwords remain the same as those used in our experiments. To understand the\nin\ufb02uence of the non-donation words, we also choose a set of random words\nas non-donation words. For this, we retain the same set of donation words\nas used in our experiments. The chosen words are shown in Table 10.\nFor these di\ufb00erent sets of seed words, we generate lexicons with SentProp\non the NYT Donation corpus at two di\ufb00erent association score thresholds.\nWe measure the overlap between the new lexicons and the original one used\nin our experiments by calculating their Jaccard similarity and overlap coef-\n\ufb01cient. For two sets of words, X and Y, we have\nJaccard(X, Y) = |X \u2229Y |/|X \u222aY |\nand\nOverlap(X, Y) = |X \u2229Y |/min(|X|, |Y |).\nThe new lexicons corresponding to altered donation words (Set 1, Set 2,\nSet 3), generated at an association score threshold of 0.7, contain many more\nwords that are not related to philanthropy. The large size of the lexicon\nis indicative of this. This could be a result of the seed words not being as\nunambiguously tied in topic as the original set of seeds. However, the overlap\ncoe\ufb03cients are close to 1, showing that the original lexicon words are present\nin the new lexicons. This implies that the donation topic was still captured,\nbut with much more noise.\nWe raise the association score threshold to \ufb01lter out the less relevant\nwords. Overall, the lexicons resulting from Set 1, Set 2, and Set 3 still\nmaintain much overlap with the original lexicon. Set 2\u2019s lexicon has a much\nlower overlap coe\ufb03cient than Set 1 or Set 3. This is likely because Set 2\u2019s\ndonation seeds contain words like \u201ckind\u201d and \u201ccharitable\u201d that have more\nambiguous meanings.\nInterestingly, having random non-donation words does not greatly per-\nturb the captured topic of the lexicon. All words generated also appear in the\noriginal lexicon. Additionally, the number of words is actually smaller than\n18\n\nSeed Words Donation Non-donation\nSet1 contribution, gift, funding, foun-\ndation\nmiserly, stingy, uncharitable,\nungenerous, frugal, sel\ufb01sh,\nskimping, scrimping, tight\ufb01sted,\nclose\ufb01sted, parsimonious,\ninhospitable, greedy, cheap\nSet2 kind, supporter, charitable, pa-\ntron\nSet3 contribution, gift, funding, foun-\ndation, kind, supporter, charita-\nble, patron, compassionate\nNegRand donation, endowment, invest-\nment, charity, generosity, bene-\nfaction, giver, grantor, donor, do-\nnation, benefactor, benefactress,\nendow, sponsor, backer\ncattle, evanescent, vague, jit-\ntery, trade, grade, excited, sig-\nnify, clear, toad\nTable 10: Di\ufb00erent sets of seed words used for SentProp. Sets 1-3 retain the same set of\nnon-donation words as used in the experiments, but with di\ufb00erent donation words. Ne-\ngRand retains the same set of experiment donation words, but with random non-donation\nwords.\nthe original set. This may be because having random non-donation words\nencourages SentProp to choose words that are unambiguously related to the\ndonation words. There is a separation of the donation topic from e\ufb00ectively\nall others, rather than from just the non-donation topic. This is desirable in\napplications where we are primarily interested in generating a lexicon related\nto one theme, rather than two polar themes, as is the case here.\n5.4. Error Analyses\nTo better understand the performance of our methods, and where and\nwhen they fail, we perform several error analyses. Speci\ufb01cally, since the\njob related information from LinkedIn was the most varied, and therefore\nthe area that could bene\ufb01t the most from our text expansion methods, we\nmainly focus our analyses on how well our features understood LinkedIn\ninformation. We have anonymized the examples below by excluding names\nand modifying job titles to be generic.\nCategorical features were not able to understand complex or non-standard\njob titles such as \u201cDirector of Major, Planned, and Special Gifts\u201d, \u201cSe-\nnior Director of Major Gifts\u201d, or \u201cCEO of A Philanthropic Trust\u201d. These\nparticular job titles are highly indicative of philanthropic tendencies, but\n19\n\nSeed Words (Threshold) Lexicon Size Jaccard Sim. Overlap Coef.\nSet1 (0.7) 897 0.14 0.95\nSet2 (0.7) 1667 0.08 1.00\nSet3 (0.7) 929 0.13 0.95\nNegRand (0.7) 49 0.37 1.00\nSet1 (0.8) 37 0.17 0.65\nSet2 (0.8) 126 0.21 0.35\nSet3 (0.8) 48 0.21 0.65\nNegRand (0.8) 0 0.00 0.00\nTable 11: Number of words, Jaccard similarity, and overlap coe\ufb03cients for di\ufb00erent sets\nof seed words at di\ufb00erent association score thresholds. Jaccard similarity and overlap\ncoe\ufb03cient are calculated with respect to the generated lexicon used in the experiments.\nThe original generated lexicon has 132 words.\nthe categorical-only DI+LinkedIn model classi\ufb01ed these individuals as non-\ndonors. The categorical model also was not able to correctly detect people\nworking in known high-pay \ufb01elds because of non-standard titles. For in-\nstance, one donor is a \u201cPulmonary Specialist\u201d, which is a type of doctor.\nFrom our data (Figure 4), we can see that health care professionals are the\nmost charitable individuals. However, the categorical model was unable to\nmake the association between \u201cPulmonary Specialist\u201d and the health care\nprofession.\nThe embedding features helped \ufb01nd such associations. The \u201cPulmonary\nSpecialist\u201d was found to be a donor by the model that incorporated embed-\nding features. It was also much better at detecting individuals with advanced\ncareer positions such as \u201cSenior Vice President\u201d, \u201cStrategic Advisor\u201d, and\n\u201cExecutive Director\u201d. While these titles may seem obvious, there exist many\nvariations on advanced titles, such as \u201cManaging Director\u201d, \u201cPrincipal Advi-\nsor\u201d, and \u201cCreative Director\u201d. Embedding features implicitly help the model\nunderstand that positions like these are indicative of donors, without explic-\nitly having a list of such titles. However, the embeddings were not good at\ndistinguishing between those who had a single position indicative of a donor\nand those who had a history of such positions.\nThe correlation lexicon features focused on \ufb01nding individuals that held\nmultiple indicative positions. For example, some of the donors that were cor-\nrectly identi\ufb01ed only by including CorrelLex each had at least three advanced\n20\n\ncareer positions. One was a \u201cSenior Counselor\u201d, \u201cCEO\u201d, and \u201cFounder\u201d; an-\nother was a \u201cSenior Development O\ufb03cer\u201d, \u201cConsultant\u201d, and \u201cPresident\u201d;\nand yet another was a \u201cSenior Manager\u201d, \u201cChief Operating O\ufb03cer\u201d, and\n\u201cSenior Clinical Manager\u201d. These results follow the fact that the generated\ncorrelation lexicons from Table 3 seem to mainly focus on high income or\nadvanced titles. However, this misses people who are philanthropic but do\nnot necessarily hold traditional advanced positions.\nSome of the donors that were correctly identi\ufb01ed only by using SeedProp\nhad titles such as \u201cHead of Police Board\u201d, \u201cWorkplace Learning Special-\nist\u201d, \u201cProgram Director/Scholarship Manager\u201d, or worked at foundations.\nThese careers involve public service, interacting with people, and being in\nenvironments that are geared towards philanthropy. SeedSim produced sim-\nilar results, though the detected associations were limited to very explicit\nindicators, such as someone being an \u201cEvangelist\u201d.\n5.5. Evaluation on Other Datasets\nWe also want to determine to what extent our methods can be applied\nto other datasets. Although there are public donation records available at\ncrowdfunding sites such as Kickstarter.com and DonorsChoose.org, there is\nusually little information revealed about the donors themselves beyond what\nthey have donated to.\nWe therefore evaluate our proposed text expansion methods on a di\ufb00erent\ntask: gender classi\ufb01cation on a dataset of blog pro\ufb01les collected from Blog-\nger.com. Previous work has shown that it is possible to detect demographic\ninformation such as gender from writings and social media content Farnadi\net al. (2018); Mukherjee and Liu (2010); Schler et al. (2006); Sarawgi et al.\n(2011).\nBloggers can choose to \ufb01ll in information, such as gender, occupation,\nand interests, on their pro\ufb01le page. We use a set of 76,971 pro\ufb01les that have\nboth gender and interests listed and are in the USA. The full set of features\nis listed in Table 12. We classify each blogger as male or female based on the\ninformation available on their pro\ufb01le.\nOur text expansion features are replicated for gender in this setting. The\ndomain embeddings are trained on the blog dataset. The correlation lexicon\nis generated from the training set of blog data. Gender-based seed words\nare used for SeedProp and SeedSim. The results are shown in Table 13.\nAll of the text expansion methods improve signi\ufb01cantly over the baseline\ncategorical method, with the domain embeddings (DomainEmbed) yielding\n21\n\nSource Features\nBlogs gender, interests,T , occupationT , city, state, country,\nintroductionT , moviesT , musicT , booksT\nTable 12: Blog dataset features (text \ufb01elds are marked with T )\nSource Accuracy\nBlogs 70.6%\nBlogs+DomainEmbed 83.3%*\nBlogs+CorrelLex 75.6%*\nBlogs+SeedProp 72.0%*\nBlogs+SeedSim 74.5%*\nTable 13: Gender prediction results on blog pro\ufb01les. Results with \u2217 are statistically\nsigni\ufb01cant compared to the Blog baseline.\nthe highest performance. These results demonstrate that our methods can\nbe successfully applied to other datasets.\n6. Conclusions\nIn this paper, we explored whether we can enhance sparse textual con-\ntent to improve data-driven predictions using the task of alumni donation\nprediction.\nWe introduced a dataset of alumni donations, and we qualitatively an-\nalyzed the donations and the backgrounds of the donors to highlight the\ndi\ufb00erences between the backgrounds of donors and non-donors as well as the\npatterns of donations attracted by di\ufb00erent academic departments.\nWe used four di\ufb00erent methods of expanding sparse text, including lex-\nicon generation methods and text embedding methods. We evaluated these\nmethods on the task of predicting whether someone is likely to donate, and\ncompared with baseline models that do not make use of any textual features.\nWe showed that we can classify large donors from non-donors with an\naccuracy of up to 80%. We also showed that the enrichment of sparse text\nthrough the extraction and use of textual features does bene\ufb01t model per-\nformance. Our domain-speci\ufb01c embeddings and correlation-based lexicon\nconsistently improved over the baseline models that only use categorical fea-\n22\n\ntures. We also showed that our methods can be successfully applied to other\nsparse-text datasets.\nArgamon, S., Whitelaw, C., Chase, P., Hota, S. R., Garg, N., Levitan, S.,\n2007. Stylistic text classi\ufb01cation using functional lexical features. Journal\nof the American Society for Information Science and Technology 58 (6),\n802\u2013822.\nBanerjee, S., Pedersen, T., 2002. An adapted lesk algorithm for word sense\ndisambiguation using wordnet. In: International conference on intelligent\ntext processing and computational linguistics. Springer, pp. 136\u2013145.\nDeerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., Harshman,\nR., 1990. Indexing by latent semantic analysis. Journal of the American\nsociety for information science 41 (6), 391.\nEsuli, A., Sebastiani, F., 2007a. Pageranking wordnet synsets: An application\nto opinion mining. In: ACL. Vol. 7. pp. 442\u2013431.\nEsuli, A., Sebastiani, F., 2007b. Sentiwordnet: A high-coverage lexical re-\nsource for opinion mining. Evaluation, 1\u201326.\nFarnadi, G., Tang, J., De Cock, M., Moens, M.-F., 2018. User pro\ufb01ling\nthrough deep multimodal fusion. In: Proceedings of the Eleventh ACM\nInternational Conference on Web Search and Data Mining. ACM, pp. 171\u2013\n179.\nFaruqui, M., Dodge, J., Jauhar, S. K., Dyer, C., Hovy, E., Smith, N. A.,\n2015. Retro\ufb01tting word vectors to semantic lexicons. In: Proc. of NAACL.\nHamilton, W. L., Clark, K., Leskovec, J., Jurafsky, D., 2016. Inducing\ndomain-speci\ufb01c sentiment lexicons from unlabeled corpora. arXiv preprint\narXiv:1606.02820.\nHoyt, J. E., 2004. Understanding alumni giving: Theory and predictors of\ndonor status. Online Submission.\nHu, M., Liu, B., 2004. Mining and summarizing customer reviews. In: Pro-\nceedings of the tenth ACM SIGKDD international conference on Knowl-\nedge discovery and data mining. ACM, pp. 168\u2013177.\n23\n\nJiang, L., Yu, M., Zhou, M., Liu, X., Zhao, T., 2011. Target-dependent\ntwitter sentiment classi\ufb01cation. In: Proceedings of the 49th Annual Meet-\ning of the Association for Computational Linguistics: Human Language\nTechnologies-Volume 1. Association for Computational Linguistics, pp.\n151\u2013160.\nKenter, T., Borisov, A., de Rijke, M., 2016. Siamese cbow: Optimizing word\nembeddings for sentence representations. CoRR abs/1606.04640.\nKenter, T., De Rijke, M., 2015. Short text similarity with word embeddings.\nIn: Proceedings of the 24th ACM international on conference on informa-\ntion and knowledge management. ACM, pp. 1411\u20131420.\nKiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun, R., Torralba,\nA., Fidler, S., 2015. Skip-thought vectors. In: Advances in neural informa-\ntion processing systems. pp. 3294\u20133302.\nLe, Q. V., Mikolov, T., 2014. Distributed representations of sentences and\ndocuments. In: ICML. Vol. 14. pp. 1188\u20131196.\nLevy, O., Goldberg, Y., 2014. Neural word embedding as implicit matrix\nfactorization. In: Advances in neural information processing systems. pp.\n2177\u20132185.\nLevy, O., Goldberg, Y., Dagan, I., 2015. Improving distributional similarity\nwith lessons learned from word embeddings. Transactions of the Associa-\ntion for Computational Linguistics 3, 211\u2013225.\nMaas, A., Daly, R., Pham, P., Huang, D., Ng, A., Potts, C., 2011. Learning\nword vectors for sentiment analysis. In: Proceedings of the Association for\nComputational Linguistics (ACL 2011). Portland, OR.\nMcDearmon, J. T., 2013. Hail to thee, our alma mater: Alumni role identity\nand the relationship to institutional support behaviors. Research in Higher\nEducation 54 (3), 283\u2013302.\nMeer, J., Rosen, H. S., 2012. Does generosity beget generosity? alumni giving\nand undergraduate \ufb01nancial aid. Economics of Education Review 31 (6),\n890\u2013907.\n24\n\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., Dean, J., 2013. Dis-\ntributed representations of words and phrases and their compositionality.\nIn: Advances in neural information processing systems. pp. 3111\u20133119.\nMiller, G. A., 1995. Wordnet: a lexical database for English. Communications\nof the ACM 38 (11), 39\u201341.\nMohammad, S., Kiritchenko, S., Zhu, X., 2013. Nrc-canada: Building the\nstate-of-the-art in sentiment analysis of tweets. In: Proceedings of Semeval.\nMohammad, S. M., Turney, P. D., 2010. Emotions evoked by common words\nand phrases: Using mechanical turk to create an emotion lexicon. In: Pro-\nceedings of the NAACL HLT 2010 workshop on computational approaches\nto analysis and generation of emotion in text. Association for Computa-\ntional Linguistics, pp. 26\u201334.\nMukherjee, A., Liu, B., 2010. Improving gender classi\ufb01cation of blog au-\nthors. In: Proceedings of the Conference on Empirical Methods in natural\nLanguage Processing. pp. 207\u2013217.\nOtt, M., Choi, Y., Cardie, C., Hancock, J. T., 2011. Finding deceptive opin-\nion spam by any stretch of the imagination. In: Proceedings of the 49th\nAnnual Meeting of the Association for Computational Linguistics: Human\nLanguage Technologies-Volume 1. Association for Computational Linguis-\ntics, pp. 309\u2013319.\nPennebaker, J. W., Graybeal, A., 2001. Patterns of natural language use:\nDisclosure, personality, and social integration. Current Directions in Psy-\nchological Science 10 (3), 90\u201393.\nPennington, J., Socher, R., Manning, C. D., 2014. Glove: Global vectors for\nword representation. In: EMNLP. Vol. 14. pp. 1532\u20131543.\nRao, D., Ravichandran, D., 2009. Semi-supervised polarity lexicon induction.\nIn: Proceedings of the 12th Conference of the European Chapter of the\nAssociation for Computational Linguistics. Association for Computational\nLinguistics, pp. 675\u2013682.\nSarawgi, R., Gajulapalli, K., Choi, Y., 2011. Gender attribution: tracing sty-\nlometric evidence beyond topic and genre. In: Proceedings of the Fifteenth\n25\n\nConference on Computational Natural Language Learning. Association for\nComputational Linguistics, pp. 78\u201386.\nSchler, J., Koppel, M., Argamon, S., Pennebaker, J., 2006. E\ufb00ects of age and\ngender on blogging. In: Proceedings of 2006 AAAI Spring Symposium on\nComputational Approaches for Analyzing Weblogs. Stanford, pp. 199\u2013204.\nSchwartz, H. A., Eichstaedt, J. C., Kern, M. L., Dziurzynski, L., Ramones,\nS. M., Agrawal, M., Shah, A., Kosinski, M., Stillwell, D., Seligman, M. E.,\net al., 2013. Personality, gender, and age in the language of social media:\nThe open-vocabulary approach. PloS one 8 (9), e73791.\nTaboada, M., Brooke, J., To\ufb01loski, M., Voll, K., Stede, M., 2011. Lexicon-\nbased methods for sentiment analysis. Computational linguistics 37 (2),\n267\u2013307.\nWilson, T., Wiebe, J., Ho\ufb00mann, P., 2005. Recognizing contextual polarity\nin phrase-level sentiment analysis. In: Proceedings of the conference on\nhuman language technology and empirical methods in natural language\nprocessing. Association for Computational Linguistics, pp. 347\u2013354.\nYu, L., Hermann, K. M., Blunsom, P., Pulman, S. G., 2014. Deep learning\nfor answer sentence selection. CoRR abs/1412.1632.\n26",
  "metadata": {
    "title": "Luan_2021_Sparse_Dense_Attentional_Representations",
    "author": "Unknown",
    "pages": 26,
    "source_file": "Luan_2021_Sparse_Dense_Attentional_Representations.pdf",
    "pdf_library": "pypdf"
  },
  "page_mapping": {
    "0": [
      1,
      0,
      1945
    ],
    "1945": [
      2,
      1945,
      4674
    ],
    "4674": [
      3,
      4674,
      7041
    ],
    "7041": [
      4,
      7041,
      9563
    ],
    "9563": [
      5,
      9563,
      11979
    ],
    "11979": [
      6,
      11979,
      13905
    ],
    "13905": [
      7,
      13905,
      16078
    ],
    "16078": [
      8,
      16078,
      17590
    ],
    "17590": [
      9,
      17590,
      17833
    ],
    "17833": [
      10,
      17833,
      20029
    ],
    "20029": [
      11,
      20029,
      21898
    ],
    "21898": [
      12,
      21898,
      23172
    ],
    "23172": [
      13,
      23172,
      25076
    ],
    "25076": [
      14,
      25076,
      25997
    ],
    "25997": [
      15,
      25997,
      28192
    ],
    "28192": [
      16,
      28192,
      28928
    ],
    "28928": [
      17,
      28928,
      30776
    ],
    "30776": [
      18,
      30776,
      33155
    ],
    "33155": [
      19,
      33155,
      35269
    ],
    "35269": [
      20,
      35269,
      37409
    ],
    "37409": [
      21,
      37409,
      39910
    ],
    "39910": [
      22,
      39910,
      41572
    ],
    "41572": [
      23,
      41572,
      43381
    ],
    "43381": [
      24,
      43381,
      45216
    ],
    "45216": [
      25,
      45216,
      47212
    ],
    "47212": [
      26,
      47212,
      48383
    ]
  }
}