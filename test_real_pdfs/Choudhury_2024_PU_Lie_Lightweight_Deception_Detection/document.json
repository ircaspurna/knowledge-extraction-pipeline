{
  "text": "PU-Lie: Lightweight Deception Detection in\nImbalanced Diplomatic Dialogues via\nPositive-Unlabeled Learning\nBhavinkumar Vinodbhai Kuwar1,*, Bikrant Bikram Pratap Maurya 1,*, Priyanshu Gupta 1,*, Nitin Choudhury 1\n1Indraprastha Institute of Information Technology Delhi\nNew Delhi, India\nEmails: {bhavinkumar24212, bikrant24116, priyanshu24130, nitinc }@iiitd.ac.in\n*Contributed equally as first authors.\nAbstract\u2014Detecting deception in strategic dialogues is a\ncomplex and high-stakes task due to the subtlety of language\nand extreme class imbalance between deceptive and truthful\ncommunications. In this work, we revisit deception detection\nin the Diplomacy dataset, where less than 5% of messages are\nlabeled deceptive. We introduce a lightweight yet effective model\ncombining frozen BERT embeddings, interpretable linguistic and\ngame-specific features, and a Positive-Unlabeled (PU) learning\nobjective. Unlike traditional binary classifiers, PU-Lie is tailored\nfor situations where only a small portion of deceptive messages are\nlabeled, and the majority are unlabeled. Our model achieves a new\nbest macro F1 of 0.60 while reducing trainable parameters by over\n650\u00d7. Through comprehensive evaluations and ablation studies\nacross seven models, we demonstrate the value of PU learning,\nlinguistic interpretability, and speaker-aware representations.\nNotably, we emphasize that in this problem setting, accurately\ndetecting deception is more critical than identifying truthful\nmessages. This priority guides our choice of PU learning, which\nexplicitly models the rare but vital deceptive class.\nI. I NTRODUCTION\nDeception plays a pivotal role in strategic communication,\nshaping the outcomes of negotiations, military strategies,\nand political campaigns. In environments like the game of\nDiplomacy, where trust and betrayal are in constant tension,\nthe ability to detect lies can be the difference between victory\nand defeat. Automated deception detection in such scenarios\nremains challenging due to the nuanced nature of human\nlanguage and the extreme imbalance in class distribution.\nDeception detection has long been a critical research chal-\nlenge in computational linguistics, psychology, and security\ndomains. Early efforts primarily focused on manual feature\nengineering, where linguistic cues such as lexical diversity,\nhedging, sentiment polarity, and syntactic complexity were\nanalyzed to differentiate deceptive from truthful texts [ 1],\n[2]. While such methods offered interpretability, they lacked\nscalability and contextual depth.\nWith the advent of deep learning, particularly transformer-\nbased models like BERT, deception detection shifted towards\nend-to-end learning from raw text [ 3]. These models achieved\nsubstantial improvements across domains such as fake reviews,\nphishing emails, and online fraud [ 4], [5]. However, most of\nthese approaches presume balanced datasets or moderately\nskewed distributions, limiting their utility in real-world ap-\nplications where deceptive instances are sparse and highly\ncontextual.\nWe focus on the Diplomacy dataset introduced by Peskov\net al. (2020), which contains over 17,000 annotated messages\nexchanged between players. This dataset marked a pivotal\nshift by offering annotated real-world instances of deception\nembedded within strategic negotiations. However, only about\n4.5% of these messages are labeled as deceptive, resulting in\na highly skewed class distribution that hinders conventional\nsupervised models. Traditional solutions like oversampling,\nundersampling, or cost-sensitive learning (e.g., class weights\nor focal loss [ 6]) have been employed, but these methods often\nintroduce overfitting or rely on noisy/missing negative labels.\nTo overcome these limitations, an alternative\nparadigm\u2014Positive-Unlabeled (PU) Learning\u2014has been\nexplored in domains where only positive labels are reliable\nand negatives are ambiguous. PU learning has shown success\nin bioinformatics, fraud detection, and recommendation\nsystems [ 7], [ 8], as it estimates the risk of misclassification\nbased solely on the positive class and unlabeled examples.\nDespite its relevance, PU learning has seen limited application\nin NLP, and to our knowledge, no prior work has applied it to\ndeception in strategic communication. Our work emphasizes\nthe importance of prioritizing rare deception cases over more\nabundant truthful ones.\nIn parallel, models that combine deep learning with struc-\ntured and interpretable features are gaining traction. Stud-\nies like [ 9] highlight how integrating human-understandable\nfeatures (e.g., sentiment, discourse, power dynamics) with\nneural architectures can boost both accuracy and transparency.\nIn strategic games like Diplomacy, features such as power\nimbalance, pronoun usage, and game phase offer vital domain-\nspecific context often overlooked by purely neural models.\nGraph neural networks (GNNs) and dialogue-aware LSTMs\nhave also been explored to model player interactions [ 10], [11],\nbut these often suffer from overfitting under extreme class\nimbalance.\nDetecting deception is more critical than detecting truthful\ncommunication in this setting, as the consequences of missing\narXiv:2507.09157v1  [cs.CL]  12 Jul 2025\n\na lie can significantly affect strategic decisions. Hence, we\nprioritize models that perform well on the deceptive class.\nTraditional models often collapse to the majority class, failing\nto generalize or identify rare but impactful deceptive instances.\nTo this end, we introduce PU-Lie, a novel application\nand a compact model for deception detection. It integrates\nfrozen BERT embeddings with handcrafted linguistic and\ngame-specific features and employs a Positive-Unlabeled (PU)\nlearning approach to handle data scarcity and imbalance. PU-\nLie not only achieves a new state-of-the-art macro F1-score\nof 0.60 but does so with only 1,345 trainable parameters\u2014a\n650\u00d7 reduction from prior models. To this end, we introduce\nPU-Lie, a novel application and a compact model for decep-\ntion detection. It integrates frozen BERT embeddings with\nhandcrafted linguistic and game-specific features and employs\na Positive-Unlabeled (PU) learning approach to handle data\nscarcity and imbalance. PU-Lie not only achieves a new state-\nof-the-art macro F1-score of 0.60 but does so with only 1,345\ntrainable parameters\u2014a 650\u00d7 reduction from prior models.\nResearch Gap. Existing work on deception detection in\nstrategic dialogues largely relies on fully supervised learning,\nassuming access to balanced datasets with clear negative labels.\nHowever, this assumption breaks down in real-world scenarios\nlike Diplomacy, where deceptive messages are rare, unlabeled\nmessages dominate, and truth labels are often ambiguous or\nmissing. Moreover, most prior studies overlook the critical\nasymmetry in importance\u2014accurately detecting deception is far\nmore impactful than identifying truth. Despite this, few models\nhave explicitly prioritized deception or explored anomaly-based\napproaches such as Positive-Unlabeled (PU) learning to address\nthis skew. Finally, many existing models are computationally\ncomplex, limiting their applicability in low-resource or real-\ntime environments. This highlights the need for a lightweight,\ninterpretable, and deception-focused model that leverages PU\nlearning for effective detection in highly imbalanced dialogue\nsettings. Research Questions: This study is motivated by the\nfollowing core research questions:\n1) Can anomaly detection-inspired techniques such as\nPositive-Unlabeled (PU) learning effectively address\nextreme class imbalance in deception detection tasks?\n2) Does reducing the number of trainable parameters\nby several orders of magnitude (e.g., 650 \u00d7 fewer)\ncompromise the model\u2019s ability to detect subtle\ndeception cues?\n3) Is a lightweight model, with minimal architecture and\nhandcrafted features, sufficient to capture deceptive\nintent in complex strategic dialogues?\n4) To what extent do linguistic features\u2014such as hedg-\ning, pronoun usage, and sentiment\u2014contribute to\nidentifying lies in human language?\nOur main contributions are:\n\u2022 A PU-learning-based architecture that prioritizes deceptive\nmessage detection in highly imbalanced settings.\n\u2022 An interpretable and efficient feature set combining\nlinguistic signals and game-specific metadata.\n\u2022 Thorough experimentation across seven model variants,\ncovering deep, classical, and graph-based paradigms.\n\u2022 Focusing on deceptive messages rather than truthful.\n\u2022 A lightweight model with significantly less trainable\nparameters, a significant reduced training time and a\nsignificant reduced inference time.\nOur main contributions are:\n\u2022 A PU-learning-based architecture that prioritizes deceptive\nmessage detection in highly imbalanced settings.\n\u2022 An interpretable and efficient feature set combining\nlinguistic signals and game-specific metadata.\n\u2022 Thorough experimentation across seven model variants,\ncovering deep, classical, and graph-based paradigms.\n\u2022 Focusing on deceptive messages rather than truthful.\n\u2022 A lightweight model with significantly less trainable\nparameters, a significant reduced training time and a\nsignificant reduced inference time.\nII. R ELATED WORK\nDeception Detection.Early studies explored linguistic mark-\ners such as hedging, sentiment, and syntactic complexity\nto detect deception [ 1], [ 2], [ 12]. With the rise of deep\nlearning, models like RNNs and transformers have been\napplied to deception tasks on domains like fake reviews\nand misinformation [ 4], [5]. However, these assume balanced\ndatasets and often fail under severe class skew.\nStrategic Dialogue and Diplomacy. Peskov et al. (2020)\nintroduced the Diplomacy dataset\u2014a rare collection of real\ndeception instances from strategic gameplay. Their BERT +\nLSTM model set a benchmark but remained resource-heavy\nand under-optimized for imbalance.\nImbalanced Learning. Traditional methods include class\nweighting, resampling, SMOTE [ 13] and focal loss [ 6], but\nthey still rely on supervised labels for both classes. These\nassumptions do not hold in deception tasks, where negative\nlabels may be ambiguous or missing.\nPositive-Unlabeled Learning. PU learning addresses sce-\nnarios with few labeled positives and many unlabeled samples.\nIt estimates true risk using the known positive subset and class\nprior [7]. Though PU learning is widely used in fraud detection\nand bioinformatics [7], it has also been applied to review spam\ndetection in NLP [ 14] and classifier learning from limited\nlabels [8]. Our work emphasizes the importance of prioritizing\nrare deception cases over more abundant truthful ones.\nHybrid Architectures. Models that combine BERT with\nstructured or interpretable features have shown improved\ngeneralization and user trust [ 9]. PU-Lie follows this path\nby blending semantics with linguistics and domain-specific\nsignals.\nIII. M ETHODOLOGY\nWe evaluate seven models for deception detection. Our best\nmodel, PU-Lie, uses the following components:\n\nA. Dataset\nThe Diplomacy dataset consists of 17,289 private messages\nfrom 12 complete games. Each instance includes rich game\nand message metadata. Key fields:\n\u2022 messages: Sequence of text messages in dialogue.\n\u2022 speakers, recipients: Player roles per message.\n\u2022 sender labels, receiver labels: Deception tags: true (lie),\nfalse (truth), NOANNOTATION.\n\u2022 game score, game score delta: Player scores and dif-\nferences.\n\u2022 seasons, years: Game timeline (e.g., Spring 1902).\n\u2022 absolute message index: The index the message is in\nthe entire game, across all dialogs\n\u2022 relative message index: The index of the message in\nthe current dialog\n\u2022 game id Unique game.\n\u2022 players: Involved players(Countries).\nExample \u2014 Truthful Message:\nTurkey: That sounds good to me. I\u2019ll move to defend\nagainst Italy while you move west.\nExample \u2014 Deceptive Message:\nItaly: We\u2019re friends, right? I believe every single\nmessage I\u2019ve sent you all game has been truth...\nThe key challenge lies in identifying deceptive messages\n(\u02dc4.5%) amid mostly truthful exchanges. We address this by\ntreating unlabeled instances as a mix of positives and negatives\nunder PU learning, focusing our model design on maximizing\ndeceptive class detection performance.\nB. Proposed Methodology\nOur proposed system, PU-Lie, is a lightweight and in-\nterpretable model designed to detect deception in highly\nimbalanced diplomatic dialogues. The architecture combines\nfrozen BERT embeddings, handcrafted linguistic and game-\nspecific features, and a PU learning objective to optimize for\nrare but critical deceptive instances. Figure 1 illustrates the\noverall pipeline.\n1) Input Processing: All input messages from the Diplomacy\ndataset are passed through three parallel branches:\nBERT Tokenizer and Encoder: Messages are first tokenized\nand embedded using a frozen bert-base-uncased model to\nextract contextualized sentence-level representations. These em-\nbeddings are fixed during training to reduce model complexity.\nLinguistic Feature Extractor: In parallel, a set of handcrafted\nlinguistic features is computed. These include pronoun ratios,\nhedge word usage, assertiveness scores, and sentiment scores\n(using V ADER), which help interpret deception-related cues.\nGame Feature Extractor: Game-related metadata such as\nseason, game score delta, and player roles are extracted to\ncapture strategic context relevant to deception.\n2) FeatureNet and Fusion: The linguistic and game-specific\nfeatures are fed into a small feed-forward subnetwork (Fea-\ntureNet), which consists of a linear layer followed by ReLU\nactivation and dropout for regularization. The output of Fea-\ntureNet is then concatenated with the frozen BERT embeddings\nto form a fused representation combining deep semantics with\ninterpretable structure.\n3) Classification and PU Learning: The fused representation\nis passed through a lightweight linear classifier to output a\nsingle logit value. Instead of using traditional binary cross-\nentropy, we employ a PU (Positive-Unlabeled) loss module\nthat estimates the classification risk using the known positive\nsamples (deceptive messages) and the unlabeled set, with a\nclass prior \u03c0 = 0.05.\n4) Optimization: The model is trained end-to-end using\nAdamW optimizer with a learning rate of 1e-3 and batch size\nof 32 for 25 epochs. Only the FeatureNet and classifier layers\nare updated during training, keeping BERT frozen, resulting in\na highly efficient model with only 1,345 trainable parameters.\nFig. 1. Overview of PU-Lie model architecture combining BERT embeddings,\nlinguistic and game-specific features, and PU learning.\nIV. E XPERIMENTS\nIn this section, we conduct comprehensive experiments\nto evaluate PU-Lie against six alternative models on the\nDiplomacy deception detection task. We describe our evaluation\nprotocol, model settings, and present detailed analysis of results.\nA. Evaluation Protocol\nGiven the extreme class imbalance (only 4.5% deceptive\nmessages), we adopt macro F1-score as our primary evaluation\nmetric. Macro F1 gives equal importance to both deceptive\n(positive) and truthful (negative) classes, making it more reliable\nthan accuracy under skewed distributions. We also report\n\nTABLE I\nMODEL PERFORMANCE ON DECEPTION DETECTION . PU-L IE OUTPERFORMS ALL BASELINES WHILE BEING SIGNIFICANTLY SMALLER , FASTER , AND MORE\nEFFICIENT .\nModel Macro F1 Trainable Params Training Time Inference Time Epochs\nTF-IDF + Logistic Regression 0.3900 1012 2 mins 10 secs -\nBERT + Power Embeddings + LSTM 0.4901 ( \u00b10.01) 2,300,000 18 mins 30 secs 10\nGNN with Graph Attention 0.5000 ( \u00b10.01) 1,101,954 22 mins 40 secs 10\nOversampled BERT + LSTM 0.5100 ( \u00b10.01) 2,300,000 19 mins 40 secs 10\nBERT + Linguistic + Game + LSTM 0.5453 ( \u00b10.02) 1,322,626 28 mins 40 secs 25\nBERT + Game Features + LSTM 0.5471 ( \u00b10.01) 1,180,994 25 mins 35 secs 25\nPU-Lie (Ours) 0.6000 ( \u00b10.01) 1,345 1.7 mins 5 secs 25\nstandard deviation across five runs with different seeds to\nensure robustness.\nFor PU-Lie, we apply precision-recall curve-based thresh-\nold tuning to better capture deceptive messages by maximizing\nrecall at an optimal precision.\nB. Models Evaluated\n\u2022 TF-IDF + Logistic Regression: Classical baseline using\nsparse features. Poor performance due to lack of context\n[15].\n\u2022 BERT + Game Features + LSTM: Combines BERT\nwith in-game features and LSTM. Moderate F1 but\ncomputationally heavy.\n\u2022 BERT + Linguistic + Game + LSTM: Adds linguistic\nmarkers (e.g., pronouns, hedging) to the previous model.\nSlight F1 boost.\n\u2022 BERT + Power Embeddings + LSTM: Adds strategic\npower dynamics as embeddings. Performance drops due\nto feature noise.\n\u2022 Oversampled BERT + LSTM: Uses naive oversampling\nof positive (deceptive) class. Slight improvement, but\noverfits.\n\u2022 GNN with Graph Attention: Represents players and\nmessages as a graph. Fails to generalize due to imbalance.\n\u2022 PU-Lie (Ours): Combines frozen BERT, handcrafted\nfeatures, and PU loss. Best performance with lowest\nparameter count.\nV. R ESULTS\nThe experimental results, highlighting the comparative\nperformance of all evaluated models, are comprehensively\npresented in Table I. This table illustrates the macro F1\nscores, parameter counts, training time and inferencing time\ndemonstrating the effectiveness and efficiency of the proposed\nPU-Lie model.\nA. Inference from Model Comparison\nAlthough the TF-IDF + Logistic Regression model has\nslightly fewer trainable parameters than PU-Lie (1,012 vs.\n1,345) and both models exhibit comparable training and\ninference times (approximately 2 minutes and 10 seconds for\nTF-IDF + LR vs. 1.7 minutes and 5 seconds for PU-Lie), the\nPU-Lie model significantly outperforms it in terms of macro F1\nscore (0.60 vs. 0.39). This highlights that while both models are\nlightweight and efficient, PU-Lie captures deceptive patterns\nfar more effectively due to its integration of contextual BERT\nembeddings, interpretable linguistic features, and PU learning\noptimization.\nB. Ablation Studies\n\u2022 BERT + Game Features + LSTM: This model uses\nBERT embeddings, game context features, and a bi-\ndirectional LSTM to capture sequential dependencies.\nIt performs better (F1 = 0.5471), showing the value of\ncombining deep language models with game dynamics.\n\u2022 BERT + Linguistic + Game + LSTM: Adds linguistic\nfeatures (e.g., pronouns, hedges) to the previous model.\nPerformance improves marginally (F1 = 0.5453), indi-\ncating that LSTMs may not effectively leverage shallow\nfeatures when trained jointly.\n\u2022 BERT + Power Embeddings + LSTM: Introduces hand-\ncrafted \u201cpower\u201d features reflecting strategic dominance or\ncontrol, but reduces performance (F1 = 0.4901), suggesting\nthese embeddings may be noisy or redundant.\n\u2022 Oversampled BERT + LSTM: Oversamples deceptive\nmessages to balance the dataset before training. Perfor-\nmance (F1 = 0.5100) is better than the power model but\nstill below PU-Lie, indicating that naive balancing does\nnot solve the core problem.\n\u2022 GNN with Graph Attention: Models player relationships\nand game structure as a graph. Despite its sophistication,\nthe model achieves only 0.50 F1, likely due to overfitting\non sparse deception data.\n\u2022 PU-Lie (Ours): Combines frozen BERT embeddings,\nlinguistic + game features, and PU loss. It achieves the\nhighest macro F1 of 0.60 with only 1,345 parameters,\ndemonstrating superior generalization and efficiency.\nVI. D ISCUSSION\nPU-Lie demonstrates strong performance and efficiency\nunder imbalanced settings. It avoids majority collapse and\nenhances interpretability.\nFocus on Deception: In deception detection, the cost of\nmissing a lie is typically higher than a false alarm. Our\nuse of PU learning reflects this bias by focusing on better\n\nidentifying deceptive messages and tailoring the architecture\nfor this asymmetric objective.\nEfficiency and Deployment: With only 1,345 parameters,\nPU-Lie is ideal for deployment in edge environments or real-\ntime settings.\nLimitations: PU-Lie assumes a static class prior. Cultural\nnuances or shifts in discourse style may not be captured.\nFuture work may explore dynamic priors and cross-domain\ntransferability.\nVII. C ONCLUSION\nWe present PU-Lie, a novel, interpretable, and lightweight\nmodel for deception detection in imbalanced strategic dialogue.\nIt combines frozen BERT embeddings with handcrafted features\nand a PU learning framework to outperform heavy baselines\nusing a fraction of the parameters. In addition to its strong\nperformance, PU-Lie is highly efficient, requiring significantly\nfewer computational resources and enabling faster inference\nand deployment in low-resource environments. Our results show\nthat explicitly targeting deceptive message detection\u2014rather\nthan treating all messages equally\u2014leads to more robust,\ninterpretable, and generalizable models. Future work includes\nmultilingual extensions, dynamic class prior estimation, and\nreal-time deception detection in interactive settings.\nREFERENCES\n[1] J. T. Hancock, L. E. Curry, S. Goorha, and M. Woodworth, \u201cLying for\nmoney: How people lie differently when they are paid more,\u201d Personality\nand Social Psychology Bulletin , vol. 34, no. 4, pp. 536\u2013548, 2008.\n[2] M. Ott, Y . Choi, C. Cardie, and J. T. Hancock, \u201cFinding deceptive opinion\nspam by any stretch of the imagination,\u201d in Proceedings of the 49th\nAnnual Meeting of the Association for Computational Linguistics , 2011.\n[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training\nof deep bidirectional transformers for language understanding,\u201d NAACL-\nHLT, 2019.\n[4] N. Ruchansky, S. Seo, and Y . Liu, \u201cCsi: A hybrid deep model for fake\nnews detection,\u201d in Proceedings of the 2017 ACM on Conference on\nInformation and Knowledge Management , 2017, pp. 797\u2013806.\n[5] M. V ollmer and S. Adali, \u201cLinguistic deception detection using neural\nnetworks,\u201d in Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , 2021.\n[6] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Doll \u00b4ar, \u201cFocal loss\nfor dense object detection,\u201d in Proceedings of the IEEE International\nConference on Computer Vision , 2017, pp. 2980\u20132988.\n[7] J. Bekker and J. Davis, \u201cLearning from positive and unlabeled data: A\nsurvey,\u201d Machine Learning, vol. 109, pp. 719\u2013760, 2020.\n[8] C. Elkan and K. Noto, \u201cLearning classifiers from only positive and\nunlabeled data,\u201d Proceedings of the 14th ACM SIGKDD international\nconference on Knowledge discovery and data mining , pp. 213\u2013220, 2008.\n[9] N. F. Rajani, B. McCann, C. Xiong, and R. Socher, \u201cExplain yourself!\nleveraging language models for commonsense reasoning,\u201d in Proceedings\nof the 57th Annual Meeting of the Association for Computational\nLinguistics, 2019.\n[10] P. Veli \u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and\nY . Bengio, \u201cGraph attention networks,\u201d in International Conference\non Learning Representations (ICLR) , 2018.\n[11] T. Zhang, R. Xu, Q. Zhang, and Z. Wu, \u201cStructure-aware dialogue state\ntracking,\u201d in Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics (ACL) , 2021.\n[12] S. Feng, R. Banerjee, and Y . Choi, \u201cSyntactic stylometry for deception\ndetection,\u201d ACL, 2012.\n[13] N. V . Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer,\n\u201cSmote: Synthetic minority over-sampling technique,\u201d Journal of artificial\nintelligence research, vol. 16, pp. 321\u2013357, 2002.\n[14] F. Li, M. Huang, Y . Yang, and X. Zhu, \u201cLearning to identify review spam,\u201d\nin Proceedings of the Twenty-Second international joint conference on\nArtificial Intelligence, 2011.\n[15] D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Sch \u00a8olkopf, \u201cLearning\nwith local and global consistency,\u201d in Advances in neural information\nprocessing systems, 2004.",
  "metadata": {
    "title": "PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning",
    "author": "Bhavinkumar Vinodbhai Kuwar; Bikrant Bikram Pratap Maurya; Priyanshu Gupta; Nitin Choudhury",
    "pages": 5,
    "source_file": "Choudhury_2024_PU_Lie_Lightweight_Deception_Detection.pdf",
    "pdf_library": "pypdf"
  },
  "page_mapping": {
    "0": [
      1,
      0,
      5213
    ],
    "5213": [
      2,
      5213,
      10957
    ],
    "10957": [
      3,
      10957,
      15126
    ],
    "15126": [
      4,
      15126,
      19598
    ],
    "19598": [
      5,
      19598,
      23585
    ]
  }
}